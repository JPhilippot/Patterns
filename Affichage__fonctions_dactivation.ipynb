{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.265565Z",
     "start_time": "2020-02-13T18:33:00.211348Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/quentin/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/quentin/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/quentin/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/quentin/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/quentin/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/quentin/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import keras\n",
    "import os\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.282036Z",
     "start_time": "2020-02-13T18:33:02.268265Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_goodXy (X,y):\n",
    "    ynew = model.predict_classes(X)\n",
    "    X_good =[]\n",
    "    y_good=[]\n",
    "    for i in range(len(X)):\n",
    "        if (ynew[i]==0 and y[i]==1) or (ynew[i]==1 and y[i]==0):\n",
    "            print (\"error prediction for X=%s, Predicted=%s, Real=%s\"% (X[i], ynew[i], y[i]))\n",
    "        else :\n",
    "            X_good.append(X[i])\n",
    "            y_good.append(y[i])\n",
    "    return X_good,y_good        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.299211Z",
     "start_time": "2020-02-13T18:33:02.285314Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_result_layers(model,X):\n",
    "    result_layers=[]\n",
    "    for i in range (len(model.layers)-1):\n",
    "        hidden_layers= keras.backend.function(\n",
    "                [model.layers[0].input],   \n",
    "                [model.layers[i].output,] \n",
    "                )    \n",
    "        result_layers.append(hidden_layers([X])[0])  \n",
    "    return result_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.324742Z",
     "start_time": "2020-02-13T18:33:02.303088Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_result_layers(filename,X,y,result_layers):\n",
    "    f = open(filename, \"w\")\n",
    "    for nb_X in range (len(X)):\n",
    "        #my_string=\"\"\n",
    "        my_string=str(y[nb_X])+','\n",
    "        for nb_layers in range (len(model.layers)-1):\n",
    "            my_string+=\"<b>,\"\n",
    "            for j in range (len(result_layers[nb_layers][nb_X])):\n",
    "                my_string+=str(result_layers[nb_layers][nb_X][j])+','\n",
    "            my_string+=\"</b>,\"    \n",
    "        my_string=my_string [0:-1]\n",
    "        my_string+='\\n'\n",
    "        f.write(my_string)    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:03.159174Z",
     "start_time": "2020-02-13T18:33:02.327414Z"
    }
   },
   "outputs": [],
   "source": [
    "url=\"./iris.csv\"\n",
    "names = ['SepalLengthCm', 'SepalWidthCm', \n",
    "         'PetalLengthCm', 'PetalWidthCm', \n",
    "         'Species']\n",
    "\n",
    "data = pd.read_csv(url, names=names)\n",
    "\n",
    "#Classification binaire sur Virginica et Setosa seulement\n",
    "data=data[data['Species'].isin(['Iris-virginica', 'Iris-setosa'])]\n",
    "\n",
    "i = 8\n",
    "data_to_predict = data[:i].reset_index(drop = True) \n",
    "\"\"\"\n",
    "reset_index() is a method to reset index of a Data Frame. \n",
    "reset_index() method sets a list of integer ranging from 0 to length of data as index. \n",
    "\"\"\"\n",
    "predict_species = data_to_predict.Species \n",
    "\"\"\" Species de la class Iris \"\"\"\n",
    "\n",
    "predict_species = np.array(predict_species) \n",
    "\"\"\"\n",
    "An array object satisfying the specified requirements.\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html\n",
    "\"\"\"\n",
    "\n",
    "prediction = np.array(data_to_predict.drop(['Species'],axis= 1))\n",
    "\n",
    "data = data[i:].reset_index(drop = True)\n",
    "\n",
    "X = data.drop(['Species'], axis = 1) \n",
    "\"\"\"\n",
    "The drop() function is used to drop specified labels from rows or columns.\n",
    "\"\"\"\n",
    "X = np.array(X)\n",
    "y = data['Species']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y=encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "train_X, test_X, train_y, test_y = model_selection.train_test_split(X,y,test_size = 0.1, random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.678441Z",
     "start_time": "2020-02-13T18:33:03.161667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.9901 - acc: 0.4390\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 0s 493us/step - loss: 0.6593 - acc: 0.4390\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 0s 434us/step - loss: 0.5813 - acc: 0.4390\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 0s 412us/step - loss: 0.5363 - acc: 0.4390\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 0s 444us/step - loss: 0.4924 - acc: 0.6463\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 0s 439us/step - loss: 0.4538 - acc: 0.9634\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 0s 407us/step - loss: 0.4193 - acc: 0.9756\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 0s 433us/step - loss: 0.3947 - acc: 0.9878\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 0s 443us/step - loss: 0.3758 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 0s 425us/step - loss: 0.3595 - acc: 0.9878\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "\n",
      "acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Utilisation de keras comme classifieur\n",
    "# mettre sigmoid comme fonction car binaire. Attention 1 seul neurone en sortie\n",
    "input_dim = len(data.columns) - 1\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\n",
    "\n",
    "model.fit(train_X, train_y, epochs = 10, batch_size = 2)\n",
    "\n",
    "\n",
    "\n",
    "scores = model.evaluate(test_X, test_y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.723085Z",
     "start_time": "2020-02-13T18:33:04.681566Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Récupération seulement des bons classés\n",
    "X_good,y_good=get_goodXy (train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.796700Z",
     "start_time": "2020-02-13T18:33:04.725841Z"
    }
   },
   "outputs": [],
   "source": [
    "# Récupération des valeurs de tous les layers sauf le dernier\n",
    "result_layers=get_result_layers(model,X_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.834457Z",
     "start_time": "2020-02-13T18:33:04.799432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sauvegarde du fichier\n",
    "# structure :\n",
    "# 0/1 = valeur de la classe\n",
    "# chaque valeur de layer est entourée par un []\n",
    "save_result_layers(\"iris_8_10_8_tmp\",X_good,y_good,result_layers)\n",
    "# tri du fichier\n",
    "os.system ('sort iris_8_10_8_tmp > iris_8_10_8_.csv')\n",
    "# effacer le fichier intermédiaire\n",
    "os.system ('rm iris_8_10_8_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_layers_from_csv(filename):\n",
    "    tokens=filename.split(\"_\")\n",
    "    df = pd.read_csv(filename, sep = ',', header = None) \n",
    "\n",
    "    \n",
    "    # creation d'un répertoire pour sauver tous les fichiers\n",
    "    repertoire=filename[0:-4]\n",
    "    os.makedirs(repertoire, exist_ok=True)\n",
    "    string = repertoire+'/'+tokens[0]+'_'\n",
    "    f=[]\n",
    "    filenames=[]\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        name_file=string+'l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'.csv'\n",
    "        f.append(open(name_file, \"w\"))\n",
    "        filenames.append(name_file)\n",
    "        \n",
    "        \n",
    "    # sauvegarde du dataframe dans une chaîne de caracteres\n",
    "    ch = df.to_string(header=False,\n",
    "                  index=False,\n",
    "                  index_names=False).split('\\n')\n",
    "    vals = [','.join(ele.split()) for ele in ch]\n",
    "    \n",
    "    # sauvegarde dans des fichiers spécifiques par layer\n",
    "    token_layer=[]\n",
    "    token_exemples=[]\n",
    "    for nb_exemples in range (len(vals)):\n",
    "        deb=str(df[0][nb_exemples])+','\n",
    "        # 1 ligne correspond à une chaine\n",
    "        s=vals[nb_exemples]\n",
    "        listoftokens=re.findall(r'<b>,(.+?),</b>', s)\n",
    "        nb_layers=len(listoftokens)\n",
    "        \n",
    "        for nb_token in range (nb_layers):\n",
    "            save_token=''\n",
    "            save_token=deb+str(listoftokens[nb_token])+'\\n'\n",
    "            \n",
    "            f[nb_token].write(save_token)\n",
    "\n",
    "    # sauvegarde d'un fichier qui contient tous les layers en une fois\n",
    "    # récupération des données pour enlever les <b> et </b>\n",
    "    df_all=pd.DataFrame()\n",
    "    myindex=0\n",
    "    for nb_columns in range(df.shape[1]):\n",
    "        df[nb_columns]=df[nb_columns].astype(str)\n",
    "        if (df[nb_columns]!='<b>').all() and (df[nb_columns]!='</b>').all():\n",
    "            df_all[myindex]=df[nb_columns]\n",
    "            myindex+=1\n",
    "\n",
    "    # construction du nom du fichier de sauvegarde\n",
    "    string = repertoire+'/'+tokens[0]+'_'\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        string+='l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'_'\n",
    "    string+='.csv'       \n",
    "    # sauvegarde en .csv\n",
    "    df_all.to_csv(string, sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a directory with a specific file for all the layers\n",
    "filename=\"iris_8_10_8_.csv\"    \n",
    "get_directory_layers_from_csv(filename)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a directory with a specific file for all the layers\n",
    "filename=\"iris_8_10_8_.csv\"    \n",
    "get_directory_layers_from_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajouter l'export des csv pour la classe 3\n",
    "\n",
    "Cette cellule récupère les donées de la classe 3 depuis iris.csv, enlève la colonne species, fait un predict_classes sur la classe 3, récupère les fonctions d'activation et les enregistre layer par layer dans un sous-répertoire \"iris_8_10_8_C3\" dans des csv nommés indentiquement à ceux pour les classes 1 et 2. On rajoute également la classe prédite en première colonne pour rester cohérent avec les autres CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(url, names=names)\n",
    "data=data[data['Species'].isin(['Iris-versicolor'])].drop(['Species'],axis= 1)\n",
    "classe3 = model.predict_classes(data)\n",
    "#print(classe3)\n",
    "\n",
    "result_layersC3=get_result_layers(model,data)\n",
    "#print(result_layersC3)\n",
    "\n",
    "#On rajoute une première colonne avec la classe prédite pour être cohérent avec les autres csv\n",
    "for i,arr  in enumerate(result_layersC3):\n",
    "    arr =  np.insert(arr,0,classe3[i], axis=1)\n",
    "    result_layersC3[i] = arr\n",
    "                                               \n",
    "rep = \"iris_8_10_8_C3/\"\n",
    "os.makedirs(\"iris_8_10_8_C3\", exist_ok=True)\n",
    "for i, (filename, arr) in enumerate(zip([\"iris_l1_8.csv\", \"iris_l2_10.csv\", \"iris_l3_8.csv\"],result_layersC3)):\n",
    "    np.savetxt(rep+filename, arr, delimiter=\",\", fmt=\"%f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result_cluster(filename,clusters,numLayer,cluster_class):\n",
    "#    f = open(filename, \"w\")\n",
    "    idClusters=np.unique(clusters)\n",
    "    tabCount={}\n",
    "    for id in idClusters:\n",
    "        count = 0\n",
    "        for j in range(len(clusters)):\n",
    "            if clusters[j]==id:\n",
    "                tabCount[id]=count+1\n",
    "                count+=1\n",
    "    print(tabCount)\n",
    "    my_string=\"\"\n",
    "    for key,value in tabCount.items():\n",
    "        my_string += str(numLayer)+','+str(cluster_class)+','\n",
    "        my_string+=str(value)+','+str(key)+'\\n'\n",
    "    print(my_string)\n",
    "#     f.close()\n",
    "\n",
    "def clusterizeLayer(url, nbClusters=4):\n",
    "    data = pd.read_csv(url,names=names)\n",
    "    dataC1=dataC2=[]\n",
    "    indexC2=indexC1=[]\n",
    "    i=0\n",
    "    for index, row in data.iterrows():\n",
    "        if row[0]==0:\n",
    "            dataC1.append(row)\n",
    "            indexC1=i\n",
    "        else:\n",
    "            dataC2.append(row)\n",
    "            indexC2=i\n",
    "        i+=1\n",
    "        \n",
    "#     with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#         print(data)\n",
    "\n",
    "    algo = KMeans(n_clusters = nbClusters)\n",
    "    #fit_predict fait tourner l'algo et retourne y où y[i] est l'indice du cluster auquel dataC12[i] appartient\n",
    "    clustersC1C2 = algo.fit_predict(data)\n",
    "#     print(clustersC1C2)\n",
    "\n",
    "    dataC3=pd.read_csv('iris.csv',names=names)\n",
    "    dataC3=dataC3[dataC3['Species'].isin(['Iris-versicolor'])]\n",
    "    dataC3['Species']=1.0\n",
    "\n",
    "#     print(\"print dataC3:\\n\")\n",
    "#     print(dataC3)\n",
    "\n",
    "    #predict utilise les clusters calculés par fit_predict(dataC1C2) et attribue chaque ligne de dataC3 au cluster dont le centre\n",
    "    #minimise la distance\n",
    "    C3ClusterPred = algo.predict(dataC3)\n",
    "\n",
    "#     print(\"Clusters C3:\\n\")\n",
    "#     print(C3ClusterPred)\n",
    "    return clustersC1C2, C3ClusterPred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remplacer clusterizeLayer\n",
    "\n",
    "Problèmes: \n",
    "\n",
    "1. Les .csv contiennent en première colonne la classe (0 ou 1) de l'iris. Cette information ne fait pas partie des fonctions d'activation (sauf la dernière layer) et ne devrait pas être incluse dans le clustering. En outre dès qu'on veut rajouter d'autres images sans information sur la classe (en dehors de l'entrainement donc) on n'a pas d'infos sur la classe.\n",
    "  Solution:\n",
    "   * Nommer correctement les colonnes lors de l'importation (première colonne = \"classe\", et les suivantes \"neuroneN\")\n",
    "   * Faire un drop sur la colonne classe pour ne récupérer que les colonnes correspondant aux fonctions d'activation\n",
    "2. Pour la classe 3 on utilisait les données brutes. A la place je récupère les données des fonctions d'activation de la classe (avec un truc un peu moche ligne 16 pour déterminer le nom du fichier csv de la classe 3 à partir de celui des classes 1 et 2 données par le paramètre url). Pour rester cohérent avec les classes 1 et 2 le numéro de classe est aussi présent en première colonne, je l'enlève donc également."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterizeLayer(url, nbClusters=4):\n",
    "    data = pd.read_csv(url, header=None)\n",
    "\n",
    "    data.columns = ['classe']+[\"neurone{}\".format(i) for i in range(len(data.columns)-1)]\n",
    "    \n",
    "    data = data.drop(['classe'],axis= 1)\n",
    "        \n",
    "#     with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#         print(data)\n",
    "\n",
    "    algo = KMeans(n_clusters = nbClusters)\n",
    "    #fit_predict fait tourner l'algo et retourne y où y[i] est l'indice du cluster auquel dataC12[i] appartient\n",
    "    clustersC1C2 = algo.fit_predict(data)\n",
    "#     print(clustersC1C2)\n",
    "\n",
    "    csvname = \"iris_8_10_8_C3/\"+url.split(\"/\")[2]\n",
    "    dataC3=pd.read_csv(csvname, header=None)\n",
    "    dataC3.columns = ['classe']+[\"neurone{}\".format(i) for i in range(len(dataC3.columns)-1)]\n",
    "    dataC3 = dataC3.drop(['classe'],axis= 1)\n",
    "\n",
    "\n",
    "#     print(\"print dataC3:\\n\")\n",
    "#     print(dataC3)\n",
    "\n",
    "    #predict utilise les clusters calculés par fit_predict(dataC1C2) et attribue chaque ligne de dataC3 au cluster dont le centre\n",
    "    #minimise la distance\n",
    "    C3ClusterPred = algo.predict(dataC3)\n",
    "\n",
    "#     print(\"Clusters C3:\\n\")\n",
    "#     print(C3ClusterPred)\n",
    "\n",
    "    return clustersC1C2, C3ClusterPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2, 2: 47, 3: 1}\n",
      "1,3,2,1\n",
      "1,3,47,2\n",
      "1,3,1,3\n",
      "\n",
      "{0: 1, 3: 49}\n",
      "2,3,1,0\n",
      "2,3,49,3\n",
      "\n",
      "{0: 50}\n",
      "3,3,50,0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clustersC3l1=clusterizeLayer('./iris_8_10_8_/iris_l1_8.csv')\n",
    "# clustersC3l2=clusterizeLayer('./iris_8_10_8_/iris_l2_10.csv')\n",
    "# clustersC3l3=clusterizeLayer('./iris_8_10_8_/iris_l3_8.csv')\n",
    "\n",
    "save_result_cluster(\"osef\",clusterizeLayer('./iris_8_10_8_/iris_l1_8.csv',4)[1],1,3)\n",
    "save_result_cluster(\"osef\",clusterizeLayer('./iris_8_10_8_/iris_l2_10.csv',4)[1],2,3)\n",
    "save_result_cluster(\"osef\",clusterizeLayer('./iris_8_10_8_/iris_l3_8.csv',4)[1],3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from numpy import ndarray\n",
    "from matplotlib import colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "def plot_clusters_3D (X,y_predict, nb_clusters, pca_done=False):\n",
    "    if pca_done==False:\n",
    "        pca = PCA(n_components=3) \n",
    "        pca.fit(X) \n",
    "        pca_data = pd.DataFrame(pca.transform(X))\n",
    "    else: pca_data = pd.DataFrame(X)\n",
    "    colors = list(zip(*sorted(( \n",
    "                    tuple(mcolors.rgb_to_hsv( \n",
    "                          mcolors.to_rgba(color)[:3])), name) \n",
    "                     for name, color in dict( \n",
    "                            mcolors.BASE_COLORS, **mcolors.CSS4_COLORS \n",
    "                                                      ).items())))[1] \n",
    "    # number of steps to taken generate n(clusters) colors\n",
    "    skips = math.floor(len(colors[5 : -5])/nb_clusters) \n",
    "    cluster_colors = colors[5 : -5 : skips] \n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax = fig.add_subplot(111, projection = '3d') \n",
    "    ax.scatter(pca_data[0], pca_data[1], pca_data[2],\n",
    "           c = list(map(lambda label : cluster_colors[label], \n",
    "                                            y_predict))) \n",
    "\n",
    "    str_labels = list(map(lambda label:'% s' % label, y_predict)) \n",
    "\n",
    "    list(map(lambda data1, data2, data3, str_label: \n",
    "        ax.text(data1, data2, data3, s = str_label, size = 16.5, \n",
    "        zorder = 20, color = 'k'), pca_data[0], pca_data[1], \n",
    "        pca_data[2], str_labels)) \n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def plot_clusters_2D (X,y_predict, nb_clusters, pca_done=False):\n",
    "    if pca_done==False:\n",
    "        pca = PCA(n_components=2) \n",
    "        X_r = pca.fit(X).transform(X)\n",
    "    else: X_r = X\n",
    "    data = pd.DataFrame(X_r, columns=['x','y'])\n",
    "    data['label']=y_predict\n",
    "    list_clusters=list(set(y_predict))\n",
    "    print (list_clusters)\n",
    "    #create a new figure\n",
    "    plt.figure(figsize=(5,5))\n",
    "\n",
    "    #loop through labels and plot each cluster\n",
    "    for i, label in enumerate(list_clusters):\n",
    "\n",
    "        #add data points \n",
    "        plt.scatter(x=data.loc[data['label']==label, 'x'], \n",
    "                y=data.loc[data['label']==label,'y'], \n",
    "                color=color[i], \n",
    "                alpha=0.20)\n",
    "\n",
    "        #add label\n",
    "        plt.annotate(label, \n",
    "                 data.loc[data['label']==label,['x','y']].mean(),\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 size=20, weight='bold',\n",
    "                 color=color[i])\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "def get_directory_layers_from_csv(filename):\n",
    "    tokens=filename.split(\"_\")\n",
    "    df = pd.read_csv(filename, sep = ',', header = None) \n",
    "\n",
    "    \n",
    "    # creation d'un répertoire pour sauver tous les fichiers\n",
    "    repertoire=filename[0:-4]\n",
    "    os.makedirs(repertoire, exist_ok=True)\n",
    "    string = repertoire+'/'+tokens[0]+'_'\n",
    "    f=[]\n",
    "    filenames=[]\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        name_file=string+'l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'.csv'\n",
    "        f.append(open(name_file, \"w\"))\n",
    "        filenames.append(name_file)\n",
    "        \n",
    "        \n",
    "    # sauvegarde du dataframe dans une chaîne de caracteres\n",
    "    ch = df.to_string(header=False,\n",
    "                  index=False,\n",
    "                  index_names=False).split('\\n')\n",
    "    vals = [','.join(ele.split()) for ele in ch]\n",
    "    \n",
    "    # sauvegarde dans des fichiers spécifiques par layer\n",
    "    token_layer=[]\n",
    "    token_exemples=[]\n",
    "    for nb_exemples in range (len(vals)):\n",
    "        deb=str(df[0][nb_exemples])+','\n",
    "        # 1 ligne correspond à une chaine\n",
    "        s=vals[nb_exemples]\n",
    "        listoftokens=re.findall(r'<b>,(.+?),</b>', s)\n",
    "        nb_layers=len(listoftokens)\n",
    "        \n",
    "        for nb_token in range (nb_layers):\n",
    "            save_token=''\n",
    "            save_token=deb+str(listoftokens[nb_token])+'\\n'\n",
    "            \n",
    "            f[nb_token].write(save_token)\n",
    "\n",
    "    # sauvegarde d'un fichier qui contient tous les layers en une fois\n",
    "    # récupération des données pour enlever les <b> et </b>\n",
    "    df_all=pd.DataFrame()\n",
    "    myindex=0\n",
    "    for nb_columns in range(df.shape[1]):\n",
    "        df[nb_columns]=df[nb_columns].astype(str)\n",
    "        if (df[nb_columns]!='<b>').all() and (df[nb_columns]!='</b>').all():\n",
    "            df_all[myindex]=df[nb_columns]\n",
    "            myindex+=1\n",
    "\n",
    "    # construction du nom du fichier de sauvegarde\n",
    "    string = repertoire+'/'+tokens[0]+'_'\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        string+='l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'_'\n",
    "    string+='.csv'       \n",
    "    # sauvegarde en .csv\n",
    "    df_all.to_csv(string, sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rajouter cellule pour l'affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_clusters_2D_title_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7adea1eda17a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m                ('./iris_8_10_8_/iris_l3_8.csv',8, \"C1 et C2\")]\n\u001b[1;32m     91\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_neurones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlisteLayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mafficherClassesTogether\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_neurones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;31m#afficherClasses_2(csv, 4, nb_neurones, classes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-7adea1eda17a>\u001b[0m in \u001b[0;36mafficherClassesTogether\u001b[0;34m(csv, nb_clusters, nb_neurones, classinfo)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mplot_clusters_2D_title_2\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"3 classes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_clusters_2D_title_2' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_clusters_2D_title (X,y_predict, classes, nb_clusters, title, pca_done=False):\n",
    "    color = ['red', 'blue', 'purple', 'green']\n",
    "    markers = [\"D\", \"^\",\"o\"]\n",
    "    if pca_done==False:\n",
    "        pca = PCA(n_components=2) \n",
    "        X_r = pca.fit(X).transform(X)\n",
    "    else: X_r = X\n",
    "    data = pd.DataFrame(X_r, columns=['x','y'])\n",
    "    data['label']=y_predict\n",
    "    list_clusters=list(set(y_predict))\n",
    "    #create a new figure\n",
    "    plt.figure(figsize=(8,8))\n",
    "    \n",
    "    plt.title(title)\n",
    "\n",
    "    #loop through clusters (for different colors)\n",
    "    for i, label in enumerate(list_clusters):\n",
    "        # loop for classes (for different markers)\n",
    "        for j,classe in enumerate(classes.unique()):\n",
    "            #add data points \n",
    "            plt.scatter(x=data.loc[(data['label']==label) & (classes==classe), 'x'], \n",
    "                    y=data.loc[(data['label']==label) & (classes==classe),'y'], \n",
    "                    s = 150,\n",
    "                    color=color[i],\n",
    "                    marker = markers[j],\n",
    "                    alpha=0.4)\n",
    "\n",
    "        #add cluster number\n",
    "        plt.annotate(label, \n",
    "                 data.loc[data['label']==label,['x','y']].mean(),\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 size=20, weight='bold',\n",
    "                 color=\"k\"#color[i]\n",
    "                    )\n",
    "    \n",
    "    #add legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = []\n",
    "    for i,classe in enumerate(classes.unique()):\n",
    "        legend_elements.append(Line2D([0], [0],\n",
    "                                marker = markers[i],\n",
    "                                color =\"w\",\n",
    "                                markerfacecolor = 'gray',\n",
    "                                markersize = 10,\n",
    "                                markeredgewidth=0.5,\n",
    "                                markeredgecolor = 'k',\n",
    "                                label = \"Classe {}\".format(i)))\n",
    "    plt.legend(handles = legend_elements)\n",
    "\n",
    "        \n",
    "def getNameC3CSV(csv):\n",
    "    return\"iris_8_10_8_C3/\"+csv.split(\"/\")[2]\n",
    "\n",
    "        \n",
    "def afficherClasses_2(csv, nb_clusters, nb_neurones, classinfo):\n",
    "    clustersC1C2, C3ClusterPred = clusterizeLayer(csv, nb_clusters)\n",
    "    data = pd.read_csv(csv, names=['classe']+list(range(nb_neurones)))\n",
    "    classes = pd.Series(data['classe'])\n",
    "    data = data.drop(['classe'], axis = 1)\n",
    "    print(len(data))\n",
    "    print(len(clustersC1C2))\n",
    "    plot_clusters_2D_title_2 (data, clustersC1C2,classes, nb_clusters, csv+\" \"+classinfo)\n",
    "\n",
    "def afficherClassesTogether(csv, nb_clusters, nb_neurones, classinfo):\n",
    "    clustersC1C2, C3ClusterPred = clusterizeLayer(csv, nb_clusters)\n",
    "    data = pd.read_csv(csv, names=['classe']+list(range(nb_neurones)))\n",
    "    classes = pd.Series(data['classe'])\n",
    "    dataC1C2 = data.drop(['classe'], axis = 1)\n",
    "  \n",
    "    dataC3 = pd.read_csv(getNameC3CSV(csv), names=['classe']+list(range(nb_neurones)))\n",
    "    #classes = pd.concat([classes,pd.Series(dataC3['classe'])], ignore_index = True)\n",
    "    # Le csv contient la classe prédite (soit 0 ou 1 nécessairement) mais on doit spécifier que c'est en fait la classe 2\n",
    "    # On rajoute donc une colonne avec que des 2\n",
    "    classe3 = pd.Series([2 for _ in range(len(dataC3))])\n",
    "    classes = pd.concat([classes,classe3], ignore_index = True)\n",
    "\n",
    "    dataC3 = dataC3.drop(['classe'], axis = 1)\n",
    "\n",
    "    data_all = pd.concat([dataC1C2, dataC3], axis=0, ignore_index = True)\n",
    "    cluster_all = np.concatenate([clustersC1C2, C3ClusterPred], axis=0)\n",
    "    \n",
    "    \n",
    "    plot_clusters_2D_title_2 (data_all, cluster_all, classes, nb_clusters, csv+\" \"+\"3 classes\")\n",
    "\n",
    "\n",
    "\n",
    "listeLayers = [('./iris_8_10_8_/iris_l1_8.csv', 8,  \"C1 et C2\"),\n",
    "               ('./iris_8_10_8_/iris_l2_10.csv',10, \"C1 et C2\"),\n",
    "               ('./iris_8_10_8_/iris_l3_8.csv',8, \"C1 et C2\")]\n",
    "for csv, nb_neurones, classes in listeLayers:\n",
    "    afficherClassesTogether(csv, 4, nb_neurones, classes)\n",
    "    #afficherClasses_2(csv, 4, nb_neurones, classes)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "from numpy import load\n",
    "\n",
    "data = load('mnist.npz')\n",
    "lst = data.files\n",
    "print(lst)\n",
    "\n",
    "(X_train,y_train) = (data[lst[1]],data[lst[2]])\n",
    "(X_test,y_test) = (data[lst[0]],data[lst[3]])\n",
    "\n",
    "X_train_sample=X_train[0:100]\n",
    "y_train_sample=y_train[0:100]\n",
    "\n",
    "X_train=X_train_sample\n",
    "y_train=y_train_sample\n",
    "X_train = X_train.reshape(100, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "\n",
    "X_01=[]\n",
    "y_01=[]\n",
    "nb_X=0\n",
    "for i in range(X_train.shape[0]):\n",
    "    if (y_train[i]==0 or y_train[i]==1):\n",
    "        \n",
    "        nb_X+=1\n",
    "        X_01.append(X_train[i])\n",
    "        y_01.append(y_train[i])\n",
    "\n",
    "       \n",
    "train_X=np.asarray(X_01)\n",
    "\n",
    "train_y=y_01\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "train_y=encoder.fit_transform(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\n",
    "\n",
    "model.fit(train_X, train_y, epochs = 40, batch_size = 32)\n",
    "\n",
    "\n",
    "X_good,y_good=get_goodXy (train_X, train_y)\n",
    "\n",
    "\n",
    "# Récupération des valeurs de tous les layers sauf le dernier\n",
    "result_layers=get_result_layers(model,X_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du fichier\n",
    "# structure :\n",
    "# 0/1 = valeur de la classe\n",
    "# chaque valeur de layer est entourée par une étoile *\n",
    "save_result_layers(\"mnist_512_tmp\",X_good,y_good,result_layers)\n",
    "# tri du fichier\n",
    "os.system ('sort mnist_512_tmp > mnist_512_.csv')\n",
    "# effacer le fichier intermédiaire\n",
    "os.system ('rm mnist_512_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalise = True\n",
    "includeClasses = True\n",
    "nbClusters = 10\n",
    "\n",
    "data = load('mnist.npz')\n",
    "lst = data.files\n",
    "\n",
    "print(data[lst[0]][0]) #X_test\n",
    "print(data[lst[1]][0]) #X_train\n",
    "print(data[lst[2]][0]) #y_train\n",
    "print(data[lst[3]][0]) #y_test\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#On rajoute une colonne Classe qui contient le numéro de la classe correspondant au feature Species\n",
    "listeSpecies = pd.unique(data['Species'])\n",
    "listeIndices = [i+1 for i in range(len(listeSpecies))]\n",
    "dico = dict(zip(listeSpecies, listeIndices))\n",
    "data.insert(len(data.columns), 'Classe', [dico[x] for x in data['Species']])\n",
    "\n",
    "if normalise:\n",
    "    if includeClasses:\n",
    "        m = data.columns[data.columns!='Species']\n",
    "    else:\n",
    "        #On veut garder la séparation en classes 1, 2 et 3, on exclut donc cette colonne de la normalisation\n",
    "        m = data.columns[~data.columns.isin(['Species','Classe'])]\n",
    "    data[m] = (data[m]-data[m].min())/(data[m].max()-data[m].min())\n",
    "\n",
    "#Note: dataC3 et dataC1C2 sont des copies profondes\n",
    "dataC1C2 = data[data['Species'].isin(['Iris-virginica','Iris-setosa'])].drop('Species', axis=1)\n",
    "dataC3 = data[data['Species']=='Iris-versicolor'].drop('Species', axis=1)\n",
    "\n",
    "\n",
    "algo = KMeans(n_clusters = nbClusters)\n",
    "#fit_predict fait tourner l'algo et retourne y où y[i] est l'indice du cluster auquel dataC12[i] appartient\n",
    "clustersC1C2 = algo.fit_predict(dataC1C2)\n",
    "\n",
    "#predict utilise les clusters calculés par fit_predict(dataC12) et attribue chaque ligne de dataC3 au cluster dont le centre\n",
    "#minimise la distance\n",
    "C3ClusterPred = algo.predict(dataC3)\n",
    "\n",
    "#Les Species sont-elles bien discrimninées par l'algo de clustering?\n",
    "clusters = [[] for _ in range(1+max(clustersC1C2))]\n",
    "for i,x in enumerate(clustersC1C2):\n",
    "    clusters[x].append(i)\n",
    "taillesClusters = [len(l) for l in clusters]\n",
    "print(\"Tailles des clusters non vides\")\n",
    "print(taillesClusters)\n",
    "\n",
    "for cluster in clusters:\n",
    "    if not(cluster):\n",
    "        continue\n",
    "    classe = data.at[cluster[0],'Species']\n",
    "    for sign in cluster:\n",
    "        if data.at[sign, 'Species'] != classe:\n",
    "            print(\"Présence classe mixte\")\n",
    "\n",
    "#Predict sur la classe 3\n",
    "X = dataC3.drop(columns= ['Classe'])\n",
    "X = np.array(X)\n",
    "Y = model.predict(X)\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "Y = sigmoid(Y)\n",
    "print(Y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
