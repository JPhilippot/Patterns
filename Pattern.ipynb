{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOnjour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#fonctions d'activation\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def valueoutput(y_hat):\n",
    "    for i in range(len(y_hat)):\n",
    "        if y_hat[i]>0.5:\n",
    "            y_hat[i]=1\n",
    "        else: \n",
    "            y_hat[i]=0\n",
    "    return y_hat        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.nbLayers=0\n",
    "        self.layers=[]     \n",
    "        \n",
    "    def info(self):\n",
    "        print(\"Content of the network:\");\n",
    "        j=0;\n",
    "        for i in range(len(self.layers)):\n",
    "            print(\"Layer n° \",i,\" => \")\n",
    "            print (\"\\tInput \", self.layers[i].input, \n",
    "                   \"\\tOutput\", self.layers[i].output)             \n",
    "            if (i != 0):\n",
    "                print (\"\\tActivation Function\",self.layers[i].activation_func)\n",
    "                print (\"\\tW\", self.layers[i].parameters['W'].shape,self.layers[i].parameters['W'])\n",
    "                print (\"\\tb\", self.layers[i].parameters['b'].shape,self.layers[i].parameters['b'])\n",
    "\n",
    "            \n",
    "    def addLayer(self,layer):\n",
    "        self.nbLayers += 1;\n",
    "        if (self.nbLayers==1): \n",
    "            # this is the first layer so adding a layer 0\n",
    "            layerZero=Layer(layer.input)\n",
    "            self.layers.append(layerZero)\n",
    "            \n",
    "        self.layers.append(layer) \n",
    "        self.layers[self.nbLayers].input=self.layers[self.nbLayers-1].output\n",
    "        self.layers[self.nbLayers].output=self.layers[self.nbLayers].output\n",
    "        layer.initParams()\n",
    "\n",
    "        \n",
    "        \n",
    "    def set_parametersW_b (self,numlayer,matX,matb):\n",
    "        self.layers[numlayer].parameters['W']=np.copy(matX)\n",
    "        self.layers[numlayer].parameters['b']=np.copy(matb)\n",
    "        \n",
    "        \n",
    "    def forward_propagation(self, X):\n",
    "        #Init predictive variables for the input layer\n",
    "        self.layers[0].setA(X)\n",
    "        \n",
    "        #Propagation for all the layers\n",
    "        for l in range(1, self.nbLayers + 1):\n",
    "            # Compute Z\n",
    "            self.layers[l].setZ(np.dot(self.layers[l].parameters['W'],\n",
    "                                       self.layers[l-1].parameters['A'])+self.layers[l].parameters['b'])\n",
    "            # Applying the activation function of the layer to Z\n",
    "            self.layers[l].setA(self.layers[l].activation_func(self.layers[l].parameters['Z']))\n",
    "            \n",
    "    \n",
    "    def cost_function(self,y):            \n",
    "        return (-(y*np.log(self.layers[self.nbLayers].parameters['A']+1e-8) + (1-y)*np.log( 1 - self.layers[self.nbLayers].parameters['A']+1e-8))).mean()\n",
    "    \n",
    "    def backward_propagation(self,y):\n",
    "        #calcul de dZ dW et db pour le dernier layer\n",
    "        self.layers[self.nbLayers].derivatives['dZ']=self.layers[self.nbLayers].parameters['A']-y\n",
    "        self.layers[self.nbLayers].derivatives['dW']=np.dot(self.layers[self.nbLayers].derivatives['dZ'],\n",
    "                                                             np.transpose(self.layers[self.nbLayers-1].parameters['A']))\n",
    "        m=self.layers[self.nbLayers].parameters['A'].shape[1]#égal au nombre de colonnes de A \n",
    "        self.layers[self.nbLayers].derivatives['db']=np.sum(self.layers[self.nbLayers].derivatives['dZ'], \n",
    "                                                       axis=1, keepdims=True) / m\n",
    "        \n",
    "        #calcul de dZ dW db pour les autres layers\n",
    "        for l in range(self.nbLayers-1,0,-1) :\n",
    "            self.layers[l].derivatives['dZ']=np.dot(np.transpose(self.layers[l+1].parameters['W']),\n",
    "                                            self.layers[l+1].derivatives['dZ'])*self.layers[l].backward_activation_func(self.layers[l].parameters[\"Z\"])\n",
    "            \n",
    "            self.layers[l].derivatives[\"dW\"]=np.dot(self.layers[l].derivatives['dZ'],\n",
    "                                            np.transpose(self.layers[l-1].parameters['A']))\n",
    "                       \n",
    "            m=self.layers[l-1].parameters['A'].shape[1]#égal au nombre de colonnes de A \n",
    "            self.layers[l].derivatives['db']=np.sum(self.layers[l].derivatives['dZ'], \n",
    "                                                       axis=1, keepdims=True) / m    \n",
    "            \n",
    "    def update_parameters(self, eta) :\n",
    "        for l in range(1,self.nbLayers+1) :\n",
    "            self.layers[l].parameters['W']-=eta*self.layers[l].derivatives['dW']\n",
    "            self.layers[l].parameters[\"b\"]-=eta*self.layers[l].derivatives[\"db\"]\n",
    "            \n",
    "    def convert_prob_into_class(self,probs):\n",
    "        probs = np.copy(probs)#pour ne pas perdre probs, i.e. y_hat\n",
    "        probs[probs > 0.5] = 1\n",
    "        probs[probs <= 0.5] = 0\n",
    "        return probs\n",
    "    \n",
    "    def plot_W_b_epoch (self,epoch,parameter_history):\n",
    "        mat=[]\n",
    "        max_size_layer=0\n",
    "        for l in range(1, self.nbLayers+1):    \n",
    "            value=parameter_history[epoch]['W'+str(l)]\n",
    "            if (parameter_history[epoch]['W'+str(l)].shape[1]>max_size_layer):\n",
    "                max_size_layer=parameter_history[epoch]['W'+str(l)].shape[1]  \n",
    "            mat.append(value)\n",
    "        figure=plt.figure(figsize=((self.nbLayers+1)*3,int (max_size_layer/2)))    \n",
    "        for nb_w in range (len(mat)):    \n",
    "                plt.subplot(1, len(mat), nb_w+1)\n",
    "                plt.matshow(mat[nb_w],cmap = plt.cm.gist_rainbow,fignum=False, aspect='auto')\n",
    "                plt.colorbar()    \n",
    "        thelegend=\"Epoch \"+str(epoch)\n",
    "        plt.title (thelegend)    \n",
    "\n",
    "    def accuracy(self,y_hat, y):\n",
    "        if self.layers[self.nbLayers].activation_func==softmax:\n",
    "            # si la fonction est softmax, les valeurs sont sur différentes dimensions\n",
    "            # il faut utiliser argmax avec axis=0 pour avoir un vecteur qui indique\n",
    "            # où est la valeur maximale à la fois pour y_hat et pour y\n",
    "            # comme cela il suffit de comparer les deux vecteurs qui indiquent \n",
    "            # dans quelle ligne se trouve le max\n",
    "            y_hat_encoded=np.copy(y_hat)\n",
    "            y_hat_encoded = np.argmax(y_hat_encoded, axis=0)\n",
    "            y_encoded=np.copy(y)\n",
    "            y_encoded=np.argmax(y_encoded, axis=0)\n",
    "            return (y_hat_encoded == y_encoded).mean()\n",
    "        # la dernière fonction d'activation n'est pas softmax.\n",
    "        # par exemple sigmoid pour une classification binaire\n",
    "        # il suffit de convertir la probabilité du résultat en classe\n",
    "        y_hat_ = self.convert_prob_into_class(y_hat)\n",
    "        return (y_hat_ == y).all(axis=0).mean()       \n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.forward_propagation(x)\n",
    "        return self.layers[self.nbLayers].parameters['A']\n",
    "    \n",
    "    def next_batch(self,X, y, batchsize):\n",
    "        # pour avoir X de la forme : 2 colonnes, m lignes (examples) et également y\n",
    "        # cela permet de trier les 2 tableaux avec un indices de permutation       \n",
    "        X=np.transpose(X)\n",
    "        y=np.transpose(y)\n",
    "        \n",
    "        m=len(y)\n",
    "        # permutation aléatoire de X et y pour faire des batchs avec des valeurs au hasard\n",
    "        indices = np.random.permutation(m)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "        for i in np.arange(0, X.shape[0], batchsize):\n",
    "            # creation des batchs de taille batchsize\n",
    "            yield (X[i:i + batchsize], y[i:i + batchsize])\n",
    "    def fit(self, X, y, *args,**kwargs):    \n",
    "        epochs=kwargs.get(\"epochs\",20)\n",
    "        verbose=kwargs.get(\"verbose\",False)\n",
    "        eta =kwargs.get(\"eta\",0.01)\n",
    "        batchsize=kwargs.get(\"batchsize\",32)\n",
    "    #def fit(self, X, y, epochs, eta = 0.01,batchsize=64) :\n",
    "        # sauvegarde historique coût et accuracy pour affichage\n",
    "        cost_history = []\n",
    "        accuracy_history = []\n",
    "        parameter_history = []\n",
    "        for i in range(epochs):\n",
    "            i+=1\n",
    "            # sauvegarde des coûts et accuracy par mini-batch\n",
    "            cost_batch = []\n",
    "            accuracy_batch = []\n",
    "            # Descente de gradient par mini-batch\n",
    "            for (batchX, batchy) in self.next_batch(X, y, batchsize):\n",
    "                # Extraction et traitement d'un batch à la fois\n",
    "                \n",
    "                # mise en place des données au bon format\n",
    "                batchX=np.transpose(batchX)\n",
    "                if self.layers[self.nbLayers].activation_func==softmax:\n",
    "                    # la classification n'est pas binaire, y a utilisé one-hot-encoder\n",
    "                    # le batchy doit donc être transposé et le résultat doit\n",
    "                    # être sous la forme d'une matrice de taille batchy.shape[1]\n",
    "                    \n",
    "                    batchy=np.transpose(batchy.reshape((batchy.shape[0], batchy.shape[1])))\n",
    "                else:\n",
    "                    # il s'agit d'une classification binaire donc shape[1] n'existe\n",
    "                    # pas\n",
    "                    batchy=np.transpose(batchy.reshape((batchy.shape[0], 1)))\n",
    "                #batchy=np.transpose(batchy.reshape((batchy.shape[0], 1)))\n",
    "                self.forward_propagation(batchX)\n",
    "                self.backward_propagation(batchy)\n",
    "                self.update_parameters(eta)\n",
    "                \n",
    "                # sauvegarde pour affichage\n",
    "                current_cost=self.cost_function(batchy)\n",
    "                cost_batch.append(current_cost)\n",
    "                y_hat = self.predict(batchX)\n",
    "                current_accuracy = self.accuracy(y_hat, batchy)\n",
    "                accuracy_batch.append(current_accuracy)\n",
    "               \n",
    "            # SaveStats on W, B as well as values for A,Z, W, b\n",
    "            save_values = {}\n",
    "            save_values[\"epoch\"]=i\n",
    "            for l in range(1, self.nbLayers+1):\n",
    "                save_values[\"layer\"+str(l)]=l\n",
    "                save_values[\"Wmean\"+ str(l)]=np.mean(self.layers[self.nbLayers].parameters['W'])\n",
    "                save_values[\"Wmax\"+ str(l)]=np.amax(self.layers[self.nbLayers].parameters['W'])\n",
    "                save_values[\"Wmin\"+str(l)]=np.amin(self.layers[self.nbLayers].parameters['W'])\n",
    "                save_values[\"Wstd\"+str(l)]=np.std(self.layers[self.nbLayers].parameters['W'])\n",
    "                save_values[\"bmean\"+ str(l)]=np.mean(self.layers[self.nbLayers].parameters['b'])\n",
    "                save_values[\"bmax\"+ str(l)]=np.amax(self.layers[self.nbLayers].parameters['b'])\n",
    "                save_values[\"bmin\"+str(l)]=np.amin(self.layers[self.nbLayers].parameters['b'])\n",
    "                save_values[\"bstd\"+str(l)]=np.std(self.layers[self.nbLayers].parameters['b'])\n",
    "                # be careful A,Z,W and b must be copied otherwise it is a referencee\n",
    "                save_values['A'+str(l)]=np.copy(self.layers[self.nbLayers].parameters['A'])\n",
    "                save_values['Z'+str(l)]=np.copy(self.layers[self.nbLayers].parameters['Z'])\n",
    "                save_values['W'+str(l)]=np.copy(self.layers[self.nbLayers].parameters['W'])\n",
    "                save_values['b'+str(l)]=np.copy(self.layers[self.nbLayers].parameters['b'])\n",
    "                \n",
    "            parameter_history.append(save_values)        \n",
    "            # sauvegarde de la valeur moyenne des coûts et de l'accuracy du batch pour affichage\n",
    "            current_cost=np.average(cost_batch)\n",
    "            cost_history.append(current_cost)\n",
    "            current_accuracy=np.average(accuracy_batch)\n",
    "            accuracy_history.append(current_accuracy)\n",
    "        \n",
    "            if(verbose == True):\n",
    "                print(\"Epoch : #%s/%s - %s/%s - cost : %.4f - accuracy : %.4f\"%(i,epochs,X.shape[1],X.shape[1], float(current_cost), current_accuracy))\n",
    "              \n",
    "        return self.layers, cost_history, accuracy_history, parameter_history\n",
    "    \n",
    "    #####################################################################################################################    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
