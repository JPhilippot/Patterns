{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.265565Z",
     "start_time": "2020-02-13T18:33:00.211348Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/cawosh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/cawosh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/cawosh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/cawosh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/cawosh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/cawosh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import keras\n",
    "import os\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.282036Z",
     "start_time": "2020-02-13T18:33:02.268265Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_goodXy (X,y):\n",
    "    ynew = model.predict_classes(X)\n",
    "    X_good =[]\n",
    "    y_good=[]\n",
    "    for i in range(len(X)):\n",
    "        if (ynew[i]==0 and y[i]==1) or (ynew[i]==1 and y[i]==0):\n",
    "            print (\"error prediction for X=%s, Predicted=%s, Real=%s\"% (X[i], ynew[i], y[i]))\n",
    "        else :\n",
    "            X_good.append(X[i])\n",
    "            y_good.append(y[i])\n",
    "    return X_good,y_good        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.299211Z",
     "start_time": "2020-02-13T18:33:02.285314Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_result_layers(model,X):\n",
    "    result_layers=[]\n",
    "    for i in range (len(model.layers)-1):\n",
    "        hidden_layers= keras.backend.function(\n",
    "                [model.layers[0].input],   \n",
    "                [model.layers[i].output,] \n",
    "                )    \n",
    "        result_layers.append(hidden_layers([X_good])[0])  \n",
    "    return result_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.324742Z",
     "start_time": "2020-02-13T18:33:02.303088Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_result_layers(filename,X,y,result_layers):\n",
    "    f = open(filename, \"w\")\n",
    "    for nb_X in range (len(X)):\n",
    "        #my_string=\"\"\n",
    "        my_string=str(y[nb_X])+','\n",
    "        for nb_layers in range (len(model.layers)-1):\n",
    "            my_string+=\"<b>,\"\n",
    "            for j in range (len(result_layers[nb_layers][nb_X])):\n",
    "                my_string+=str(result_layers[nb_layers][nb_X][j])+','\n",
    "            my_string+=\"</b>,\"    \n",
    "        my_string=my_string [0:-1]\n",
    "        my_string+='\\n'\n",
    "        f.write(my_string)    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:03.159174Z",
     "start_time": "2020-02-13T18:33:02.327414Z"
    }
   },
   "outputs": [],
   "source": [
    "url=\"./iris.csv\"\n",
    "names = ['SepalLengthCm', 'SepalWidthCm', \n",
    "         'PetalLengthCm', 'PetalWidthCm', \n",
    "         'Species']\n",
    "\n",
    "data = pd.read_csv(url, names=names)\n",
    "\n",
    "#Classification binaire sur Virginica et Setosa seulement\n",
    "data=data[data['Species'].isin(['Iris-virginica', 'Iris-setosa'])]\n",
    "\n",
    "i = 8\n",
    "data_to_predict = data[:i].reset_index(drop = True) \n",
    "\"\"\"\n",
    "reset_index() is a method to reset index of a Data Frame. \n",
    "reset_index() method sets a list of integer ranging from 0 to length of data as index. \n",
    "\"\"\"\n",
    "predict_species = data_to_predict.Species \n",
    "\"\"\" Species de la class Iris \"\"\"\n",
    "\n",
    "predict_species = np.array(predict_species) \n",
    "\"\"\"\n",
    "An array object satisfying the specified requirements.\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html\n",
    "\"\"\"\n",
    "\n",
    "prediction = np.array(data_to_predict.drop(['Species'],axis= 1))\n",
    "\n",
    "data = data[i:].reset_index(drop = True)\n",
    "\n",
    "X = data.drop(['Species'], axis = 1) \n",
    "\"\"\"\n",
    "The drop() function is used to drop specified labels from rows or columns.\n",
    "\"\"\"\n",
    "X = np.array(X)\n",
    "y = data['Species']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y=encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "train_X, test_X, train_y, test_y = model_selection.train_test_split(X,y,test_size = 0.1, random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.678441Z",
     "start_time": "2020-02-13T18:33:03.161667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.6723 - acc: 0.5610\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.5691 - acc: 0.9024\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 0s 828us/step - loss: 0.4720 - acc: 1.0000\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 0s 797us/step - loss: 0.3718 - acc: 1.0000\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 0s 922us/step - loss: 0.2729 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 0s 876us/step - loss: 0.1973 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 0s 994us/step - loss: 0.1419 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 0s 837us/step - loss: 0.1006 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 0s 778us/step - loss: 0.0686 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 0s 873us/step - loss: 0.0477 - acc: 1.0000\n",
      "10/10 [==============================] - 0s 5ms/step\n",
      "\n",
      "acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Utilisation de keras comme classifieur\n",
    "# mettre sigmoid comme fonction car binaire. Attention 1 seul neurone en sortie\n",
    "input_dim = len(data.columns) - 1\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\n",
    "\n",
    "model.fit(train_X, train_y, epochs = 10, batch_size = 2)\n",
    "\n",
    "scores = model.evaluate(test_X, test_y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.723085Z",
     "start_time": "2020-02-13T18:33:04.681566Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Récupération seulement des bons classés\n",
    "X_good,y_good=get_goodXy (train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.796700Z",
     "start_time": "2020-02-13T18:33:04.725841Z"
    }
   },
   "outputs": [],
   "source": [
    "# Récupération des valeurs de tous les layers sauf le dernier\n",
    "result_layers=get_result_layers(model,X_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.834457Z",
     "start_time": "2020-02-13T18:33:04.799432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sauvegarde du fichier\n",
    "# structure :\n",
    "# 0/1 = valeur de la classe\n",
    "# chaque valeur de layer est entourée par un []\n",
    "save_result_layers(\"iris_8_10_8_tmp\",X_good,y_good,result_layers)\n",
    "# tri du fichier\n",
    "os.system ('sort iris_8_10_8_tmp > iris_8_10_8_.csv')\n",
    "# effacer le fichier intermédiaire\n",
    "os.system ('rm iris_8_10_8_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_layers_from_csv(filename):\n",
    "    tokens=filename.split(\"_\")\n",
    "    df = pd.read_csv(filename, sep = ',', header = None) \n",
    "\n",
    "    \n",
    "    # creation d'un répertoire pour sauver tous les fichiers\n",
    "    repertoire=filename[0:-4]\n",
    "    os.makedirs(repertoire, exist_ok=True)\n",
    "    string = repertoire+'/'+tokens[0]+'_'\n",
    "    f=[]\n",
    "    filenames=[]\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        name_file=string+'l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'.csv'\n",
    "        f.append(open(name_file, \"w\"))\n",
    "        filenames.append(name_file)\n",
    "        \n",
    "        \n",
    "    # sauvegarde du dataframe dans une chaîne de caracteres\n",
    "    ch = df.to_string(header=False,\n",
    "                  index=False,\n",
    "                  index_names=False).split('\\n')\n",
    "    vals = [','.join(ele.split()) for ele in ch]\n",
    "    \n",
    "    # sauvegarde dans des fichiers spécifiques par layer\n",
    "    token_layer=[]\n",
    "    token_exemples=[]\n",
    "    for nb_exemples in range (len(vals)):\n",
    "        deb=str(df[0][nb_exemples])+','\n",
    "        # 1 ligne correspond à une chaine\n",
    "        s=vals[nb_exemples]\n",
    "        listoftokens=re.findall(r'<b>,(.+?),</b>', s)\n",
    "        nb_layers=len(listoftokens)\n",
    "        \n",
    "        for nb_token in range (nb_layers):\n",
    "            save_token=''\n",
    "            save_token=deb+str(listoftokens[nb_token])+'\\n'\n",
    "            \n",
    "            f[nb_token].write(save_token)\n",
    "\n",
    "    # sauvegarde d'un fichier qui contient tous les layers en une fois\n",
    "    # récupération des données pour enlever les <b> et </b>\n",
    "    df_all=pd.DataFrame()\n",
    "    myindex=0\n",
    "    for nb_columns in range(df.shape[1]):\n",
    "        df[nb_columns]=df[nb_columns].astype(str)\n",
    "        if (df[nb_columns]!='<b>').all() and (df[nb_columns]!='</b>').all():\n",
    "            df_all[myindex]=df[nb_columns]\n",
    "            myindex+=1\n",
    "\n",
    "    # construction du nom du fichier de sauvegarde\n",
    "    string = repertoire+'/'+tokens[0]+'_'\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        string+='l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'_'\n",
    "    string+='.csv'       \n",
    "    # sauvegarde en .csv\n",
    "    df_all.to_csv(string, sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a directory with a specific file for all the layers\n",
    "filename=\"iris_8_10_8_.csv\"    \n",
    "get_directory_layers_from_csv(filename)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from numpy import ndarray\n",
    "from matplotlib import colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def plot_clusters_3D (X,y_predict, nb_clusters, pca_done=False):\n",
    "    if pca_done==False:\n",
    "        pca = PCA(n_components=3) \n",
    "        pca.fit(X) \n",
    "        pca_data = pd.DataFrame(pca.transform(X))\n",
    "    else: pca_data = pd.DataFrame(X)\n",
    "    colors = list(zip(*sorted(( \n",
    "                    tuple(mcolors.rgb_to_hsv( \n",
    "                          mcolors.to_rgba(color)[:3])), name) \n",
    "                     for name, color in dict( \n",
    "                            mcolors.BASE_COLORS, **mcolors.CSS4_COLORS \n",
    "                                                      ).items())))[1] \n",
    "    # number of steps to taken generate n(clusters) colors\n",
    "    skips = math.floor(len(colors[5 : -5])/nb_clusters) \n",
    "    cluster_colors = colors[5 : -5 : skips] \n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax = fig.add_subplot(111, projection = '3d') \n",
    "    ax.scatter(pca_data[0], pca_data[1], pca_data[2],\n",
    "           c = list(map(lambda label : cluster_colors[label], \n",
    "                                            y_predict))) \n",
    "\n",
    "    str_labels = list(map(lambda label:'% s' % label, y_predict)) \n",
    "\n",
    "    list(map(lambda data1, data2, data3, str_label: \n",
    "        ax.text(data1, data2, data3, s = str_label, size = 16.5, \n",
    "        zorder = 20, color = 'k'), pca_data[0], pca_data[1], \n",
    "        pca_data[2], str_labels)) \n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def plot_clusters_2D (X,y_predict, nb_clusters, pca_done=False):\n",
    "    if pca_done==False:\n",
    "        pca = PCA(n_components=2) \n",
    "        X_r = pca.fit(X).transform(X)\n",
    "    else: X_r = X\n",
    "    data = pd.DataFrame(X_r, columns=['x','y'])\n",
    "    data['label']=y_predict\n",
    "    list_clusters=list(set(y_predict))\n",
    "    print (list_clusters)\n",
    "    #create a new figure\n",
    "    plt.figure(figsize=(5,5))\n",
    "\n",
    "    #loop through labels and plot each cluster\n",
    "    for i, label in enumerate(list_clusters):\n",
    "\n",
    "        #add data points \n",
    "        plt.scatter(x=data.loc[data['label']==label, 'x'], \n",
    "                y=data.loc[data['label']==label,'y'], \n",
    "                color=color[i], \n",
    "                alpha=0.20)\n",
    "\n",
    "        #add label\n",
    "        plt.annotate(label, \n",
    "                 data.loc[data['label']==label,['x','y']].mean(),\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 size=20, weight='bold',\n",
    "                 color=color[i])\n",
    "        \n",
    "def get_directory_layers_from_csv(filename):\n",
    "    tokens=filename.split(\"_\")\n",
    "    df = pd.read_csv(filename, sep = ',', header = None) \n",
    "\n",
    "    \n",
    "    # creation d'un répertoire pour sauver tous les fichiers\n",
    "    repertoire=filename[0:-4]\n",
    "    os.makedirs(repertoire, exist_ok=True)\n",
    "    string = repertoire+'/'+tokens[0]+'_'\n",
    "    f=[]\n",
    "    filenames=[]\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        name_file=string+'l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'.csv'\n",
    "        f.append(open(name_file, \"w\"))\n",
    "        filenames.append(name_file)\n",
    "        \n",
    "        \n",
    "    # sauvegarde du dataframe dans une chaîne de caracteres\n",
    "    ch = df.to_string(header=False,\n",
    "                  index=False,\n",
    "                  index_names=False).split('\\n')\n",
    "    vals = [','.join(ele.split()) for ele in ch]\n",
    "    \n",
    "    # sauvegarde dans des fichiers spécifiques par layer\n",
    "    token_layer=[]\n",
    "    token_exemples=[]\n",
    "    for nb_exemples in range (len(vals)):\n",
    "        deb=str(df[0][nb_exemples])+','\n",
    "        # 1 ligne correspond à une chaine\n",
    "        s=vals[nb_exemples]\n",
    "        listoftokens=re.findall(r'<b>,(.+?),</b>', s)\n",
    "        nb_layers=len(listoftokens)\n",
    "        \n",
    "        for nb_token in range (nb_layers):\n",
    "            save_token=''\n",
    "            save_token=deb+str(listoftokens[nb_token])+'\\n'\n",
    "            \n",
    "            f[nb_token].write(save_token)\n",
    "\n",
    "    # sauvegarde d'un fichier qui contient tous les layers en une fois\n",
    "    # récupération des données pour enlever les <b> et </b>\n",
    "    df_all=pd.DataFrame()\n",
    "    myindex=0\n",
    "    for nb_columns in range(df.shape[1]):\n",
    "        df[nb_columns]=df[nb_columns].astype(str)\n",
    "        if (df[nb_columns]!='<b>').all() and (df[nb_columns]!='</b>').all():\n",
    "            df_all[myindex]=df[nb_columns]\n",
    "            myindex+=1\n",
    "\n",
    "    # construction du nom du fichier de sauvegarde\n",
    "    string = repertoire+'/'+tokens[0]+'_'\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        string+='l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'_'\n",
    "    string+='.csv'       \n",
    "    # sauvegarde en .csv\n",
    "    df_all.to_csv(string, sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a directory with a specific file for all the layers\n",
    "filename=\"iris_8_10_8_.csv\"    \n",
    "get_directory_layers_from_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_cluster(clusters,numLayer,cluster_class):\n",
    "\n",
    "    idClusters=np.unique(clusters)\n",
    "    tabCount={}\n",
    "    for id in idClusters:\n",
    "        count = 0\n",
    "        for j in range(len(clusters)):\n",
    "            if clusters[j]==id:\n",
    "                tabCount[id]=count+1\n",
    "                count+=1\n",
    "    print(tabCount)\n",
    "    my_string=\"\"\n",
    "    for key,value in tabCount.items():\n",
    "        my_string += str(numLayer)+','+str(cluster_class)+','\n",
    "        my_string+=str(value)+','+str(key)+'\\n'\n",
    "    \n",
    "    return my_string\n",
    "\n",
    "def save_result_cluster(filename,C1,C2,C3):\n",
    "    f = open(filename, \"w\")\n",
    "    \n",
    "    f.write(\"Layer,Class,Count,Cluster\\n\")\n",
    "    f.write(str(C1))\n",
    "    f.write(str(C2))\n",
    "    f.write(str(C3))\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "def clusterizeLayer(url, nbClusters=4):\n",
    "    data = pd.read_csv(url,names=names)\n",
    "    dataC1=[]\n",
    "    dataC2=[]\n",
    "    indexC2=0\n",
    "    indexC1=0\n",
    "    i=0 \n",
    "    for index, row in data.iterrows():\n",
    "        \n",
    "        #print('oui: \\n',index[0])\n",
    "        if index[0]==0:\n",
    "            dataC1.append(row)\n",
    "            indexC1=i\n",
    "        if index[0]==1:\n",
    "            dataC2.append(row)\n",
    "            indexC2=i\n",
    "        i+=1\n",
    "        \n",
    "#     with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#         print(data)\n",
    "\n",
    "    algo = KMeans(n_clusters = nbClusters)\n",
    "    #fit_predict fait tourner l'algo et retourne y où y[i] est l'indice du cluster auquel dataC12[i] appartient\n",
    "    clustersC1C2 = algo.fit_predict(data)\n",
    "    clustersC1=[]\n",
    "    clustersC2=[]\n",
    "#     print(\"i1: \\n\",indexC1,\"\\n i2: \\n\",indexC2)\n",
    "    for i in range(len(clustersC1C2)):\n",
    "        if i<indexC1:\n",
    "            clustersC1.append(clustersC1C2[i])\n",
    "        else:\n",
    "            clustersC2.append(clustersC1C2[i])\n",
    "#     print(\"CC1  : \\n\",clustersC1)\n",
    "#     print(\"CC2  : \\n\",clustersC2)\n",
    "#     print(\"CC1-2  : \\n\",clustersC1C2)\n",
    "\n",
    "    dataC3=pd.read_csv('iris.csv',names=names)\n",
    "    dataC3=dataC3[dataC3['Species'].isin(['Iris-versicolor'])]\n",
    "    dataC3['Species']=1.0\n",
    "\n",
    "#     print(\"print dataC3:\\n\")\n",
    "#     print(dataC3)\n",
    "\n",
    "    #predict utilise les clusters calculés par fit_predict(dataC1C2) et attribue chaque ligne de dataC3 au cluster dont le centre\n",
    "    #minimise la distance\n",
    "    C3ClusterPred = algo.predict(dataC3)\n",
    "\n",
    "#     print(\"Clusters C3:\\n\")\n",
    "#     print(C3ClusterPred)\n",
    "#     print(\"Clusters C1C2 :\\n\")\n",
    "#     print(clustersC1C2)\n",
    "    return (C3ClusterPred,clustersC1,clustersC2)#,clustersC1,clustersC2#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 19, 2: 16}\n",
      "{1: 9, 3: 26}\n",
      "{1: 24, 3: 11}\n",
      "{0: 1, 1: 28, 3: 18}\n",
      "{0: 21, 1: 1, 2: 25}\n",
      "{0: 28, 2: 18, 3: 1}\n",
      "{1: 5, 3: 45}\n",
      "{1: 50}\n",
      "{0: 27, 3: 23}\n"
     ]
    }
   ],
   "source": [
    "clustersC3l1,clustersC1l1,clustersC2l1=clusterizeLayer('./iris_8_10_8_/iris_l1_8.csv')\n",
    "clustersC3l2,clustersC1l2,clustersC2l2=clusterizeLayer('./iris_8_10_8_/iris_l2_10.csv')\n",
    "clustersC3l3,clustersC1l3,clustersC2l3=clusterizeLayer('./iris_8_10_8_/iris_l3_8.csv')\n",
    "\n",
    "C1l1=get_result_cluster(clustersC1l1,1,1)\n",
    "C1l2=get_result_cluster(clustersC1l2,2,1)\n",
    "C1l3=get_result_cluster(clustersC1l3,3,1)\n",
    "\n",
    "C2l1=get_result_cluster(clustersC2l1,1,2)\n",
    "C2l2=get_result_cluster(clustersC2l2,2,2)\n",
    "C2l3=get_result_cluster(clustersC2l3,3,2)\n",
    "\n",
    "C3l1=get_result_cluster(clustersC3l1,1,3)\n",
    "C3l2=get_result_cluster(clustersC3l2,2,3)\n",
    "C3l3=get_result_cluster(clustersC3l3,3,3)\n",
    "\n",
    "save_result_cluster(\"./iris_8_10_8_/iris_clusters_layer1.csv\",C1l1,C2l1,C3l1)\n",
    "save_result_cluster(\"./iris_8_10_8_/iris_clusters_layer2.csv\",C1l2,C2l2,C3l2)\n",
    "save_result_cluster(\"./iris_8_10_8_/iris_clusters_layer3.csv\",C1l3,C2l3,C3l3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1:\n",
      "    Layer  Class  Count  Cluster\n",
      "0      1      1     19        0\n",
      "1      1      1     16        2\n",
      "2      1      2      1        0\n",
      "3      1      2     28        1\n",
      "4      1      2     18        3\n",
      "5      1      3      5        1\n",
      "6      1      3     45        3\n",
      "léklass:\n",
      " [19, 16] [1, 28, 18] [5, 45]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: objects cannot be broadcast to a single shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-370c1735b2c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# plt.ylabel('Count')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cawosh/.local/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mbar\u001b[0;34m(x, height, width, bottom, align, data, **kwargs)\u001b[0m\n\u001b[1;32m   2407\u001b[0m     return gca().bar(\n\u001b[1;32m   2408\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2409\u001b[0;31m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cawosh/.local/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cawosh/.local/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mbar\u001b[0;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[1;32m   2339\u001b[0m         x, height, width, y, linewidth = np.broadcast_arrays(\n\u001b[1;32m   2340\u001b[0m             \u001b[0;31m# Make args iterable too.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2341\u001b[0;31m             np.atleast_1d(x), height, width, y, linewidth)\n\u001b[0m\u001b[1;32m   2342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m         \u001b[0;31m# Now that units have been converted, set the tick locations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mbroadcast_arrays\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/home/cawosh/.local/lib/python3.6/site-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36mbroadcast_arrays\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubok\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_broadcast_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cawosh/.local/lib/python3.6/site-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36m_broadcast_shape\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;31m# use the old-iterator because np.nditer does not handle size 0 arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;31m# consistently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m     \u001b[0;31m# unfortunately, it cannot handle 32 or more arguments directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: objects cannot be broadcast to a single shape"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer1 = pd.read_csv(\"./iris_8_10_8_/iris_clusters_layer1.csv\")\n",
    "print(\"layer1:\\n\",layer1)\n",
    "\n",
    "class1=[]\n",
    "class2=[]\n",
    "class3=[]\n",
    "for index,element in layer1.iterrows():\n",
    "    if element[1]==1:\n",
    "        if element[\"Count\"] >= 0:\n",
    "            class1.append(element[\"Count\"])\n",
    "        else:\n",
    "            class1.append(0)\n",
    "    if element[1]==2:\n",
    "        if element[\"Count\"] >= 0:\n",
    "            class2.append(element[\"Count\"])\n",
    "        else:\n",
    "            class2.append(0)\n",
    "    if element[1]==3:\n",
    "        if element[\"Count\"] >= 0:\n",
    "            class3.append(element[\"Count\"])\n",
    "        else:\n",
    "            class3.append(0)\n",
    "print(\"léklass:\\n\",class1,class2,class3)\n",
    "names = ['cluster1c1', 'cluster1c2', 'cluster1c3' , \n",
    "         'cluster2c1', 'cluster2c2', 'cluster2c3',\n",
    "         'cluster3c1', 'cluster3c2', 'cluster3c3',\n",
    "         'cluster4c1', 'cluster4c2', 'cluster4c3']\n",
    "\n",
    "plt.hist([class1,class2,class3],\n",
    "         density=True,\n",
    "         histtype='bar',\n",
    "        label=['class1','class2','class3'])\n",
    "plt.xlabel('Clusters')\n",
    "# plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.bar(names,[class1,class2,class3])\n",
    "plt.show()\n",
    "\n",
    "color = ['blue', 'red', 'yellow', 'green']\n",
    "#plot_clusters_2D(clustersC1C2l1 , clustersC3l1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "from numpy import load\n",
    "\n",
    "data = load('mnist.npz')\n",
    "lst = data.files\n",
    "print(lst)\n",
    "\n",
    "(X_train,y_train) = (data[lst[1]],data[lst[2]])\n",
    "(X_test,y_test) = (data[lst[0]],data[lst[3]])\n",
    "\n",
    "X_train_sample=X_train[0:100]\n",
    "y_train_sample=y_train[0:100]\n",
    "\n",
    "X_train=X_train_sample\n",
    "y_train=y_train_sample\n",
    "X_train = X_train.reshape(100, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "\n",
    "X_01=[]\n",
    "y_01=[]\n",
    "nb_X=0\n",
    "for i in range(X_train.shape[0]):\n",
    "    if (y_train[i]==0 or y_train[i]==1):\n",
    "        \n",
    "        nb_X+=1\n",
    "        X_01.append(X_train[i])\n",
    "        y_01.append(y_train[i])\n",
    "\n",
    "       \n",
    "train_X=np.asarray(X_01)\n",
    "\n",
    "train_y=y_01\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "train_y=encoder.fit_transform(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\n",
    "\n",
    "model.fit(train_X, train_y, epochs = 40, batch_size = 32)\n",
    "\n",
    "\n",
    "X_good,y_good=get_goodXy (train_X, train_y)\n",
    "\n",
    "\n",
    "# Récupération des valeurs de tous les layers sauf le dernier\n",
    "result_layers=get_result_layers(model,X_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du fichier\n",
    "# structure :\n",
    "# 0/1 = valeur de la classe\n",
    "# chaque valeur de layer est entourée par une étoile *\n",
    "save_result_layers(\"mnist_512_tmp\",X_good,y_good,result_layers)\n",
    "# tri du fichier\n",
    "os.system ('sort mnist_512_tmp > mnist_512_.csv')\n",
    "# effacer le fichier intermédiaire\n",
    "os.system ('rm mnist_512_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalise = True\n",
    "includeClasses = True\n",
    "nbClusters = 10\n",
    "\n",
    "data = load('mnist.npz')\n",
    "lst = data.files\n",
    "\n",
    "print(data[lst[0]][0]) #X_test\n",
    "print(data[lst[1]][0]) #X_train\n",
    "print(data[lst[2]][0]) #y_train\n",
    "print(data[lst[3]][0]) #y_test\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#On rajoute une colonne Classe qui contient le numéro de la classe correspondant au feature Species\n",
    "listeSpecies = pd.unique(data['Species'])\n",
    "listeIndices = [i+1 for i in range(len(listeSpecies))]\n",
    "dico = dict(zip(listeSpecies, listeIndices))\n",
    "data.insert(len(data.columns), 'Classe', [dico[x] for x in data['Species']])\n",
    "\n",
    "if normalise:\n",
    "    if includeClasses:\n",
    "        m = data.columns[data.columns!='Species']\n",
    "    else:\n",
    "        #On veut garder la séparation en classes 1, 2 et 3, on exclut donc cette colonne de la normalisation\n",
    "        m = data.columns[~data.columns.isin(['Species','Classe'])]\n",
    "    data[m] = (data[m]-data[m].min())/(data[m].max()-data[m].min())\n",
    "\n",
    "#Note: dataC3 et dataC1C2 sont des copies profondes\n",
    "dataC1C2 = data[data['Species'].isin(['Iris-virginica','Iris-setosa'])].drop('Species', axis=1)\n",
    "dataC3 = data[data['Species']=='Iris-versicolor'].drop('Species', axis=1)\n",
    "\n",
    "\n",
    "algo = KMeans(n_clusters = nbClusters)\n",
    "#fit_predict fait tourner l'algo et retourne y où y[i] est l'indice du cluster auquel dataC12[i] appartient\n",
    "clustersC1C2 = algo.fit_predict(dataC1C2)\n",
    "\n",
    "#predict utilise les clusters calculés par fit_predict(dataC12) et attribue chaque ligne de dataC3 au cluster dont le centre\n",
    "#minimise la distance\n",
    "C3ClusterPred = algo.predict(dataC3)\n",
    "\n",
    "#Les Species sont-elles bien discrimninées par l'algo de clustering?\n",
    "clusters = [[] for _ in range(1+max(clustersC1C2))]\n",
    "for i,x in enumerate(clustersC1C2):\n",
    "    clusters[x].append(i)\n",
    "taillesClusters = [len(l) for l in clusters]\n",
    "print(\"Tailles des clusters non vides\")\n",
    "print(taillesClusters)\n",
    "\n",
    "for cluster in clusters:\n",
    "    if not(cluster):\n",
    "        continue\n",
    "    classe = data.at[cluster[0],'Species']\n",
    "    for sign in cluster:\n",
    "        if data.at[sign, 'Species'] != classe:\n",
    "            print(\"Présence classe mixte\")\n",
    "\n",
    "#Predict sur la classe 3\n",
    "X = dataC3.drop(columns= ['Classe'])\n",
    "X = np.array(X)\n",
    "Y = model.predict(X)\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "Y = sigmoid(Y)\n",
    "print(Y)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
