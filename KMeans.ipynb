{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.265565Z",
     "start_time": "2020-02-13T18:33:00.211348Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import keras\n",
    "import os\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.282036Z",
     "start_time": "2020-02-13T18:33:02.268265Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_goodXy (X,y):\n",
    "    ynew = model.predict_classes(X)\n",
    "    X_good =[]\n",
    "    y_good=[]\n",
    "    for i in range(len(X)):\n",
    "        if (ynew[i]==0 and y[i]==1) or (ynew[i]==1 and y[i]==0):\n",
    "            print (\"error prediction for X=%s, Predicted=%s, Real=%s\"% (X[i], ynew[i], y[i]))\n",
    "        else :\n",
    "            X_good.append(X[i])\n",
    "            y_good.append(y[i])\n",
    "    return X_good,y_good        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.299211Z",
     "start_time": "2020-02-13T18:33:02.285314Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_result_layers(model,X):\n",
    "    result_layers=[]\n",
    "    for i in range (len(model.layers)-1):\n",
    "        hidden_layers= keras.backend.function(\n",
    "                [model.layers[0].input],   \n",
    "                [model.layers[i].output,] \n",
    "                )    \n",
    "        result_layers.append(hidden_layers([X_good])[0])  \n",
    "    return result_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.324742Z",
     "start_time": "2020-02-13T18:33:02.303088Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_result_layers(filename,X,y,result_layers):\n",
    "    f = open(filename, \"w\")\n",
    "    for nb_X in range (len(X)):\n",
    "        #my_string=\"\"\n",
    "        my_string=str(y[nb_X])+','\n",
    "        for nb_layers in range (len(model.layers)-1):\n",
    "            my_string+=\"<b>,\"\n",
    "            for j in range (len(result_layers[nb_layers][nb_X])):\n",
    "                my_string+=str(result_layers[nb_layers][nb_X][j])+','\n",
    "            my_string+=\"</b>,\"    \n",
    "        my_string=my_string [0:-1]\n",
    "        my_string+='\\n'\n",
    "        f.write(my_string)    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:03.159174Z",
     "start_time": "2020-02-13T18:33:02.327414Z"
    }
   },
   "outputs": [],
   "source": [
    "url=\"./iris.csv\"\n",
    "names = ['SepalLengthCm', 'SepalWidthCm', \n",
    "         'PetalLengthCm', 'PetalWidthCm', \n",
    "         'Species']\n",
    "\n",
    "data = pd.read_csv(url, names=names)\n",
    "\n",
    "#Classification binaire sur Virginica et Setosa seulement\n",
    "data=data[data['Species'].isin(['Iris-virginica', 'Iris-setosa'])]\n",
    "\n",
    "i = 8\n",
    "data_to_predict = data[:i].reset_index(drop = True) \n",
    "\"\"\"\n",
    "reset_index() is a method to reset index of a Data Frame. \n",
    "reset_index() method sets a list of integer ranging from 0 to length of data as index. \n",
    "\"\"\"\n",
    "predict_species = data_to_predict.Species \n",
    "\"\"\" Species de la class Iris \"\"\"\n",
    "\n",
    "predict_species = np.array(predict_species) \n",
    "\"\"\"\n",
    "An array object satisfying the specified requirements.\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html\n",
    "\"\"\"\n",
    "\n",
    "prediction = np.array(data_to_predict.drop(['Species'],axis= 1))\n",
    "\n",
    "data = data[i:].reset_index(drop = True)\n",
    "\n",
    "X = data.drop(['Species'], axis = 1) \n",
    "\"\"\"\n",
    "The drop() function is used to drop specified labels from rows or columns.\n",
    "\"\"\"\n",
    "X = np.array(X)\n",
    "y = data['Species']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y=encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "train_X, test_X, train_y, test_y = model_selection.train_test_split(X,y,test_size = 0.1, random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.678441Z",
     "start_time": "2020-02-13T18:33:03.161667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.6871 - acc: 0.6341 \n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 0s 506us/step - loss: 0.5788 - acc: 1.0000\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 0s 612us/step - loss: 0.4625 - acc: 1.0000\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 0s 790us/step - loss: 0.3356 - acc: 1.0000\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 0s 768us/step - loss: 0.2201 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 0s 561us/step - loss: 0.1347 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 0s 636us/step - loss: 0.0821 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 0s 573us/step - loss: 0.0507 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 0s 663us/step - loss: 0.0342 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 0s 553us/step - loss: 0.0238 - acc: 1.0000\n",
      "10/10 [==============================] - 0s 5ms/step\n",
      "\n",
      "acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Utilisation de keras comme classifieur\n",
    "# mettre sigmoid comme fonction car binaire. Attention 1 seul neurone en sortie\n",
    "input_dim = len(data.columns) - 1\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\n",
    "\n",
    "model.fit(train_X, train_y, epochs = 10, batch_size = 2)\n",
    "\n",
    "scores = model.evaluate(test_X, test_y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.723085Z",
     "start_time": "2020-02-13T18:33:04.681566Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Récupération seulement des bons classés\n",
    "X_good,y_good=get_goodXy (train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.796700Z",
     "start_time": "2020-02-13T18:33:04.725841Z"
    }
   },
   "outputs": [],
   "source": [
    "# Récupération des valeurs de tous les layers sauf le dernier\n",
    "result_layers=get_result_layers(model,X_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.834457Z",
     "start_time": "2020-02-13T18:33:04.799432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sauvegarde du fichier\n",
    "# structure :\n",
    "# 0/1 = valeur de la classe\n",
    "# chaque valeur de layer est entourée par un []\n",
    "save_result_layers(\"iris_8_10_8_tmp\",X_good,y_good,result_layers)\n",
    "# tri du fichier\n",
    "os.system ('sort iris_8_10_8_tmp > iris_8_10_8_.csv')\n",
    "# effacer le fichier intermédiaire\n",
    "os.system ('rm iris_8_10_8_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_layers_from_csv(filename):\n",
    "    tokens=filename.split(\"_\")\n",
    "    df = pd.read_csv(filename, sep = ',', header = None) \n",
    "\n",
    "    \n",
    "    # creation d'un répertoire pour sauver tous les fichiers\n",
    "    repertoire=filename[0:-4]\n",
    "    os.makedirs(repertoire, exist_ok=True)\n",
    "    string = repertoire+'/'+tokens[0]+'_'\n",
    "    f=[]\n",
    "    filenames=[]\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        name_file=string+'l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'.csv'\n",
    "        f.append(open(name_file, \"w\"))\n",
    "        filenames.append(name_file)\n",
    "        \n",
    "        \n",
    "    # sauvegarde du dataframe dans une chaîne de caracteres\n",
    "    ch = df.to_string(header=False,\n",
    "                  index=False,\n",
    "                  index_names=False).split('\\n')\n",
    "    vals = [','.join(ele.split()) for ele in ch]\n",
    "    \n",
    "    # sauvegarde dans des fichiers spécifiques par layer\n",
    "    token_layer=[]\n",
    "    token_exemples=[]\n",
    "    for nb_exemples in range (len(vals)):\n",
    "        deb=str(df[0][nb_exemples])+','\n",
    "        # 1 ligne correspond à une chaine\n",
    "        s=vals[nb_exemples]\n",
    "        listoftokens=re.findall(r'<b>,(.+?),</b>', s)\n",
    "        nb_layers=len(listoftokens)\n",
    "        \n",
    "        for nb_token in range (nb_layers):\n",
    "            save_token=''\n",
    "            save_token=deb+str(listoftokens[nb_token])+'\\n'\n",
    "            \n",
    "            f[nb_token].write(save_token)\n",
    "\n",
    "    # sauvegarde d'un fichier qui contient tous les layers en une fois\n",
    "    # récupération des données pour enlever les <b> et </b>\n",
    "    df_all=pd.DataFrame()\n",
    "    myindex=0\n",
    "    for nb_columns in range(df.shape[1]):\n",
    "        df[nb_columns]=df[nb_columns].astype(str)\n",
    "        if (df[nb_columns]!='<b>').all() and (df[nb_columns]!='</b>').all():\n",
    "            df_all[myindex]=df[nb_columns]\n",
    "            myindex+=1\n",
    "\n",
    "    # construction du nom du fichier de sauvegarde\n",
    "    string = repertoire+'/'+tokens[0]+'_'\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        string+='l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'_'\n",
    "    string+='.csv'       \n",
    "    # sauvegarde en .csv\n",
    "    df_all.to_csv(string, sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a directory with a specific file for all the layers\n",
    "filename=\"iris_8_10_8_.csv\"    \n",
    "get_directory_layers_from_csv(filename)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from numpy import ndarray\n",
    "from matplotlib import colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def plot_clusters_3D (X,y_predict, nb_clusters, pca_done=False):\n",
    "    if pca_done==False:\n",
    "        pca = PCA(n_components=3) \n",
    "        pca.fit(X) \n",
    "        pca_data = pd.DataFrame(pca.transform(X))\n",
    "    else: pca_data = pd.DataFrame(X)\n",
    "    colors = list(zip(*sorted(( \n",
    "                    tuple(mcolors.rgb_to_hsv( \n",
    "                          mcolors.to_rgba(color)[:3])), name) \n",
    "                     for name, color in dict( \n",
    "                            mcolors.BASE_COLORS, **mcolors.CSS4_COLORS \n",
    "                                                      ).items())))[1] \n",
    "    # number of steps to taken generate n(clusters) colors\n",
    "    skips = math.floor(len(colors[5 : -5])/nb_clusters) \n",
    "    cluster_colors = colors[5 : -5 : skips] \n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax = fig.add_subplot(111, projection = '3d') \n",
    "    ax.scatter(pca_data[0], pca_data[1], pca_data[2],\n",
    "           c = list(map(lambda label : cluster_colors[label], \n",
    "                                            y_predict))) \n",
    "\n",
    "    str_labels = list(map(lambda label:'% s' % label, y_predict)) \n",
    "\n",
    "    list(map(lambda data1, data2, data3, str_label: \n",
    "        ax.text(data1, data2, data3, s = str_label, size = 16.5, \n",
    "        zorder = 20, color = 'k'), pca_data[0], pca_data[1], \n",
    "        pca_data[2], str_labels)) \n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def plot_clusters_2D (X,y_predict, nb_clusters, pca_done=False):\n",
    "    if pca_done==False:\n",
    "        pca = PCA(n_components=2) \n",
    "        X_r = pca.fit(X).transform(X)\n",
    "    else: X_r = X\n",
    "    data = pd.DataFrame(X_r, columns=['x','y'])\n",
    "    data['label']=y_predict\n",
    "    list_clusters=list(set(y_predict))\n",
    "    print (list_clusters)\n",
    "    #create a new figure\n",
    "    plt.figure(figsize=(5,5))\n",
    "\n",
    "    #loop through labels and plot each cluster\n",
    "    for i, label in enumerate(list_clusters):\n",
    "\n",
    "        #add data points \n",
    "        plt.scatter(x=data.loc[data['label']==label, 'x'], \n",
    "                y=data.loc[data['label']==label,'y'], \n",
    "                color=color[i], \n",
    "                alpha=0.20)\n",
    "\n",
    "        #add label\n",
    "        plt.annotate(label, \n",
    "                 data.loc[data['label']==label,['x','y']].mean(),\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 size=20, weight='bold',\n",
    "                 color=color[i])\n",
    "        \n",
    "def get_directory_layers_from_csv(filename):\n",
    "    tokens=filename.split(\"_\")\n",
    "    df = pd.read_csv(filename, sep = ',', header = None) \n",
    "\n",
    "    \n",
    "    # creation d'un répertoire pour sauver tous les fichiers\n",
    "    repertoire=filename[0:-4]\n",
    "    os.makedirs(repertoire, exist_ok=True)\n",
    "    string = repertoire+'/'+tokens[0]+'_'\n",
    "    f=[]\n",
    "    filenames=[]\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        name_file=string+'l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'.csv'\n",
    "        f.append(open(name_file, \"w\"))\n",
    "        filenames.append(name_file)\n",
    "        \n",
    "        \n",
    "    # sauvegarde du dataframe dans une chaîne de caracteres\n",
    "    ch = df.to_string(header=False,\n",
    "                  index=False,\n",
    "                  index_names=False).split('\\n')\n",
    "    vals = [','.join(ele.split()) for ele in ch]\n",
    "    \n",
    "    # sauvegarde dans des fichiers spécifiques par layer\n",
    "    token_layer=[]\n",
    "    token_exemples=[]\n",
    "    for nb_exemples in range (len(vals)):\n",
    "        deb=str(df[0][nb_exemples])+','\n",
    "        # 1 ligne correspond à une chaine\n",
    "        s=vals[nb_exemples]\n",
    "        listoftokens=re.findall(r'<b>,(.+?),</b>', s)\n",
    "        nb_layers=len(listoftokens)\n",
    "        \n",
    "        for nb_token in range (nb_layers):\n",
    "            save_token=''\n",
    "            save_token=deb+str(listoftokens[nb_token])+'\\n'\n",
    "            \n",
    "            f[nb_token].write(save_token)\n",
    "\n",
    "    # sauvegarde d'un fichier qui contient tous les layers en une fois\n",
    "    # récupération des données pour enlever les <b> et </b>\n",
    "    df_all=pd.DataFrame()\n",
    "    myindex=0\n",
    "    for nb_columns in range(df.shape[1]):\n",
    "        df[nb_columns]=df[nb_columns].astype(str)\n",
    "        if (df[nb_columns]!='<b>').all() and (df[nb_columns]!='</b>').all():\n",
    "            df_all[myindex]=df[nb_columns]\n",
    "            myindex+=1\n",
    "\n",
    "    # construction du nom du fichier de sauvegarde\n",
    "    string = repertoire+'/'+tokens[0]+'_'\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        string+='l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'_'\n",
    "    string+='.csv'       \n",
    "    # sauvegarde en .csv\n",
    "    df_all.to_csv(string, sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a directory with a specific file for all the layers\n",
    "filename=\"iris_8_10_8_.csv\"    \n",
    "get_directory_layers_from_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_cluster(clusters,numLayer,cluster_class):\n",
    "\n",
    "    idClusters=np.unique(clusters)\n",
    "    tabCount={}\n",
    "    for id in idClusters:\n",
    "        count = 0\n",
    "        for j in range(len(clusters)):\n",
    "            if clusters[j]==id:\n",
    "                tabCount[id]=count+1\n",
    "                count+=1\n",
    "    print(tabCount)\n",
    "    my_string=\"\"\n",
    "    for key,value in tabCount.items():\n",
    "        my_string += str(numLayer)+','+str(cluster_class)+','\n",
    "        my_string+=str(value)+','+str(key)+'\\n'\n",
    "    \n",
    "    return my_string\n",
    "\n",
    "def save_result_cluster(filename,C1,C2,C3):\n",
    "    f = open(filename, \"w\")\n",
    "    \n",
    "    f.write(\"Layer,Class,Count,Cluster\\n\")\n",
    "    f.write(str(C1))\n",
    "    f.write(str(C2))\n",
    "    f.write(str(C3))\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "def clusterizeLayer(url, nbClusters=4):\n",
    "    data = pd.read_csv(url,names=names)\n",
    "    dataC1=[]\n",
    "    dataC2=[]\n",
    "    indexC2=0\n",
    "    indexC1=0\n",
    "    i=0 \n",
    "    for index, row in data.iterrows():\n",
    "        \n",
    "        #print('oui: \\n',index[0])\n",
    "        if index[0]==0:\n",
    "            dataC1.append(row)\n",
    "            indexC1=i\n",
    "        if index[0]==1:\n",
    "            dataC2.append(row)\n",
    "            indexC2=i\n",
    "        i+=1\n",
    "        \n",
    "#     with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#         print(data)\n",
    "\n",
    "    algo = KMeans(n_clusters = nbClusters)\n",
    "    #fit_predict fait tourner l'algo et retourne y où y[i] est l'indice du cluster auquel dataC12[i] appartient\n",
    "    clustersC1C2 = algo.fit_predict(data)\n",
    "    clustersC1=[]\n",
    "    clustersC2=[]\n",
    "#     print(\"i1: \\n\",indexC1,\"\\n i2: \\n\",indexC2)\n",
    "    for i in range(len(clustersC1C2)):\n",
    "        if i<indexC1:\n",
    "            clustersC1.append(clustersC1C2[i])\n",
    "        else:\n",
    "            clustersC2.append(clustersC1C2[i])\n",
    "#     print(\"CC1  : \\n\",clustersC1)\n",
    "#     print(\"CC2  : \\n\",clustersC2)\n",
    "#     print(\"CC1-2  : \\n\",clustersC1C2)\n",
    "\n",
    "    dataC3=pd.read_csv('iris.csv',names=names)\n",
    "    dataC3=dataC3[dataC3['Species'].isin(['Iris-versicolor'])]\n",
    "    dataC3['Species']=1.0\n",
    "\n",
    "#     print(\"print dataC3:\\n\")\n",
    "#     print(dataC3)\n",
    "\n",
    "    #predict utilise les clusters calculés par fit_predict(dataC1C2) et attribue chaque ligne de dataC3 au cluster dont le centre\n",
    "    #minimise la distance\n",
    "    C3ClusterPred = algo.predict(dataC3)\n",
    "\n",
    "#     print(\"Clusters C3:\\n\")\n",
    "#     print(C3ClusterPred)\n",
    "#     print(\"Clusters C1C2 :\\n\")\n",
    "#     print(clustersC1C2)\n",
    "    return (C3ClusterPred,clustersC1,clustersC2)#,clustersC1,clustersC2#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 15, 3: 20}\n",
      "{1: 35}\n",
      "{1: 20, 2: 15}\n",
      "{0: 23, 2: 23, 3: 1}\n",
      "{0: 20, 1: 1, 2: 7, 3: 19}\n",
      "{0: 31, 1: 1, 3: 15}\n",
      "{0: 50}\n",
      "{3: 50}\n",
      "{1: 9, 2: 41}\n"
     ]
    }
   ],
   "source": [
    "clustersC3l1,clustersC1l1,clustersC2l1=clusterizeLayer('./iris_8_10_8_/iris_l1_8.csv')\n",
    "clustersC3l2,clustersC1l2,clustersC2l2=clusterizeLayer('./iris_8_10_8_/iris_l2_10.csv')\n",
    "clustersC3l3,clustersC1l3,clustersC2l3=clusterizeLayer('./iris_8_10_8_/iris_l3_8.csv')\n",
    "\n",
    "C1l1=get_result_cluster(clustersC1l1,1,1)\n",
    "C1l2=get_result_cluster(clustersC1l2,2,1)\n",
    "C1l3=get_result_cluster(clustersC1l3,3,1)\n",
    "\n",
    "C2l1=get_result_cluster(clustersC2l1,1,2)\n",
    "C2l2=get_result_cluster(clustersC2l2,2,2)\n",
    "C2l3=get_result_cluster(clustersC2l3,3,2)\n",
    "\n",
    "C3l1=get_result_cluster(clustersC3l1,1,3)\n",
    "C3l2=get_result_cluster(clustersC3l2,2,3)\n",
    "C3l3=get_result_cluster(clustersC3l3,3,3)\n",
    "\n",
    "save_result_cluster(\"./iris_8_10_8_/iris_clusters_layer1.csv\",C1l1,C2l1,C3l1)\n",
    "save_result_cluster(\"./iris_8_10_8_/iris_clusters_layer2.csv\",C1l2,C2l2,C3l2)\n",
    "save_result_cluster(\"./iris_8_10_8_/iris_clusters_layer3.csv\",C1l3,C2l3,C3l3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1:\n",
      "    Layer  Class  Count  Cluster\n",
      "0      1      1     15        1\n",
      "1      1      1     20        3\n",
      "2      1      2     23        0\n",
      "3      1      2     23        2\n",
      "4      1      2      1        3\n",
      "5      1      3     50        0\n",
      "léklass:\n",
      " [15, 20] [23, 23, 1] [50]\n"
     ]
    }
   ],
   "source": [
    "layer1 = pd.read_csv(\"./iris_8_10_8_/iris_clusters_layer1.csv\")\n",
    "print(\"layer1:\\n\",layer1)\n",
    "\n",
    "class1=[]\n",
    "class2=[]\n",
    "class3=[]\n",
    "for index,element in layer1.iterrows():\n",
    "    if element[1]==1:\n",
    "        if element[\"Count\"] >= 0:\n",
    "            class1.append(element[\"Count\"])\n",
    "        else:\n",
    "            class1.append(0)\n",
    "    if element[1]==2:\n",
    "        if element[\"Count\"] >= 0:\n",
    "            class2.append(element[\"Count\"])\n",
    "        else:\n",
    "            class2.append(0)\n",
    "    if element[1]==3:\n",
    "        if element[\"Count\"] >= 0:\n",
    "            class3.append(element[\"Count\"])\n",
    "        else:\n",
    "            class3.append(0)\n",
    "print(\"léklass:\\n\",class1,class2,class3)\n",
    "\n",
    "        \n",
    "color = ['blue', 'red', 'yellow', 'green']\n",
    "#plot_clusters_2D(clustersC1C2l1 , clustersC3l1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 23, 50]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def graphCluster(layer):\n",
    "    names = ['cluster0c1', 'cluster0c2', 'cluster0c3' ,\n",
    "             'cluster1c1', 'cluster1c2', 'cluster1c3' , \n",
    "             'cluster2c1', 'cluster2c2', 'cluster2c3',\n",
    "             'cluster3c1', 'cluster3c2', 'cluster3c3']\n",
    "\n",
    "    cluster0 = [0,0,0]\n",
    "    cluster1 = [0,0,0]\n",
    "    cluster2 = [0,0,0]\n",
    "    cluster3 = [0,0,0]\n",
    "\n",
    "    for index,element in layer.iterrows():\n",
    "        if element[\"Cluster\"]==0:\n",
    "            cluster0[element[\"Class\"] - 1]=element[\"Count\"]\n",
    "        if element[\"Cluster\"]==1:\n",
    "            cluster1[element[\"Class\"]-1]=element[\"Count\"]\n",
    "        if element[\"Class\"]==2:\n",
    "            cluster2[element[\"Class\"]-1]=element[\"Count\"]\n",
    "        if element[\"Cluster\"]==3:\n",
    "            cluster3[element[\"Class\"]-1]=element[\"Count\"]\n",
    "\n",
    "    print(cluster0)\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    values = cluster0 + cluster1 + cluster2 +cluster3 \n",
    "\n",
    "    plt.bar(names,values,color = ['blue', 'red', 'green'])\n",
    "    \n",
    "graphCluster(layer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mnist.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-2a29c28fefcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hayaat/.local/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mnist.npz'"
     ]
    }
   ],
   "source": [
    "#(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "from numpy import load\n",
    "\n",
    "data = load('mnist.npz')\n",
    "lst = data.files\n",
    "print(lst)\n",
    "\n",
    "(X_train,y_train) = (data[lst[1]],data[lst[2]])\n",
    "(X_test,y_test) = (data[lst[0]],data[lst[3]])\n",
    "\n",
    "X_train_sample=X_train[0:100]\n",
    "y_train_sample=y_train[0:100]\n",
    "\n",
    "X_train=X_train_sample\n",
    "y_train=y_train_sample\n",
    "X_train = X_train.reshape(100, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "\n",
    "X_01=[]\n",
    "y_01=[]\n",
    "nb_X=0\n",
    "for i in range(X_train.shape[0]):\n",
    "    if (y_train[i]==0 or y_train[i]==1):\n",
    "        \n",
    "        nb_X+=1\n",
    "        X_01.append(X_train[i])\n",
    "        y_01.append(y_train[i])\n",
    "\n",
    "       \n",
    "train_X=np.asarray(X_01)\n",
    "\n",
    "train_y=y_01\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "train_y=encoder.fit_transform(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\n",
    "\n",
    "model.fit(train_X, train_y, epochs = 40, batch_size = 32)\n",
    "\n",
    "\n",
    "X_good,y_good=get_goodXy (train_X, train_y)\n",
    "\n",
    "\n",
    "# Récupération des valeurs de tous les layers sauf le dernier\n",
    "result_layers=get_result_layers(model,X_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du fichier\n",
    "# structure :\n",
    "# 0/1 = valeur de la classe\n",
    "# chaque valeur de layer est entourée par une étoile *\n",
    "save_result_layers(\"mnist_512_tmp\",X_good,y_good,result_layers)\n",
    "# tri du fichier\n",
    "os.system ('sort mnist_512_tmp > mnist_512_.csv')\n",
    "# effacer le fichier intermédiaire\n",
    "os.system ('rm mnist_512_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalise = True\n",
    "includeClasses = True\n",
    "nbClusters = 10\n",
    "\n",
    "data = load('mnist.npz')\n",
    "lst = data.files\n",
    "\n",
    "print(data[lst[0]][0]) #X_test\n",
    "print(data[lst[1]][0]) #X_train\n",
    "print(data[lst[2]][0]) #y_train\n",
    "print(data[lst[3]][0]) #y_test\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#On rajoute une colonne Classe qui contient le numéro de la classe correspondant au feature Species\n",
    "listeSpecies = pd.unique(data['Species'])\n",
    "listeIndices = [i+1 for i in range(len(listeSpecies))]\n",
    "dico = dict(zip(listeSpecies, listeIndices))\n",
    "data.insert(len(data.columns), 'Classe', [dico[x] for x in data['Species']])\n",
    "\n",
    "if normalise:\n",
    "    if includeClasses:\n",
    "        m = data.columns[data.columns!='Species']\n",
    "    else:\n",
    "        #On veut garder la séparation en classes 1, 2 et 3, on exclut donc cette colonne de la normalisation\n",
    "        m = data.columns[~data.columns.isin(['Species','Classe'])]\n",
    "    data[m] = (data[m]-data[m].min())/(data[m].max()-data[m].min())\n",
    "\n",
    "#Note: dataC3 et dataC1C2 sont des copies profondes\n",
    "dataC1C2 = data[data['Species'].isin(['Iris-virginica','Iris-setosa'])].drop('Species', axis=1)\n",
    "dataC3 = data[data['Species']=='Iris-versicolor'].drop('Species', axis=1)\n",
    "\n",
    "\n",
    "algo = KMeans(n_clusters = nbClusters)\n",
    "#fit_predict fait tourner l'algo et retourne y où y[i] est l'indice du cluster auquel dataC12[i] appartient\n",
    "clustersC1C2 = algo.fit_predict(dataC1C2)\n",
    "\n",
    "#predict utilise les clusters calculés par fit_predict(dataC12) et attribue chaque ligne de dataC3 au cluster dont le centre\n",
    "#minimise la distance\n",
    "C3ClusterPred = algo.predict(dataC3)\n",
    "\n",
    "#Les Species sont-elles bien discrimninées par l'algo de clustering?\n",
    "clusters = [[] for _ in range(1+max(clustersC1C2))]\n",
    "for i,x in enumerate(clustersC1C2):\n",
    "    clusters[x].append(i)\n",
    "taillesClusters = [len(l) for l in clusters]\n",
    "print(\"Tailles des clusters non vides\")\n",
    "print(taillesClusters)\n",
    "\n",
    "for cluster in clusters:\n",
    "    if not(cluster):\n",
    "        continue\n",
    "    classe = data.at[cluster[0],'Species']\n",
    "    for sign in cluster:\n",
    "        if data.at[sign, 'Species'] != classe:\n",
    "            print(\"Présence classe mixte\")\n",
    "\n",
    "#Predict sur la classe 3\n",
    "X = dataC3.drop(columns= ['Classe'])\n",
    "X = np.array(X)\n",
    "Y = model.predict(X)\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "Y = sigmoid(Y)\n",
    "print(Y)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
