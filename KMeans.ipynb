{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.265565Z",
     "start_time": "2020-02-13T18:33:00.211348Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/aurelien/Desktop/CoursFac/Info/TER/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/aurelien/Desktop/CoursFac/Info/TER/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/aurelien/Desktop/CoursFac/Info/TER/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/aurelien/Desktop/CoursFac/Info/TER/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/aurelien/Desktop/CoursFac/Info/TER/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/aurelien/Desktop/CoursFac/Info/TER/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import keras\n",
    "import os\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.282036Z",
     "start_time": "2020-02-13T18:33:02.268265Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_goodXy (X,y):\n",
    "    ynew = model.predict_classes(X)\n",
    "    X_good =[]\n",
    "    y_good=[]\n",
    "    for i in range(len(X)):\n",
    "        if (ynew[i]==0 and y[i]==1) or (ynew[i]==1 and y[i]==0):\n",
    "            print (\"error prediction for X=%s, Predicted=%s, Real=%s\"% (X[i], ynew[i], y[i]))\n",
    "        else :\n",
    "            X_good.append(X[i])\n",
    "            y_good.append(y[i])\n",
    "    return X_good,y_good        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.299211Z",
     "start_time": "2020-02-13T18:33:02.285314Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_result_layers(model,X):\n",
    "    result_layers=[]\n",
    "    for i in range (len(model.layers)-1):\n",
    "        hidden_layers= keras.backend.function(\n",
    "                [model.layers[0].input],   \n",
    "                [model.layers[i].output,] \n",
    "                )    \n",
    "        result_layers.append(hidden_layers([X_good])[0])  \n",
    "    return result_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.324742Z",
     "start_time": "2020-02-13T18:33:02.303088Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_result_layers(filename,X,y,result_layers):\n",
    "    f = open(filename, \"w\")\n",
    "    for nb_X in range (len(X)):\n",
    "        #my_string=\"\"\n",
    "        my_string=str(y[nb_X])+','\n",
    "        for nb_layers in range (len(model.layers)-1):\n",
    "            my_string+=\"<b>,\"\n",
    "            for j in range (len(result_layers[nb_layers][nb_X])):\n",
    "                my_string+=str(result_layers[nb_layers][nb_X][j])+','\n",
    "            my_string+=\"</b>,\"    \n",
    "        my_string=my_string [0:-1]\n",
    "        my_string+='\\n'\n",
    "        f.write(my_string)    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:03.159174Z",
     "start_time": "2020-02-13T18:33:02.327414Z"
    }
   },
   "outputs": [],
   "source": [
    "url=\"./iris.csv\"\n",
    "names = ['SepalLengthCm', 'SepalWidthCm', \n",
    "         'PetalLengthCm', 'PetalWidthCm', \n",
    "         'Species']\n",
    "\n",
    "data = pd.read_csv(url, names=names)\n",
    "\n",
    "#Classification binaire sur Virginica et Setosa seulement\n",
    "data=data[data['Species'].isin(['Iris-virginica', 'Iris-setosa'])]\n",
    "\n",
    "i = 8\n",
    "data_to_predict = data[:i].reset_index(drop = True) \n",
    "\"\"\"\n",
    "reset_index() is a method to reset index of a Data Frame. \n",
    "reset_index() method sets a list of integer ranging from 0 to length of data as index. \n",
    "\"\"\"\n",
    "predict_species = data_to_predict.Species \n",
    "\"\"\" Species de la class Iris \"\"\"\n",
    "\n",
    "predict_species = np.array(predict_species) \n",
    "\"\"\"\n",
    "An array object satisfying the specified requirements.\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html\n",
    "\"\"\"\n",
    "\n",
    "prediction = np.array(data_to_predict.drop(['Species'],axis= 1))\n",
    "\n",
    "data = data[i:].reset_index(drop = True)\n",
    "\n",
    "X = data.drop(['Species'], axis = 1) \n",
    "\"\"\"\n",
    "The drop() function is used to drop specified labels from rows or columns.\n",
    "\"\"\"\n",
    "X = np.array(X)\n",
    "y = data['Species']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y=encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "train_X, test_X, train_y, test_y = model_selection.train_test_split(X,y,test_size = 0.1, random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.678441Z",
     "start_time": "2020-02-13T18:33:03.161667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 1.0917 - acc: 0.4390\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4943 - acc: 0.8537\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.2674 - acc: 1.0000\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1547 - acc: 1.0000\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.0921 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 0s 700us/step - loss: 0.0579 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.0387 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.0273 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 0s 894us/step - loss: 0.0202 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 0s 626us/step - loss: 0.0155 - acc: 1.0000\n",
      "10/10 [==============================] - 0s 4ms/step\n",
      "\n",
      "acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Utilisation de keras comme classifieur\n",
    "# mettre sigmoid comme fonction car binaire. Attention 1 seul neurone en sortie\n",
    "input_dim = len(data.columns) - 1\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\n",
    "\n",
    "model.fit(train_X, train_y, epochs = 10, batch_size = 2)\n",
    "\n",
    "scores = model.evaluate(test_X, test_y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.723085Z",
     "start_time": "2020-02-13T18:33:04.681566Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Récupération seulement des bons classés\n",
    "X_good,y_good=get_goodXy (train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.796700Z",
     "start_time": "2020-02-13T18:33:04.725841Z"
    }
   },
   "outputs": [],
   "source": [
    "# Récupération des valeurs de tous les layers sauf le dernier\n",
    "result_layers=get_result_layers(model,X_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.834457Z",
     "start_time": "2020-02-13T18:33:04.799432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sauvegarde du fichier\n",
    "# structure :\n",
    "# 0/1 = valeur de la classe\n",
    "# chaque valeur de layer est entourée par un []\n",
    "save_result_layers(\"iris_8_10_8_tmp\",X_good,y_good,result_layers)\n",
    "# tri du fichier\n",
    "os.system ('sort iris_8_10_8_tmp > iris_8_10_8_.csv')\n",
    "# effacer le fichier intermédiaire\n",
    "os.system ('rm iris_8_10_8_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_layers_from_csv(filename):\n",
    "    tokens=filename.split(\"_\")\n",
    "    df = pd.read_csv(filename, sep = ',', header = None) \n",
    "\n",
    "    \n",
    "    # creation d'un répertoire pour sauver tous les fichiers\n",
    "    repertoire=filename[0:-4]\n",
    "    os.makedirs(repertoire, exist_ok=True)\n",
    "    string = repertoire+'/'+tokens[0]+'_'\n",
    "    f=[]\n",
    "    filenames=[]\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        name_file=string+'l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'.csv'\n",
    "        f.append(open(name_file, \"w\"))\n",
    "        filenames.append(name_file)\n",
    "        \n",
    "        \n",
    "    # sauvegarde du dataframe dans une chaîne de caracteres\n",
    "    ch = df.to_string(header=False,\n",
    "                  index=False,\n",
    "                  index_names=False).split('\\n')\n",
    "    vals = [','.join(ele.split()) for ele in ch]\n",
    "    \n",
    "    # sauvegarde dans des fichiers spécifiques par layer\n",
    "    token_layer=[]\n",
    "    token_exemples=[]\n",
    "    for nb_exemples in range (len(vals)):\n",
    "        deb=str(df[0][nb_exemples])+','\n",
    "        # 1 ligne correspond à une chaine\n",
    "        s=vals[nb_exemples]\n",
    "        listoftokens=re.findall(r'<b>,(.+?),</b>', s)\n",
    "        nb_layers=len(listoftokens)\n",
    "        \n",
    "        for nb_token in range (nb_layers):\n",
    "            save_token=''\n",
    "            save_token=deb+str(listoftokens[nb_token])+'\\n'\n",
    "            \n",
    "            f[nb_token].write(save_token)\n",
    "\n",
    "    # sauvegarde d'un fichier qui contient tous les layers en une fois\n",
    "    # récupération des données pour enlever les <b> et </b>\n",
    "    df_all=pd.DataFrame()\n",
    "    myindex=0\n",
    "    for nb_columns in range(df.shape[1]):\n",
    "        df[nb_columns]=df[nb_columns].astype(str)\n",
    "        if (df[nb_columns]!='<b>').all() and (df[nb_columns]!='</b>').all():\n",
    "            df_all[myindex]=df[nb_columns]\n",
    "            myindex+=1\n",
    "\n",
    "    # construction du nom du fichier de sauvegarde\n",
    "    string = repertoire+'/'+tokens[0]+'_'\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        string+='l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'_'\n",
    "    string+='.csv'       \n",
    "    # sauvegarde en .csv\n",
    "    df_all.to_csv(string, sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a directory with a specific file for all the layers\n",
    "filename=\"iris_8_10_8_.csv\"    \n",
    "get_directory_layers_from_csv(filename)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from numpy import ndarray\n",
    "from matplotlib import colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def plot_clusters_3D (X,y_predict, nb_clusters, pca_done=False):\n",
    "    if pca_done==False:\n",
    "        pca = PCA(n_components=3) \n",
    "        pca.fit(X) \n",
    "        pca_data = pd.DataFrame(pca.transform(X))\n",
    "    else: pca_data = pd.DataFrame(X)\n",
    "    colors = list(zip(*sorted(( \n",
    "                    tuple(mcolors.rgb_to_hsv( \n",
    "                          mcolors.to_rgba(color)[:3])), name) \n",
    "                     for name, color in dict( \n",
    "                            mcolors.BASE_COLORS, **mcolors.CSS4_COLORS \n",
    "                                                      ).items())))[1] \n",
    "    # number of steps to taken generate n(clusters) colors\n",
    "    skips = math.floor(len(colors[5 : -5])/nb_clusters) \n",
    "    cluster_colors = colors[5 : -5 : skips] \n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax = fig.add_subplot(111, projection = '3d') \n",
    "    ax.scatter(pca_data[0], pca_data[1], pca_data[2],\n",
    "           c = list(map(lambda label : cluster_colors[label], \n",
    "                                            y_predict))) \n",
    "\n",
    "    str_labels = list(map(lambda label:'% s' % label, y_predict)) \n",
    "\n",
    "    list(map(lambda data1, data2, data3, str_label: \n",
    "        ax.text(data1, data2, data3, s = str_label, size = 16.5, \n",
    "        zorder = 20, color = 'k'), pca_data[0], pca_data[1], \n",
    "        pca_data[2], str_labels)) \n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def plot_clusters_2D (X,y_predict, nb_clusters, pca_done=False):\n",
    "    if pca_done==False:\n",
    "        pca = PCA(n_components=2) \n",
    "        X_r = pca.fit(X).transform(X)\n",
    "    else: X_r = X\n",
    "    data = pd.DataFrame(X_r, columns=['x','y'])\n",
    "    data['label']=y_predict\n",
    "    list_clusters=list(set(y_predict))\n",
    "    print (list_clusters)\n",
    "    #create a new figure\n",
    "    plt.figure(figsize=(5,5))\n",
    "\n",
    "    #loop through labels and plot each cluster\n",
    "    for i, label in enumerate(list_clusters):\n",
    "\n",
    "        #add data points \n",
    "        plt.scatter(x=data.loc[data['label']==label, 'x'], \n",
    "                y=data.loc[data['label']==label,'y'], \n",
    "                color=color[i], \n",
    "                alpha=0.20)\n",
    "\n",
    "        #add label\n",
    "        plt.annotate(label, \n",
    "                 data.loc[data['label']==label,['x','y']].mean(),\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 size=20, weight='bold',\n",
    "                 color=color[i])\n",
    "        \n",
    "def get_directory_layers_from_csv(filename):\n",
    "    tokens=filename.split(\"_\")\n",
    "    df = pd.read_csv(filename, sep = ',', header = None) \n",
    "\n",
    "    \n",
    "    # creation d'un répertoire pour sauver tous les fichiers\n",
    "    repertoire=filename[0:-4]\n",
    "    os.makedirs(repertoire, exist_ok=True)\n",
    "    string = repertoire+'/'+tokens[0]+'_'\n",
    "    f=[]\n",
    "    filenames=[]\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        name_file=string+'l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'.csv'\n",
    "        f.append(open(name_file, \"w\"))\n",
    "        filenames.append(name_file)\n",
    "        \n",
    "        \n",
    "    # sauvegarde du dataframe dans une chaîne de caracteres\n",
    "    ch = df.to_string(header=False,\n",
    "                  index=False,\n",
    "                  index_names=False).split('\\n')\n",
    "    vals = [','.join(ele.split()) for ele in ch]\n",
    "    \n",
    "    # sauvegarde dans des fichiers spécifiques par layer\n",
    "    token_layer=[]\n",
    "    token_exemples=[]\n",
    "    for nb_exemples in range (len(vals)):\n",
    "        deb=str(df[0][nb_exemples])+','\n",
    "        # 1 ligne correspond à une chaine\n",
    "        s=vals[nb_exemples]\n",
    "        listoftokens=re.findall(r'<b>,(.+?),</b>', s)\n",
    "        nb_layers=len(listoftokens)\n",
    "        \n",
    "        for nb_token in range (nb_layers):\n",
    "            save_token=''\n",
    "            save_token=deb+str(listoftokens[nb_token])+'\\n'\n",
    "            \n",
    "            f[nb_token].write(save_token)\n",
    "\n",
    "    # sauvegarde d'un fichier qui contient tous les layers en une fois\n",
    "    # récupération des données pour enlever les <b> et </b>\n",
    "    df_all=pd.DataFrame()\n",
    "    myindex=0\n",
    "    for nb_columns in range(df.shape[1]):\n",
    "        df[nb_columns]=df[nb_columns].astype(str)\n",
    "        if (df[nb_columns]!='<b>').all() and (df[nb_columns]!='</b>').all():\n",
    "            df_all[myindex]=df[nb_columns]\n",
    "            myindex+=1\n",
    "\n",
    "    # construction du nom du fichier de sauvegarde\n",
    "    string = repertoire+'/'+tokens[0]+'_'\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        string+='l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'_'\n",
    "    string+='.csv'       \n",
    "    # sauvegarde en .csv\n",
    "    df_all.to_csv(string, sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a directory with a specific file for all the layers\n",
    "filename=\"iris_8_10_8_.csv\"    \n",
    "get_directory_layers_from_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result_cluster(filename,clusters,numLayer,cluster_class):\n",
    "#    f = open(filename, \"w\")\n",
    "    idClusters=np.unique(clusters)\n",
    "    tabCount={}\n",
    "    for id in idClusters:\n",
    "        count = 0\n",
    "        for j in range(len(clusters)):\n",
    "            if clusters[j]==id:\n",
    "                tabCount[id]=count+1\n",
    "                count+=1\n",
    "    print(tabCount)\n",
    "    my_string=\"\"\n",
    "    for key,value in tabCount.items():\n",
    "        my_string += str(numLayer)+','+str(cluster_class)+','\n",
    "        my_string+=str(value)+','+str(key)+'\\n'\n",
    "    print(my_string)\n",
    "#     f.close()\n",
    "\n",
    "def clusterizeLayer(url, nbClusters=4):\n",
    "    data = pd.read_csv(url,names=names)\n",
    "    dataC1=dataC2=[]\n",
    "    indexC2=indexC1=[]\n",
    "    i=0\n",
    "    for index, row in data.iterrows():\n",
    "        if row[0]==0:\n",
    "            dataC1.append(row)\n",
    "            indexC1=i\n",
    "        else:\n",
    "            dataC2.append(row)\n",
    "            indexC2=i\n",
    "        i+=1\n",
    "        \n",
    "#     with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#         print(data)\n",
    "\n",
    "    algo = KMeans(n_clusters = nbClusters)\n",
    "    #fit_predict fait tourner l'algo et retourne y où y[i] est l'indice du cluster auquel dataC12[i] appartient\n",
    "    clustersC1C2 = algo.fit_predict(data)\n",
    "#     print(clustersC1C2)\n",
    "\n",
    "    dataC3=pd.read_csv('iris.csv',names=names)\n",
    "    dataC3=dataC3[dataC3['Species'].isin(['Iris-versicolor'])]\n",
    "    dataC3['Species']=1.0\n",
    "\n",
    "#     print(\"print dataC3:\\n\")\n",
    "#     print(dataC3)\n",
    "\n",
    "    #predict utilise les clusters calculés par fit_predict(dataC1C2) et attribue chaque ligne de dataC3 au cluster dont le centre\n",
    "    #minimise la distance\n",
    "    C3ClusterPred = algo.predict(dataC3)\n",
    "\n",
    "#     print(\"Clusters C3:\\n\")\n",
    "#     print(C3ClusterPred)\n",
    "    return C3ClusterPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 23, 2: 27}\n",
      "1,3,23,0\n",
      "1,3,27,2\n",
      "\n",
      "{1: 50}\n",
      "2,3,50,1\n",
      "\n",
      "{2: 50}\n",
      "3,3,50,2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clustersC3l1=clusterizeLayer('./iris_8_10_8_/iris_l1_8.csv')\n",
    "# clustersC3l2=clusterizeLayer('./iris_8_10_8_/iris_l2_10.csv')\n",
    "# clustersC3l3=clusterizeLayer('./iris_8_10_8_/iris_l3_8.csv')\n",
    "\n",
    "save_result_cluster(\"osef\",clusterizeLayer('./iris_8_10_8_/iris_l1_8.csv',4),1,3)\n",
    "save_result_cluster(\"osef\",clusterizeLayer('./iris_8_10_8_/iris_l2_10.csv',4),2,3)\n",
    "save_result_cluster(\"osef\",clusterizeLayer('./iris_8_10_8_/iris_l3_8.csv',4),3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x_test', 'x_train', 'y_train', 'y_test']\n"
     ]
    }
   ],
   "source": [
    "#(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "from numpy import load\n",
    "\n",
    "data = load('mnist.npz')\n",
    "lst = data.files\n",
    "print(lst)\n",
    "\n",
    "(X_train,y_train) = (data[lst[1]],data[lst[2]])\n",
    "(X_test,y_test) = (data[lst[0]],data[lst[3]])\n",
    "\n",
    "X_train_sample=X_train[0:100]\n",
    "y_train_sample=y_train[0:100]\n",
    "\n",
    "X_train=X_train_sample\n",
    "y_train=y_train_sample\n",
    "X_train = X_train.reshape(100, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "\n",
    "X_01=[]\n",
    "y_01=[]\n",
    "nb_X=0\n",
    "for i in range(X_train.shape[0]):\n",
    "    if (y_train[i]==0 or y_train[i]==1):\n",
    "        \n",
    "        nb_X+=1\n",
    "        X_01.append(X_train[i])\n",
    "        y_01.append(y_train[i])\n",
    "\n",
    "       \n",
    "train_X=np.asarray(X_01)\n",
    "\n",
    "train_y=y_01\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "train_y=encoder.fit_transform(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.6789 - acc: 0.5926\n",
      "Epoch 2/40\n",
      "27/27 [==============================] - 0s 683us/step - loss: 0.3608 - acc: 0.9630\n",
      "Epoch 3/40\n",
      "27/27 [==============================] - 0s 641us/step - loss: 0.2242 - acc: 1.0000\n",
      "Epoch 4/40\n",
      "27/27 [==============================] - 0s 482us/step - loss: 0.1366 - acc: 1.0000\n",
      "Epoch 5/40\n",
      "27/27 [==============================] - 0s 725us/step - loss: 0.0812 - acc: 1.0000\n",
      "Epoch 6/40\n",
      "27/27 [==============================] - 0s 492us/step - loss: 0.0488 - acc: 1.0000\n",
      "Epoch 7/40\n",
      "27/27 [==============================] - 0s 539us/step - loss: 0.0304 - acc: 1.0000\n",
      "Epoch 8/40\n",
      "27/27 [==============================] - 0s 728us/step - loss: 0.0199 - acc: 1.0000\n",
      "Epoch 9/40\n",
      "27/27 [==============================] - 0s 557us/step - loss: 0.0137 - acc: 1.0000\n",
      "Epoch 10/40\n",
      "27/27 [==============================] - 0s 518us/step - loss: 0.0099 - acc: 1.0000\n",
      "Epoch 11/40\n",
      "27/27 [==============================] - 0s 434us/step - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 12/40\n",
      "27/27 [==============================] - 0s 789us/step - loss: 0.0056 - acc: 1.0000\n",
      "Epoch 13/40\n",
      "27/27 [==============================] - 0s 718us/step - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 14/40\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 15/40\n",
      "27/27 [==============================] - 0s 483us/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 16/40\n",
      "27/27 [==============================] - 0s 919us/step - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 17/40\n",
      "27/27 [==============================] - 0s 570us/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 18/40\n",
      "27/27 [==============================] - 0s 775us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 19/40\n",
      "27/27 [==============================] - 0s 553us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 20/40\n",
      "27/27 [==============================] - 0s 730us/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 21/40\n",
      "27/27 [==============================] - 0s 943us/step - loss: 8.5591e-04 - acc: 1.0000\n",
      "Epoch 22/40\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 7.2989e-04 - acc: 1.0000\n",
      "Epoch 23/40\n",
      "27/27 [==============================] - 0s 941us/step - loss: 6.2915e-04 - acc: 1.0000\n",
      "Epoch 24/40\n",
      "27/27 [==============================] - 0s 660us/step - loss: 5.4786e-04 - acc: 1.0000\n",
      "Epoch 25/40\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 4.8197e-04 - acc: 1.0000\n",
      "Epoch 26/40\n",
      "27/27 [==============================] - 0s 594us/step - loss: 4.2849e-04 - acc: 1.0000\n",
      "Epoch 27/40\n",
      "27/27 [==============================] - 0s 749us/step - loss: 3.8434e-04 - acc: 1.0000\n",
      "Epoch 28/40\n",
      "27/27 [==============================] - 0s 453us/step - loss: 3.4767e-04 - acc: 1.0000\n",
      "Epoch 29/40\n",
      "27/27 [==============================] - 0s 735us/step - loss: 3.1674e-04 - acc: 1.0000\n",
      "Epoch 30/40\n",
      "27/27 [==============================] - 0s 984us/step - loss: 2.9068e-04 - acc: 1.0000\n",
      "Epoch 31/40\n",
      "27/27 [==============================] - 0s 484us/step - loss: 2.6859e-04 - acc: 1.0000\n",
      "Epoch 32/40\n",
      "27/27 [==============================] - 0s 746us/step - loss: 2.4974e-04 - acc: 1.0000\n",
      "Epoch 33/40\n",
      "27/27 [==============================] - 0s 820us/step - loss: 2.3356e-04 - acc: 1.0000\n",
      "Epoch 34/40\n",
      "27/27 [==============================] - 0s 881us/step - loss: 2.1958e-04 - acc: 1.0000\n",
      "Epoch 35/40\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 2.0740e-04 - acc: 1.0000\n",
      "Epoch 36/40\n",
      "27/27 [==============================] - 0s 798us/step - loss: 1.9679e-04 - acc: 1.0000\n",
      "Epoch 37/40\n",
      "27/27 [==============================] - 0s 708us/step - loss: 1.8747e-04 - acc: 1.0000\n",
      "Epoch 38/40\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 1.7927e-04 - acc: 1.0000\n",
      "Epoch 39/40\n",
      "27/27 [==============================] - 0s 810us/step - loss: 1.7202e-04 - acc: 1.0000\n",
      "Epoch 40/40\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 1.6558e-04 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "input_dim = 784\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\n",
    "\n",
    "model.fit(train_X, train_y, epochs = 40, batch_size = 32)\n",
    "\n",
    "\n",
    "X_good,y_good=get_goodXy (train_X, train_y)\n",
    "\n",
    "\n",
    "# Récupération des valeurs de tous les layers sauf le dernier\n",
    "result_layers=get_result_layers(model,X_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sauvegarde du fichier\n",
    "# structure :\n",
    "# 0/1 = valeur de la classe\n",
    "# chaque valeur de layer est entourée par une étoile *\n",
    "save_result_layers(\"mnist_512_tmp\",X_good,y_good,result_layers)\n",
    "# tri du fichier\n",
    "os.system ('sort mnist_512_tmp > mnist_512_.csv')\n",
    "# effacer le fichier intermédiaire\n",
    "os.system ('rm mnist_512_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  84 185 159 151  60  36   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 222 254 254 254 254 241 198 198 198 198 198 198\n",
      "  198 198 170  52   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  67 114  72 114 163 227 254 225 254 254 254 250\n",
      "  229 254 254 140   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  17  66  14  67  67  67  59\n",
      "   21 236 254 106   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   83 253 209  18   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  22\n",
      "  233 255  83   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 129\n",
      "  254 238  44   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  59 249\n",
      "  254  62   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 133 254\n",
      "  187   5   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   9 205 248\n",
      "   58   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 126 254 182\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  75 251 240  57\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  19 221 254 166   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3 203 254 219  35   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  38 254 254  77   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  31 224 254 115   1   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 133 254 254  52   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  61 242 254 254  52   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 121 254 254 219  40   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 121 254 207  18   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "5\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#On rajoute une colonne Classe qui contient le numéro de la classe correspondant au feature Species\\nlisteSpecies = pd.unique(data[\\'Species\\'])\\nlisteIndices = [i+1 for i in range(len(listeSpecies))]\\ndico = dict(zip(listeSpecies, listeIndices))\\ndata.insert(len(data.columns), \\'Classe\\', [dico[x] for x in data[\\'Species\\']])\\n\\nif normalise:\\n    if includeClasses:\\n        m = data.columns[data.columns!=\\'Species\\']\\n    else:\\n        #On veut garder la séparation en classes 1, 2 et 3, on exclut donc cette colonne de la normalisation\\n        m = data.columns[~data.columns.isin([\\'Species\\',\\'Classe\\'])]\\n    data[m] = (data[m]-data[m].min())/(data[m].max()-data[m].min())\\n\\n#Note: dataC3 et dataC1C2 sont des copies profondes\\ndataC1C2 = data[data[\\'Species\\'].isin([\\'Iris-virginica\\',\\'Iris-setosa\\'])].drop(\\'Species\\', axis=1)\\ndataC3 = data[data[\\'Species\\']==\\'Iris-versicolor\\'].drop(\\'Species\\', axis=1)\\n\\n\\nalgo = KMeans(n_clusters = nbClusters)\\n#fit_predict fait tourner l\\'algo et retourne y où y[i] est l\\'indice du cluster auquel dataC12[i] appartient\\nclustersC1C2 = algo.fit_predict(dataC1C2)\\n\\n#predict utilise les clusters calculés par fit_predict(dataC12) et attribue chaque ligne de dataC3 au cluster dont le centre\\n#minimise la distance\\nC3ClusterPred = algo.predict(dataC3)\\n\\n#Les Species sont-elles bien discrimninées par l\\'algo de clustering?\\nclusters = [[] for _ in range(1+max(clustersC1C2))]\\nfor i,x in enumerate(clustersC1C2):\\n    clusters[x].append(i)\\ntaillesClusters = [len(l) for l in clusters]\\nprint(\"Tailles des clusters non vides\")\\nprint(taillesClusters)\\n\\nfor cluster in clusters:\\n    if not(cluster):\\n        continue\\n    classe = data.at[cluster[0],\\'Species\\']\\n    for sign in cluster:\\n        if data.at[sign, \\'Species\\'] != classe:\\n            print(\"Présence classe mixte\")\\n\\n#Predict sur la classe 3\\nX = dataC3.drop(columns= [\\'Classe\\'])\\nX = np.array(X)\\nY = model.predict(X)\\ndef sigmoid(x):\\n    return 1/(1 + np.exp(-x))\\nY = sigmoid(Y)\\nprint(Y)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalise = True\n",
    "includeClasses = True\n",
    "nbClusters = 10\n",
    "\n",
    "data = load('mnist.npz')\n",
    "lst = data.files\n",
    "\n",
    "print(data[lst[0]][0]) #X_test\n",
    "print(data[lst[1]][0]) #X_train\n",
    "print(data[lst[2]][0]) #y_train\n",
    "print(data[lst[3]][0]) #y_test\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#On rajoute une colonne Classe qui contient le numéro de la classe correspondant au feature Species\n",
    "listeSpecies = pd.unique(data['Species'])\n",
    "listeIndices = [i+1 for i in range(len(listeSpecies))]\n",
    "dico = dict(zip(listeSpecies, listeIndices))\n",
    "data.insert(len(data.columns), 'Classe', [dico[x] for x in data['Species']])\n",
    "\n",
    "if normalise:\n",
    "    if includeClasses:\n",
    "        m = data.columns[data.columns!='Species']\n",
    "    else:\n",
    "        #On veut garder la séparation en classes 1, 2 et 3, on exclut donc cette colonne de la normalisation\n",
    "        m = data.columns[~data.columns.isin(['Species','Classe'])]\n",
    "    data[m] = (data[m]-data[m].min())/(data[m].max()-data[m].min())\n",
    "\n",
    "#Note: dataC3 et dataC1C2 sont des copies profondes\n",
    "dataC1C2 = data[data['Species'].isin(['Iris-virginica','Iris-setosa'])].drop('Species', axis=1)\n",
    "dataC3 = data[data['Species']=='Iris-versicolor'].drop('Species', axis=1)\n",
    "\n",
    "\n",
    "algo = KMeans(n_clusters = nbClusters)\n",
    "#fit_predict fait tourner l'algo et retourne y où y[i] est l'indice du cluster auquel dataC12[i] appartient\n",
    "clustersC1C2 = algo.fit_predict(dataC1C2)\n",
    "\n",
    "#predict utilise les clusters calculés par fit_predict(dataC12) et attribue chaque ligne de dataC3 au cluster dont le centre\n",
    "#minimise la distance\n",
    "C3ClusterPred = algo.predict(dataC3)\n",
    "\n",
    "#Les Species sont-elles bien discrimninées par l'algo de clustering?\n",
    "clusters = [[] for _ in range(1+max(clustersC1C2))]\n",
    "for i,x in enumerate(clustersC1C2):\n",
    "    clusters[x].append(i)\n",
    "taillesClusters = [len(l) for l in clusters]\n",
    "print(\"Tailles des clusters non vides\")\n",
    "print(taillesClusters)\n",
    "\n",
    "for cluster in clusters:\n",
    "    if not(cluster):\n",
    "        continue\n",
    "    classe = data.at[cluster[0],'Species']\n",
    "    for sign in cluster:\n",
    "        if data.at[sign, 'Species'] != classe:\n",
    "            print(\"Présence classe mixte\")\n",
    "\n",
    "#Predict sur la classe 3\n",
    "X = dataC3.drop(columns= ['Classe'])\n",
    "X = np.array(X)\n",
    "Y = model.predict(X)\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "Y = sigmoid(Y)\n",
    "print(Y)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
