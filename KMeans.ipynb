{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.265565Z",
     "start_time": "2020-02-13T18:33:00.211348Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/cawosh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/cawosh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/cawosh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/cawosh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/cawosh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/cawosh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import keras\n",
    "import os\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.282036Z",
     "start_time": "2020-02-13T18:33:02.268265Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_goodXy (X,y):\n",
    "    ynew = model.predict_classes(X)\n",
    "    X_good =[]\n",
    "    y_good=[]\n",
    "    for i in range(len(X)):\n",
    "        if (ynew[i]==0 and y[i]==1) or (ynew[i]==1 and y[i]==0):\n",
    "            print (\"error prediction for X=%s, Predicted=%s, Real=%s\"% (X[i], ynew[i], y[i]))\n",
    "        else :\n",
    "            X_good.append(X[i])\n",
    "            y_good.append(y[i])\n",
    "    return X_good,y_good        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.299211Z",
     "start_time": "2020-02-13T18:33:02.285314Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_result_layers(model,X):\n",
    "    result_layers=[]\n",
    "    for i in range (len(model.layers)-1):\n",
    "        hidden_layers= keras.backend.function(\n",
    "                [model.layers[0].input],   \n",
    "                [model.layers[i].output,] \n",
    "                )    \n",
    "        result_layers.append(hidden_layers([X])[0])  \n",
    "    return result_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.324742Z",
     "start_time": "2020-02-13T18:33:02.303088Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_result_layers(filename,X,y,result_layers):\n",
    "    f = open(filename, \"w\")\n",
    "    for nb_X in range (len(X)):\n",
    "        #my_string=\"\"\n",
    "        my_string=str(y[nb_X])+','\n",
    "        for nb_layers in range (len(model.layers)-1):\n",
    "            my_string+=\"<b>,\"\n",
    "            for j in range (len(result_layers[nb_layers][nb_X])):\n",
    "                my_string+=str(result_layers[nb_layers][nb_X][j])+','\n",
    "            my_string+=\"</b>,\"    \n",
    "        my_string=my_string [0:-1]\n",
    "        my_string+='\\n'\n",
    "        f.write(my_string)    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:03.159174Z",
     "start_time": "2020-02-13T18:33:02.327414Z"
    }
   },
   "outputs": [],
   "source": [
    "url=\"./iris.csv\"\n",
    "names = ['SepalLengthCm', 'SepalWidthCm', \n",
    "         'PetalLengthCm', 'PetalWidthCm', \n",
    "         'Species']\n",
    "\n",
    "data = pd.read_csv(url, names=names)\n",
    "\n",
    "#Classification binaire sur Virginica et Setosa seulement\n",
    "data=data[data['Species'].isin(['Iris-virginica', 'Iris-setosa'])]\n",
    "\n",
    "i = 8\n",
    "data_to_predict = data[:i].reset_index(drop = True) \n",
    "\"\"\"\n",
    "reset_index() is a method to reset index of a Data Frame. \n",
    "reset_index() method sets a list of integer ranging from 0 to length of data as index. \n",
    "\"\"\"\n",
    "predict_species = data_to_predict.Species \n",
    "\"\"\" Species de la class Iris \"\"\"\n",
    "\n",
    "predict_species = np.array(predict_species) \n",
    "\"\"\"\n",
    "An array object satisfying the specified requirements.\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html\n",
    "\"\"\"\n",
    "\n",
    "prediction = np.array(data_to_predict.drop(['Species'],axis= 1))\n",
    "\n",
    "data = data[i:].reset_index(drop = True)\n",
    "\n",
    "X = data.drop(['Species'], axis = 1) \n",
    "\"\"\"\n",
    "The drop() function is used to drop specified labels from rows or columns.\n",
    "\"\"\"\n",
    "X = np.array(X)\n",
    "y = data['Species']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y=encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "train_X, test_X, train_y, test_y = model_selection.train_test_split(X,y,test_size = 0.1, random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.678441Z",
     "start_time": "2020-02-13T18:33:03.161667Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 1s 8ms/step - loss: 0.7966 - acc: 0.4390\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.7351 - acc: 0.4390\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.6918 - acc: 0.5488\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.6435 - acc: 0.7073\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4826 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3149 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.1970 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.1155 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.0663 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.0395 - acc: 1.0000\n",
      "10/10 [==============================] - 0s 7ms/step\n",
      "\n",
      "acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Utilisation de keras comme classifieur\n",
    "# mettre sigmoid comme fonction car binaire. Attention 1 seul neurone en sortie\n",
    "input_dim = len(data.columns) - 1\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\n",
    "\n",
    "model.fit(train_X, train_y, epochs = 10, batch_size = 2)\n",
    "\n",
    "scores = model.evaluate(test_X, test_y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.723085Z",
     "start_time": "2020-02-13T18:33:04.681566Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Récupération seulement des bons classés\n",
    "X_good,y_good=get_goodXy (train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.796700Z",
     "start_time": "2020-02-13T18:33:04.725841Z"
    }
   },
   "outputs": [],
   "source": [
    "# Récupération des valeurs de tous les layers sauf le dernier\n",
    "result_layers=get_result_layers(model,X_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.834457Z",
     "start_time": "2020-02-13T18:33:04.799432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sauvegarde du fichier\n",
    "# structure :\n",
    "# 0/1 = valeur de la classe\n",
    "# chaque valeur de layer est entourée par un []\n",
    "save_result_layers(\"iris_8_10_8_tmp\",X_good,y_good,result_layers)\n",
    "# tri du fichier\n",
    "os.system ('sort iris_8_10_8_tmp > iris_8_10_8_.csv')\n",
    "# effacer le fichier intermédiaire\n",
    "os.system ('rm iris_8_10_8_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_layers_from_csv(filename):\n",
    "    tokens=filename.split(\"_\")\n",
    "    df = pd.read_csv(filename, sep = ',', header = None) \n",
    "\n",
    "    \n",
    "    # creation d'un répertoire pour sauver tous les fichiers\n",
    "    repertoire=filename[0:-4]\n",
    "    os.makedirs(repertoire, exist_ok=True)\n",
    "    string = repertoire+'/'+tokens[0]+'_'\n",
    "    f=[]\n",
    "    filenames=[]\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        name_file=string+'l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'.csv'\n",
    "        f.append(open(name_file, \"w\"))\n",
    "        filenames.append(name_file)\n",
    "        \n",
    "        \n",
    "    # sauvegarde du dataframe dans une chaîne de caracteres\n",
    "    ch = df.to_string(header=False,\n",
    "                  index=False,\n",
    "                  index_names=False).split('\\n')\n",
    "    vals = [','.join(ele.split()) for ele in ch]\n",
    "    \n",
    "    # sauvegarde dans des fichiers spécifiques par layer\n",
    "    token_layer=[]\n",
    "    token_exemples=[]\n",
    "    for nb_exemples in range (len(vals)):\n",
    "        deb=str(df[0][nb_exemples])+','\n",
    "        # 1 ligne correspond à une chaine\n",
    "        s=vals[nb_exemples]\n",
    "        listoftokens=re.findall(r'<b>,(.+?),</b>', s)\n",
    "        nb_layers=len(listoftokens)\n",
    "        \n",
    "        for nb_token in range (nb_layers):\n",
    "            save_token=''\n",
    "            save_token=deb+str(listoftokens[nb_token])+'\\n'\n",
    "            \n",
    "            f[nb_token].write(save_token)\n",
    "\n",
    "    # sauvegarde d'un fichier qui contient tous les layers en une fois\n",
    "    # récupération des données pour enlever les <b> et </b>\n",
    "    df_all=pd.DataFrame()\n",
    "    myindex=0\n",
    "    for nb_columns in range(df.shape[1]):\n",
    "        df[nb_columns]=df[nb_columns].astype(str)\n",
    "        if (df[nb_columns]!='<b>').all() and (df[nb_columns]!='</b>').all():\n",
    "            df_all[myindex]=df[nb_columns]\n",
    "            myindex+=1\n",
    "\n",
    "    # construction du nom du fichier de sauvegarde\n",
    "    string = repertoire+'/'+tokens[0]+'_'\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        string+='l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'_'\n",
    "    string+='.csv'       \n",
    "    # sauvegarde en .csv\n",
    "    df_all.to_csv(string, sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from numpy import ndarray\n",
    "from matplotlib import colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def plot_clusters_3D (X,y_predict, nb_clusters, pca_done=False):\n",
    "    if pca_done==False:\n",
    "        pca = PCA(n_components=3) \n",
    "        pca.fit(X) \n",
    "        pca_data = pd.DataFrame(pca.transform(X))\n",
    "    else: pca_data = pd.DataFrame(X)\n",
    "    colors = list(zip(*sorted(( \n",
    "                    tuple(mcolors.rgb_to_hsv( \n",
    "                          mcolors.to_rgba(color)[:3])), name) \n",
    "                     for name, color in dict( \n",
    "                            mcolors.BASE_COLORS, **mcolors.CSS4_COLORS \n",
    "                                                      ).items())))[1] \n",
    "    # number of steps to taken generate n(clusters) colors\n",
    "    skips = math.floor(len(colors[5 : -5])/nb_clusters) \n",
    "    cluster_colors = colors[5 : -5 : skips] \n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax = fig.add_subplot(111, projection = '3d') \n",
    "    ax.scatter(pca_data[0], pca_data[1], pca_data[2],\n",
    "           c = list(map(lambda label : cluster_colors[label], \n",
    "                                            y_predict))) \n",
    "\n",
    "    str_labels = list(map(lambda label:'% s' % label, y_predict)) \n",
    "\n",
    "    list(map(lambda data1, data2, data3, str_label: \n",
    "        ax.text(data1, data2, data3, s = str_label, size = 16.5, \n",
    "        zorder = 20, color = 'k'), pca_data[0], pca_data[1], \n",
    "        pca_data[2], str_labels)) \n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def plot_clusters_2D (X,y_predict, nb_clusters, pca_done=False):\n",
    "    if pca_done==False:\n",
    "        pca = PCA(n_components=2) \n",
    "        X_r = pca.fit(X).transform(X)\n",
    "    else: X_r = X\n",
    "    data = pd.DataFrame(X_r, columns=['x','y'])\n",
    "    data['label']=y_predict\n",
    "    list_clusters=list(set(y_predict))\n",
    "    print (list_clusters)\n",
    "    #create a new figure\n",
    "    plt.figure(figsize=(5,5))\n",
    "\n",
    "    #loop through labels and plot each cluster\n",
    "    for i, label in enumerate(list_clusters):\n",
    "\n",
    "        #add data points \n",
    "        plt.scatter(x=data.loc[data['label']==label, 'x'], \n",
    "                y=data.loc[data['label']==label,'y'], \n",
    "                color=color[i], \n",
    "                alpha=0.20)\n",
    "\n",
    "        #add label\n",
    "        plt.annotate(label, \n",
    "                 data.loc[data['label']==label,['x','y']].mean(),\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 size=20, weight='bold',\n",
    "                 color=color[i])\n",
    "        \n",
    "def get_directory_layers_from_csv(filename,destination):\n",
    "    tokens=filename.split(\"_\")\n",
    "    print(tokens)\n",
    "    tokens[0]=tokens[0].split(\"/\")[len(tokens[0].split(\"/\"))-1]\n",
    "    df = pd.read_csv(filename, sep = ',', header = None) \n",
    "\n",
    "    \n",
    "    # creation d'un répertoire pour sauver tous les fichiers\n",
    "    repertoire=filename[0:-4]\n",
    "    print(repertoire)\n",
    "    os.makedirs(repertoire, exist_ok=True)\n",
    "    string = destination+\"/\"+repertoire+'/'+tokens[0]+'_'\n",
    "    f=[]\n",
    "    filenames=[]\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        name_file=string+'l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'.csv'\n",
    "        print(name_file)\n",
    "        f.append(open(name_file, \"w\"))\n",
    "        filenames.append(name_file)\n",
    "        \n",
    "        \n",
    "    # sauvegarde du dataframe dans une chaîne de caracteres\n",
    "    ch = df.to_string(header=False,\n",
    "                  index=False,\n",
    "                  index_names=False).split('\\n')\n",
    "    vals = [','.join(ele.split()) for ele in ch]\n",
    "    \n",
    "    # sauvegarde dans des fichiers spécifiques par layer\n",
    "    token_layer=[]\n",
    "    token_exemples=[]\n",
    "    for nb_exemples in range (len(vals)):\n",
    "        deb=str(df[0][nb_exemples])+','\n",
    "        # 1 ligne correspond à une chaine\n",
    "        s=vals[nb_exemples]\n",
    "        listoftokens=re.findall(r'<b>,(.+?),</b>', s)\n",
    "        nb_layers=len(listoftokens)\n",
    "        \n",
    "        for nb_token in range (nb_layers):\n",
    "            save_token=''\n",
    "            save_token=deb+str(listoftokens[nb_token])+'\\n'\n",
    "            \n",
    "            f[nb_token].write(save_token)\n",
    "\n",
    "    # sauvegarde d'un fichier qui contient tous les layers en une fois\n",
    "    # récupération des données pour enlever les <b> et </b>\n",
    "    df_all=pd.DataFrame()\n",
    "    myindex=0\n",
    "    for nb_columns in range(df.shape[1]):\n",
    "        df[nb_columns]=df[nb_columns].astype(str)\n",
    "        if (df[nb_columns]!='<b>').all() and (df[nb_columns]!='</b>').all():\n",
    "            df_all[myindex]=df[nb_columns]\n",
    "            myindex+=1\n",
    "\n",
    "\n",
    "    # sauvegarde en .csv\n",
    "    df_all.to_csv(string, sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iris', '8', '10', '8', '.csv']\n",
      "iris_8_10_8_\n",
      "./KmeansI/iris_8_10_8_/iris_l1_8.csv\n",
      "./KmeansI/iris_8_10_8_/iris_l2_10.csv\n",
      "./KmeansI/iris_8_10_8_/iris_l3_8.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a directory with a specific file for all the layers\n",
    "filename=\"iris_8_10_8_.csv\"\n",
    "destination=\"./KmeansI\"\n",
    "get_directory_layers_from_csv(filename,destination)\n",
    "os.system ('rm ./KmeansI/iris_8_10_8_/iris_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_cluster(clusters,numLayer,cluster_class):\n",
    "\n",
    "    idClusters=np.unique(clusters)\n",
    "    tabCount={}\n",
    "    for id in idClusters:\n",
    "        count = 0\n",
    "        for j in range(len(clusters)):\n",
    "            if clusters[j]==id:\n",
    "                tabCount[id]=count+1\n",
    "                count+=1\n",
    "    print(tabCount)\n",
    "    my_string=\"\"\n",
    "    for key,value in tabCount.items():\n",
    "        my_string += str(numLayer)+','+str(cluster_class)+','\n",
    "        my_string+=str(value)+','+str(key)+'\\n'\n",
    "    \n",
    "    return my_string\n",
    "\n",
    "def save_result_cluster(filename,clustersArray):\n",
    "    f = open(filename, \"w\")\n",
    "    \n",
    "    f.write(\"Layer,Class,Count,Cluster\\n\")\n",
    "    for i in range(len(clustersArray)):\n",
    "        f.write(str(clustersArray[i]))\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "def clusterizeLayer(url, classtoanalyze, nbClusters=4):\n",
    "    data = pd.read_csv(url, header=None)\n",
    "\n",
    "    data.columns = ['classe']+[\"neurone{}\".format(i) for i in range(len(data.columns)-1)]\n",
    "    classes = pd.Series(data['classe'])\n",
    "    \n",
    "    data = data.drop(['classe'],axis= 1)\n",
    "        \n",
    "#     with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#         print(data)\n",
    "\n",
    "    algo = KMeans(n_clusters = nbClusters)\n",
    "    #fit_predict fait tourner l'algo et retourne y où y[i] est l'indice du cluster auquel dataC12[i] appartient\n",
    "    clustersC1C2 = algo.fit_predict(data)\n",
    "#     print(clustersC1C2)\n",
    "\n",
    "    csvname = classtoanalyze\n",
    "    dataC3=pd.read_csv(csvname, header=None)\n",
    "    dataC3.columns = ['classe']+[\"neurone{}\".format(i) for i in range(len(dataC3.columns)-1)]\n",
    "    dataC3 = dataC3.drop(['classe'],axis= 1)\n",
    "\n",
    "\n",
    "#     print(\"print dataC3:\\n\")\n",
    "#     print(dataC3)\n",
    "\n",
    "    #predict utilise les clusters calculés par fit_predict(dataC1C2) et attribue chaque ligne de dataC3 au cluster dont le centre\n",
    "    #minimise la distance\n",
    "    C3ClusterPred = algo.predict(dataC3)\n",
    "\n",
    "#     print(\"Clusters C3:\\n\")\n",
    "#     print(C3ClusterPred)\n",
    "\n",
    "    clustersC1 = clustersC1C2[classes == 0]\n",
    "    clustersC2 = clustersC1C2[classes == 1]\n",
    "\n",
    "    \n",
    "    return (C3ClusterPred,clustersC1,clustersC2)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPLEMENTATION DE LA CLASSE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(url, names=names)\n",
    "data=data[data['Species'].isin(['Iris-versicolor'])].drop(['Species'],axis= 1)\n",
    "classe3 = model.predict_classes(data)\n",
    "#print(classe3)\n",
    "\n",
    "result_layersC3=get_result_layers(model,data)\n",
    "#print(result_layersC3)\n",
    "\n",
    "#On rajoute une première colonne avec la classe prédite pour être cohérent avec les autres csv\n",
    "for i,arr  in enumerate(result_layersC3):\n",
    "    arr =  np.insert(arr,0,classe3[i], axis=1)\n",
    "    result_layersC3[i] = arr\n",
    "                                               \n",
    "rep = \"./KmeansI/iristest_8_10_8_/\"\n",
    "os.makedirs(\"./KmeansI/iristest_8_10_8_\", exist_ok=True)\n",
    "for i, (filename, arr) in enumerate(zip([\"iris_l1_8.csv\", \"iris_l2_10.csv\", \"iris_l3_8.csv\"],result_layersC3)):\n",
    "    np.savetxt(rep+filename, arr, delimiter=\",\", fmt=\"%f\")\n",
    "os.system ('rm ./KmeansI/iris_8_10_8_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialisation part: reading the files in the directory ./KmeansI\n",
      "Compute the clustering for the original data\n",
      "Layer 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Cluster'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-69d864e95de5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mkmeans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNB_CLUSTERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"iris_l1_8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cawosh/.local/lib/python3.6/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"C\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32],\n\u001b[0;32m--> 859\u001b[0;31m                         order=order, copy=self.copy_x)\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;31m# verify that the number of samples given is larger than k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cawosh/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m/home/cawosh/.local/lib/python3.6/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Cluster'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "\n",
    "\n",
    "################## PARAMETERS ###############\n",
    "# DIRECTORY STANDS FOR THE DIRECTORY TO READ\n",
    "DIRECTORY=\"./KmeansI\"\n",
    "NB_CLUSTERS=4\n",
    "JSONFILE=\"myJson.json\"\n",
    "# CLASS_TEST IS THE NUMBER OF THE CLASS TO TEST\n",
    "# IT MUST BE SET AT THE HIGHEST RANK\n",
    "CLASS_TEST=3\n",
    "# TEST_EXIST allows to know if there is a test to perform\n",
    "# it is upadated if there is a directory with the keyword test for testing a class\n",
    "TEST_EXIST=False\n",
    "\n",
    "#############################################\n",
    "\n",
    "#############################################\n",
    "#########       FUNCTIONS      ##############\n",
    "############################################# \n",
    "def get_clusters_forclasses(y,y_predict):\n",
    "    df_y_nbdifferentvalues=pd.DataFrame(y)\n",
    "    list_unique_value_y=df_y_nbdifferentvalues[0].unique()\n",
    "    clusters=[]\n",
    "    for i in range (len(y_predict)):\n",
    "        dict_clusters={}\n",
    "        for j in range (len(list_unique_value_y)):\n",
    "            if y[i]==list_unique_value_y[j]:\n",
    "                dict_clusters['y']=y[i]\n",
    "                dict_clusters['y_predict']=y_predict[i]\n",
    "                clusters.append(dict_clusters)\n",
    "    df_clusters = pd.DataFrame(clusters)\n",
    "    clustersperclass=[]\n",
    "    objectsperclusterperclass=[]\n",
    "    for j in range (len(list_unique_value_y)):\n",
    "        objectsperclusterperclass=[]\n",
    "        dict_clustersperclass={}\n",
    "        df_y=df_clusters.loc[df_clusters['y'] == list_unique_value_y[j]]\n",
    "        print (\"Clusters de la classe #\",\n",
    "               list_unique_value_y[j],\":\",df_y['y_predict'].unique())\n",
    "        dict_clustersperclass['class']=list_unique_value_y[j]\n",
    "        dict_clustersperclass['clusters']=df_y['y_predict'].unique()\n",
    "        dict_clustersperclass['nbobjects']=np.nan\n",
    "        df=pd.DataFrame(df_y['y_predict'].value_counts())\n",
    "        df = df.reset_index()\n",
    "        df.columns = ['cluster', 'counts']\n",
    "        for nb_objects in range(len(df)):\n",
    "            val_to_test=df['cluster'].loc[nb_objects] #number of the cluster\n",
    "            for nb_clusters in range (len(dict_clustersperclass['clusters'])):\n",
    "                if val_to_test==dict_clustersperclass['clusters'][nb_clusters]:\n",
    "                        objectsperclusterperclass.append(df['counts'].loc[nb_objects])    \n",
    "        dict_clustersperclass['nbobjects']=objectsperclusterperclass            \n",
    "        clustersperclass.append(dict_clustersperclass)\n",
    "        \n",
    "    df_clustersperclass = pd.DataFrame(clustersperclass)\n",
    "    return df_clusters,df_clustersperclass\n",
    "\n",
    "\n",
    "def build_dataframe_forJson(df_objectsclasslayers_ext):\n",
    "    '''\n",
    "    This function creates a dataframe that will be used to generate the links and nodes \n",
    "    of the Json for the Sankey.\n",
    "    \n",
    "    Input : df_objectsclasslayers_ext example\n",
    "                class  l0  l1  l2    l0r    l1r    l2r  predict\n",
    "            0      0   1   1   1  Cl1_1  Cl2_1  Cl3_1        0\n",
    "            1      0   1   1   1  Cl1_1  Cl2_1  Cl3_1        0\n",
    "            2      0   1   1   1  Cl1_1  Cl2_1  Cl3_1        0\n",
    "            3      0   1   1   1  Cl1_1  Cl2_1  Cl3_1        0\n",
    "            4      0   1   1   1  Cl1_1  Cl2_1  Cl3_1        0\n",
    "            \n",
    "    Ouput : \n",
    "        - a dataframe for the links (df_forJsonLinks)  \n",
    "        - a dataframe for the nodes (df_forJsonNodes)\n",
    "    Example of df_forJsonLinks:\n",
    "                    class source target  value\n",
    "            0       0     X0    Cl1_1     50\n",
    "            1       1     X1    Cl1_0     25\n",
    "            2       1     X1    Cl1_2     10\n",
    "            3       1     X1    Cl1_3     15\n",
    "            4       3     X3    Cl1_1     50\n",
    "            5       0  Cl1_1    Cl2_1     50\n",
    "    Each line stands for an entry with the class, source, target and value columns.\n",
    "    At line 0, 0 stands for the number of the initial, \n",
    "    X0 means the input of the neural network. Each class has the number of the corresponding layer.\n",
    "    For instance Cl1_1 means that it is a the cluster C1 at the layer 1 (l1).\n",
    "    Value stands for the number of objects  \n",
    "    \n",
    "    Example of df_forJsonNodes:\n",
    "                source class shared\n",
    "            0      X0     0  false\n",
    "            1      X1     1   true\n",
    "            2      X3     3  false\n",
    "            3   Cl1_1     0   true\n",
    "            4   Cl1_0     1   true\n",
    "    Each line stands for a node. Source is the name, class is the initial class of the node.\n",
    "    if shared is true that means that other clusters are using this cluster (cluster multiclass or\n",
    "    test class)\n",
    "        \n",
    "    '''\n",
    "    #  build the dataframe to create the Json file\n",
    "    # Get the list of the columns, this will be used to combine the layers two per two\n",
    "    # The dataframe has n columns, 2 for class and predict then (n-2)/2 is the number\n",
    "    # where the name for Json is available, i.e. X0\n",
    "    start_column=int((df_objectsclasslayers_ext.shape[1])/2)\n",
    "    list_columns=list(df_objectsclasslayers_ext)\n",
    "    del list_columns[0:start_column]\n",
    "    df_forJsonLinks=pd.DataFrame()#the dataframe that will store the result\n",
    "    # Link part\n",
    "    # the following loop will construct the dataframe from the input to the last layer\n",
    "    for i in range(len(list_columns)-1):\n",
    "        df_val=df_objectsclasslayers_ext[['class',list_columns[i],list_columns[i+1]]]\n",
    "        result=df_val.groupby(['class',list_columns[i],list_columns[i+1]]).size().reset_index()\n",
    "        result.columns = ['class','source', 'target','value']\n",
    "        df_forJsonLinks=pd.concat([df_forJsonLinks,result])    \n",
    "    df_forJsonLinks.index = range(len(df_forJsonLinks.index))\n",
    "    \n",
    "    # Node part\n",
    "    df_forJsonNodes=pd.DataFrame()\n",
    "    df_forJsonNodes=df_forJsonLinks[['source','class']].copy()\n",
    "    df_forJsonNodes['shared']='false'\n",
    "    df_dup=pd.concat(g for _, g in df_forJsonNodes.groupby([df_forJsonNodes['source']]) if len(g) > 1)\n",
    "    df_dup=df_dup.reset_index()\n",
    "    for nb_transform in range (len(df_dup)):\n",
    "        df_forJsonNodes['shared'].iloc[df_dup['index'][nb_transform]]='true'\n",
    "    df_forJsonNodes=df_forJsonNodes.drop_duplicates(subset ='source').reset_index(drop=True)\n",
    "    # adding the number of classes from predict, i.e. the last layer\n",
    "    df_nb = df_objectsclasslayers_ext['predict'].value_counts().rename_axis('predict').reset_index(name='counts')\n",
    "    nb=df_nb['predict']\n",
    "    for nb_classes in range (len(nb)):\n",
    "        df_forJsonNodes = df_forJsonNodes.append({'source' : str(nb[nb_classes]) , \n",
    "                                      'class' : str(nb[nb_classes]),\n",
    "                                      'shared': 'false'},\n",
    "                                        ignore_index=True)\n",
    "    df_forJsonNodes=df_forJsonNodes.reset_index(drop=True)  \n",
    "    \n",
    "    return df_forJsonLinks,df_forJsonNodes\n",
    "\n",
    "\n",
    "\n",
    "###############################################\n",
    "########             SEARCH FILES         #####\n",
    "###############################################\n",
    "\n",
    "# Extract the list of directories\n",
    "print (\"Initialisation part: reading the files in the directory\",DIRECTORY)\n",
    "content_directory=os.listdir(DIRECTORY)\n",
    "list_dir=[]\n",
    "for file_dir in range(len(content_directory)):\n",
    "    if os.path.isdir(DIRECTORY+'/'+content_directory[file_dir]):\n",
    "        list_dir.append(DIRECTORY+'/'+content_directory[file_dir]) \n",
    "\n",
    "\n",
    "# Get the different files in order to apply the clustering\n",
    "list_files_to_learn=[]\n",
    "list_files_to_test=[]\n",
    "for nb_files in range (len(list_dir)):\n",
    "    content=os.listdir(list_dir[nb_files])\n",
    "    if \"test\" not in list_dir[nb_files]:\n",
    "        for nb_files_in_directory in range (len(content)):\n",
    "            if \"DS\" not in content[nb_files_in_directory]:\n",
    "                list_files_to_learn.append(list_dir[nb_files]+'/'+content[nb_files_in_directory])\n",
    "    else:\n",
    "        TEST_EXIST=True\n",
    "        for nb_files_in_directory in range (len(content)):\n",
    "            if \"DS\" not in content[nb_files_in_directory]:\n",
    "                list_files_to_test.append(list_dir[nb_files]+'/'+content[nb_files_in_directory])\n",
    "                \n",
    "# sort by alphabetical order the files                \n",
    "list_files_to_learn=sorted(list_files_to_learn)     \n",
    "if TEST_EXIST:\n",
    "    list_files_to_test=sorted(list_files_to_test)   \n",
    "     \n",
    "\n",
    "###############################################\n",
    "##################   CLUSTERING ###############\n",
    "###############################################\n",
    "\n",
    "print (\"Compute the clustering for the original data\")\n",
    "'''\n",
    "Apply the kmeans for all layer for the original data\n",
    "\n",
    "- k_means_perlayer stores the return of the k_means for each layer. It will be used\n",
    "for predicting the test\n",
    "- cluster_classes_perlayer is an array of dataframe. It has as many dataframe as layers.\n",
    "Each dataframe is obtained by the get_clusters_forclasses function\n",
    "Example:\n",
    "    [class clusters nbobjects\n",
    "0    0.0   [1, 3]  [28, 22]\n",
    "1    1.0   [2, 0]  [38, 12],    \n",
    "    class clusters nbobjects\n",
    "0    0.0   [3, 1]  [26, 24]\n",
    "1    1.0   [0, 2]  [36, 14]]\n",
    "Each dataframe has for each layer the class, the corresponding clusters as well as the number of objects\n",
    "for each cluster. For instance,  0.0   [1, 3]  [28, 22]  means that at the first layer, for the class 0, \n",
    "there are 2 clusters (1 and 3) and the number of objects of cluster 1 is 28 (resp. 22 for cluster 3).\n",
    "- objects_cluster_perlayer is an array of dataframe which has as many dataframes as layers.\n",
    "Each dataframe is obtained by the get_clusters_forclasses function. For each layer it stores for each object \n",
    "the class and the y_predict from the k-means, i.e. the number of the cluster.\n",
    "Example:\n",
    "    [      y  y_predict\n",
    "    0   0.0          1\n",
    "    1   0.0          1\n",
    "    2   0.0          1\n",
    "    ...],\n",
    "    [      y  y_predict\n",
    "    0   0.0          2\n",
    "    1   0.0          2\n",
    "0   0.0          2 means that at the second layer (2nd row of the array), for the object 0 (index), \n",
    "the class (y) is 0 and the predicted cluster is 2.\n",
    "'''\n",
    "kmeans_per_layer=[]\n",
    "cluster_classes_perlayer=[]\n",
    "objects_cluster_perlayer=[]\n",
    "for nb_layers in range (len(list_files_to_learn)):\n",
    "    print(\"Layer\",nb_layers+1)\n",
    "    df = pd.read_csv(list_files_to_learn[nb_layers], sep = ',', header = None)\n",
    "    array = df.values\n",
    "    y = array[:,0]\n",
    "    X = array[:,1:df.shape[1]] \n",
    "    kmeans=KMeans(n_clusters=NB_CLUSTERS, random_state=30).fit(X)\n",
    "    y_predict = kmeans.predict(X)\n",
    "    title=\"iris_l1_8\"\n",
    "    #plot_clusters_2D (\"irisl1.png\",legend,title,X,y,y_predict1, nb_clusters)\n",
    "    df_clusters,df_clustersperclass=get_clusters_forclasses(y,y_predict)\n",
    "    cluster_classes_perlayer.append(df_clustersperclass)\n",
    "    objects_cluster_perlayer.append(df_clusters)\n",
    "    kmeans_per_layer.append(kmeans)\n",
    "\n",
    "if TEST_EXIST:\n",
    "    print (\"\\nCompute the clustering for the test class\\n\")\n",
    "    # Apply the kmeans predict for all layer for the test data\n",
    "    cluster_test_perlayer=[]\n",
    "    objectstest_cluster_perlayer=[]\n",
    "    if list_files_to_test:\n",
    "        for nb_layers in range (len(list_files_to_test)):\n",
    "            print(\"Layer\",nb_layers+1)\n",
    "            df = pd.read_csv(list_files_to_test[nb_layers], sep = ',', header = None)\n",
    "            array = df.values\n",
    "            y = array[:,0]\n",
    "            X = array[:,1:df.shape[1]] \n",
    "            y_predict = kmeans_per_layer[nb_layers].predict(X)\n",
    "            title=\"iris_l1_8\"\n",
    "            #plot_clusters_2D (\"irisl1.png\",legend,title,X,y,y_predict1, nb_clusters)\n",
    "            df_clusters_test,df_clustersperclass=get_clusters_forclasses(y,y_predict)\n",
    "            cluster_test_perlayer.append(df_clustersperclass)\n",
    "            objectstest_cluster_perlayer.append(df_clusters_test)\n",
    "            kmeans_per_layer.append(kmeans)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################\n",
    "########  CREATION OF JSON FILE ###############\n",
    "###############################################\n",
    "\n",
    "# Build the array of objects, class, layer1, layer2, ..., layern\n",
    "'''\n",
    "objects_cluster is an array of array (i.e. the number of objects in the dataset\n",
    " where for each object we have :\n",
    "   - its class, the cluster of layer1, the cluster of layer 2, ...\n",
    "Example :  \n",
    "    [[0. 1. 3. 1.]\n",
    "     [0. 1. 3. 1.]\n",
    "     [0. 1. 3. 1.]\n",
    "The first three objects of the dataset are in class 0 (0.), are in cluster 1 at layer 0, cluster 2 \n",
    "at layer 1, 1 at layer 2.\n",
    "Be carefull layers are numeroted from 0.      \n",
    "'''\n",
    "objects_cluster=objects_cluster_perlayer[0]['y'].copy()\n",
    "for nb_layers in range (len(objects_cluster_perlayer)):\n",
    "    objects_cluster=np.c_[objects_cluster,objects_cluster_perlayer[nb_layers]['y_predict']]\n",
    "\n",
    "if TEST_EXIST:\n",
    "    # Build the array for test objects, class, layer1, layer2, ..., layern\n",
    "    objectstest_cluster=objectstest_cluster_perlayer[0]['y'].copy()\n",
    "    for nb_layers in range (len(objects_cluster_perlayer)):\n",
    "        objectstest_cluster=np.c_[objectstest_cluster,objectstest_cluster_perlayer[nb_layers]['y_predict']]\n",
    "            \n",
    "\n",
    "'''\n",
    "Creation of a unique dataframe for original data and test\n",
    "At the end,         \n",
    "df_object_class_layers is the dataframe that store for each occurrence \n",
    "its class and the clusters for each layer\n",
    "Example:\n",
    "   class   l0   l1   l2\n",
    "0    0.0  1.0  3.0  1.0\n",
    "1    0.0  1.0  3.0  1.0\n",
    "2    0.0  1.0  3.0  1.0\n",
    "3    0.0  1.0  3.0  1.0\n",
    "4    0.0  1.0  3.0  1.0\n",
    "'''\n",
    "nb_layers= len(objects_cluster_perlayer)   \n",
    "columname=[]\n",
    "columname.append('class')\n",
    "for i in range (nb_layers):\n",
    "    columname.append('l'+str(i+1))\n",
    "df_original=pd.DataFrame(objects_cluster,columns=columname)\n",
    "# nb_X_original will be used later to know how many test are added\n",
    "NB_X_ORIGINAL=df_original.shape[0]\n",
    "if TEST_EXIST:\n",
    "    df_test=pd.DataFrame(objectstest_cluster, columns=columname)\n",
    "    # Concatenate original and test\n",
    "    df_object_class_layers=pd.concat([df_original, df_test])\n",
    "    df_object_class_layers = df_object_class_layers.reset_index(drop=True)\n",
    "else:\n",
    "    df_object_class_layers=df_original.copy()\n",
    "    df_object_class_layers = df_object_class_layers.reset_index(drop=True)\n",
    "\n",
    "\n",
    "############ CREATE AN EXTENDED DATAFRAME ##############\n",
    "'''\n",
    "Creation of a unique dataframe for original data and test. It is an extension\n",
    "of the previous one :\n",
    "    - convert in int\n",
    "    - add columns with the name of the clusters for Json (l0r, l1r, ...)\n",
    "    - add a new column (predict) for the prediction of the value\n",
    "    - the lines corresponding to the test data are replaced by the number of the class, i.e.\n",
    "    if we have original data as class 0 and 1, the test class is 2.\n",
    "df_objectsclasslayers_ext example:\n",
    "    class  l0  l1  l2    l0r    l1r    l2r  predict\n",
    "0      0   1   1   1  Cl1_1  Cl2_1  Cl3_1        0\n",
    "1      0   1   1   1  Cl1_1  Cl2_1  Cl3_1        0\n",
    "2      0   1   1   1  Cl1_1  Cl2_1  Cl3_1        0\n",
    "3      0   1   1   1  Cl1_1  Cl2_1  Cl3_1        0\n",
    "4      0   1   1   1  Cl1_1  Cl2_1  Cl3_1        0\n",
    "'''\n",
    "df_objectsclasslayers_ext=df_object_class_layers.copy()\n",
    "# convert to int\n",
    "df_objectsclasslayers_ext=df_objectsclasslayers_ext.astype(int)\n",
    "\n",
    "df_tempo=df_objectsclasslayers_ext['class'].copy()\n",
    "if TEST_EXIST:\n",
    "    df_objectsclasslayers_ext['class'][NB_X_ORIGINAL:]=CLASS_TEST\n",
    "for nb_layers in range(df_objectsclasslayers_ext.shape[1]-1):\n",
    "    if nb_layers==0:\n",
    "        df_objectsclasslayers_ext['l0r']=df_objectsclasslayers_ext['class'].apply(lambda x: 'X'+str(int(x)))\n",
    "    else:\n",
    "        df_objectsclasslayers_ext['l'+str(nb_layers)+'r']=\\\n",
    "        df_objectsclasslayers_ext['l'+str(nb_layers)].apply(lambda x: 'Cl'+str(nb_layers)+'_'+str(int(x)))\n",
    "df_objectsclasslayers_ext['predict']=df_tempo\n",
    "df_objectsclasslayers_ext['class']=df_objectsclasslayers_ext['class'].apply(lambda x: 'C'+str(x))\n",
    "df_objectsclasslayers_ext['predict']=df_objectsclasslayers_ext['predict'].apply(lambda x: 'C'+str(x))\n",
    "print (df_objectsclasslayers_ext['class'].head())\n",
    "\n",
    "\n",
    "# CREATE THE JSON\n",
    "\n",
    "# Call the function to create the dataframe for the Json\n",
    "df_forJsonLinks,df_forJsonNodes=build_dataframe_forJson(df_objectsclasslayers_ext)    \n",
    "\n",
    "# Create the link part of the Json\n",
    "links_json=\"\\\"links\\\":[\\n\"\n",
    "for i in range (len(df_forJsonLinks)):\n",
    "    links_json+=\"{\"+\"\\\"source\\\"\"+':'+\"\\\"\"+str(df_forJsonLinks['source'].loc[i])+\"\\\",\"\n",
    "    links_json+=\"\\\"target\\\"\"+':'+\"\\\"\"+str(df_forJsonLinks['target'].loc[i])+\"\\\",\"\n",
    "    links_json+=\"\\\"value\\\"\"+':'+\"\\\"\"+str(df_forJsonLinks['value'].loc[i])+\"\\\",\"\n",
    "    links_json+=\"\\\"classname\\\"\"+':'+\"\\\"\"+str(df_forJsonLinks['class'].loc[i])+\"\\\"}\"\n",
    "    if i != len(df_forJsonLinks)-1:\n",
    "        links_json+=\",\\n\"\n",
    "    else:\n",
    "        links_json+=\"\\n]\"# pas de saut de ligne pour mettre la virgule avant node \n",
    "\n",
    "\n",
    "# Create the node part of the Json\n",
    "nodes_json=\"\\\"nodes\\\":[\\n\"\n",
    "for i in range (len(df_forJsonNodes)):\n",
    "    nodes_json+=\"{\"+\"\\\"name\\\"\"+':'+\"\\\"\"+str(df_forJsonNodes['source'].loc[i])+\"\\\",\"\n",
    "    nodes_json+=\"\\\"classname\\\"\"+':'+\"\\\"\"+str(df_forJsonNodes['class'].loc[i])+\"\\\",\"\n",
    "    nodes_json+=\"\\\"shared\\\"\"+':'+\"\\\"\"+str(df_forJsonNodes['shared'].loc[i])+\"\\\"}\"\n",
    "    if i != len(df_forJsonNodes)-1:\n",
    "        nodes_json+=\",\\n\"\n",
    "    else:\n",
    "        nodes_json+=\"\\n]\\n\"  \n",
    "\n",
    "final_json='{\\n'+links_json+',\\n'+nodes_json+'}'\n",
    "print (\"The final json is \\n\",final_json)\n",
    "\n",
    "# save the file\n",
    "with open(JSONFILE, 'w') as f:\n",
    "    f.write(final_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustersC3l1,clustersC1l1,clustersC2l1=clusterizeLayer(destination+'/'+'iris_8_10_8_/iris_l1_8.csv',destination+'/iristest_8_10_8_/iris_l1_8.csv')\n",
    "clustersC3l2,clustersC1l2,clustersC2l2=clusterizeLayer(destination+'/iris_8_10_8_/iris_l2_10.csv',destination+'/iristest_8_10_8_/iris_l2_10.csv')\n",
    "clustersC3l3,clustersC1l3,clustersC2l3=clusterizeLayer(destination+'/iris_8_10_8_/iris_l3_8.csv',destination+'/iristest_8_10_8_/iris_l3_8.csv')\n",
    "\n",
    "C1l1=get_result_cluster(clustersC1l1,1,1)\n",
    "C1l2=get_result_cluster(clustersC1l2,2,1)\n",
    "C1l3=get_result_cluster(clustersC1l3,3,1)\n",
    "\n",
    "C2l1=get_result_cluster(clustersC2l1,1,2)\n",
    "C2l2=get_result_cluster(clustersC2l2,2,2)\n",
    "C2l3=get_result_cluster(clustersC2l3,3,2)\n",
    "\n",
    "C3l1=get_result_cluster(clustersC3l1,1,3)\n",
    "C3l2=get_result_cluster(clustersC3l2,2,3)\n",
    "C3l3=get_result_cluster(clustersC3l3,3,3)\n",
    "\n",
    "save_result_cluster(destination+\"/iris_8_10_8_/iris_clusters_layer1.csv\",[C1l1,C2l1,C3l1])\n",
    "save_result_cluster(destination+\"/iris_8_10_8_/iris_clusters_layer2.csv\",[C1l2,C2l2,C3l2])\n",
    "save_result_cluster(destination+\"/iris_8_10_8_/iris_clusters_layer3.csv\",[C1l3,C2l3,C3l3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphCluster(layer,title):\n",
    "    names = ['cluster0c1', 'cluster0c2', 'cluster0c3' ,\n",
    "             'cluster1c1', 'cluster1c2', 'cluster1c3' , \n",
    "             'cluster2c1', 'cluster2c2', 'cluster2c3',\n",
    "             'cluster3c1', 'cluster3c2', 'cluster3c3']\n",
    "\n",
    "    cluster0 = [0,0,0]\n",
    "    cluster1 = [0,0,0]\n",
    "    cluster2 = [0,0,0]\n",
    "    cluster3 = [0,0,0]\n",
    "\n",
    "    for index,element in layer.iterrows():\n",
    "        if element[\"Cluster\"]==0:\n",
    "            cluster0[element[\"Class\"] - 1]=element[\"Count\"]\n",
    "        if element[\"Cluster\"]==1:\n",
    "            cluster1[element[\"Class\"]-1]=element[\"Count\"]\n",
    "        if element[\"Cluster\"]==2:\n",
    "            cluster2[element[\"Class\"]-1]=element[\"Count\"]\n",
    "        if element[\"Cluster\"]==3:\n",
    "            cluster3[element[\"Class\"]-1]=element[\"Count\"]\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    values = cluster0 + cluster1 + cluster2 +cluster3 \n",
    "#     print(cluster0)\n",
    "#     print(cluster1)\n",
    "#     print(cluster2)\n",
    "#     print(cluster3)\n",
    "\n",
    "    colors = {'Classe 1':'#f91212', 'Classe 2':'#FFAA00', 'Classe 3':'#0055FF'}\n",
    "    labels = list(colors.keys())\n",
    "    handles = [plt.bar(names,values,color=colors[label],width=1) for label in labels]\n",
    "    plt.legend(handles, labels)\n",
    "    plt.bar(names,values,color = list(colors.values()),width=1)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layer1 = pd.read_csv(destination+\"/iris_8_10_8_/iris_clusters_layer1.csv\")\n",
    "print(\"layer1:\\n\",layer1)\n",
    "graphCluster(layer1,'Clusters du layer 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2 = pd.read_csv(destination+\"/iris_8_10_8_/iris_clusters_layer2.csv\")\n",
    "print(\"layer1:\\n\",layer2)\n",
    "graphCluster(layer2,'Clusters du layer 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layer3 = pd.read_csv(destination+\"/iris_8_10_8_/iris_clusters_layer3.csv\")\n",
    "print(\"layer1:\\n\",layer3)\n",
    "graphCluster(layer3,'Clusters du layer 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "from numpy import load\n",
    "np.set_printoptions(linewidth=500)\n",
    "data = load('mnist.npz')\n",
    "lst = data.files\n",
    "print(lst)\n",
    "\n",
    "X_class0_1 = []\n",
    "X_class2 = []\n",
    "y_class0_1 = []\n",
    "y_class2 = []\n",
    "\n",
    "\n",
    "for i in range(int(len(data[lst[1]])/100)):\n",
    "    if data[lst[2]][i] == 0:\n",
    "        X_class0_1.append(data[lst[1]][i])\n",
    "        y_class0_1.append(data[lst[2]][i])\n",
    "    if data[lst[2]][i] == 1:\n",
    "        X_class0_1.append(data[lst[1]][i])\n",
    "        y_class0_1.append(data[lst[2]][i])\n",
    "    if data[lst[2]][i] == 2:\n",
    "        X_class2.append(data[lst[1]][i])\n",
    "        y_class2.append(data[lst[2]][i])\n",
    "\n",
    "(X_train,y_train) = (X_class0_1,y_class0_1)\n",
    "(X_test,y_test) = (data[lst[0]],data[lst[3]])\n",
    "\n",
    "X_train_sample=X_train[0:100]\n",
    "y_train_sample=y_train[0:100]\n",
    "\n",
    "X_train=X_train_sample\n",
    "y_train=y_train_sample\n",
    "X_train = np.reshape(X_train, (784, 100)).T\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "\n",
    "X_01=[]\n",
    "y_01=[]\n",
    "nb_X=0\n",
    "for i in range(X_train.shape[0]):\n",
    "    if (y_train[i]==0 or y_train[i]==1):\n",
    "        \n",
    "        nb_X+=1\n",
    "        X_01.append(X_train[i])\n",
    "        y_01.append(y_train[i])\n",
    "\n",
    "       \n",
    "\n",
    "train_X=np.asarray(X_01)\n",
    "\n",
    "train_y=y_01\n",
    "nbClusters = 10\n",
    "encoder = LabelEncoder()\n",
    "train_y=encoder.fit_transform(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(512, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(512, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(512, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(512, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\n",
    "\n",
    "model.fit(train_X, train_y, epochs = 40, batch_size = 32)\n",
    "\n",
    "\n",
    "X_good,y_good=get_goodXy (train_X, train_y)\n",
    "\n",
    "\n",
    "# Récupération des valeurs de tous les layers sauf le dernier\n",
    "result_layers=get_result_layers(model,X_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du fichier\n",
    "# structure :\n",
    "# 0/1 = valeur de la classe\n",
    "# chaque valeur de layer est entourée par une étoile *\n",
    "save_result_layers(\"mnist_512_512_512_512_512_tmp\",X_good,y_good,result_layers)\n",
    "# tri du fichier\n",
    "os.system ('sort mnist_512_512_512_512_512_tmp > mnist_512_512_512_512_512_.csv')\n",
    "# effacer le fichier intermédiaire\n",
    "os.system ('rm mnist_512_512_512_512_512_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"mnist_512_512_512_512_512_.csv\"    \n",
    "destination=\"./KmeansM\"\n",
    "get_directory_layers_from_csv(filename,destination)\n",
    "os.system ('rm ./KmeansM/mnist_512_512_512_512_512_/mnist_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [X_class2,y_class2]\n",
    "\n",
    "arr = np.array(data)\n",
    "\n",
    "dfClass2 = pd.DataFrame(data=arr.flatten())\n",
    "\n",
    "print(dfClass2)\n",
    "\n",
    "# data = pd.read_csv(url, names=names)\n",
    "# data=data[data['Species'].isin(['Iris-versicolor'])].drop(['Species'],axis= 1)\n",
    "classe2 = model.predict_classes(dfClass2)\n",
    "#print(classe3)\n",
    "\n",
    "result_layersC2=get_result_layers(model,dfClass2)\n",
    "#print(result_layersC3)\n",
    "filenametest=\"mnisttest_512_512_512_512_512_.csv\"    \n",
    "destination=\"./KmeansM\"\n",
    "get_directory_layers_from_csv(filenametest,destination)\n",
    "#On rajoute une première colonne avec la classe prédite pour être cohérent avec les autres csv\n",
    "# for i,arr  in enumerate(result_layersC2):\n",
    "#     arr =  np.insert(arr,0,classe2[i], axis=1)\n",
    "#     result_layersC3[i] = arr\n",
    "                                               \n",
    "# rep = \"./KmeansM/mnist_512_512_512_512_512_/\"\n",
    "# os.makedirs(\"./KmeansM/mnist_512_512_512_512_512_\", exist_ok=True)\n",
    "# for i, (filename, arr) in enumerate(zip([\"iris_l1_8.csv\", \"iris_l2_10.csv\", \"iris_l3_8.csv\"],result_layersC3)):\n",
    "#     np.savetxt(rep+filename, arr, delimiter=\",\", fmt=\"%f\")\n",
    "os.system ('rm ./KmeansI/iris_8_10_8_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustersC3l1,clustersC1l1,clustersC2l1=clusterizeLayer('./mnist_512_512_512_512_512_/mnist_l1_512.csv',10,dfClass2)\n",
    "clustersC3l2,clustersC1l2,clustersC2l2=clusterizeLayer('./mnist_512_512_512_512_512_/mnist_l2_512.csv',10,dfClass2)\n",
    "clustersC3l3,clustersC1l3,clustersC2l3=clusterizeLayer('./mnist_512_512_512_512_512_/mnist_l3_512.csv',10,dfClass2)\n",
    "clustersC3l4,clustersC1l4,clustersC2l4=clusterizeLayer('./mnist_512_512_512_512_512_/mnist_l4_512.csv',10,dfClass2)\n",
    "clustersC3l5,clustersC1l5,clustersC2l5=clusterizeLayer('./mnist_512_512_512_512_512_/mnist_l5_512.csv',10,dfClass2)\n",
    "\n",
    "C1l1=get_result_cluster(clustersC1l1,1,1)\n",
    "C1l2=get_result_cluster(clustersC1l2,2,1)\n",
    "C1l3=get_result_cluster(clustersC1l3,3,1)\n",
    "C1l4=get_result_cluster(clustersC1l4,4,1)\n",
    "C1l5=get_result_cluster(clustersC1l5,5,1)\n",
    "\n",
    "C2l1=get_result_cluster(clustersC2l1,1,2)\n",
    "C2l2=get_result_cluster(clustersC2l2,2,2)\n",
    "C2l3=get_result_cluster(clustersC2l3,3,2)\n",
    "C2l4=get_result_cluster(clustersC2l2,4,2)\n",
    "C2l5=get_result_cluster(clustersC2l3,5,2)\n",
    "\n",
    "C3l1=get_result_cluster(clustersC3l1,1,3)\n",
    "C3l2=get_result_cluster(clustersC3l2,2,3)\n",
    "C3l3=get_result_cluster(clustersC3l3,3,3)\n",
    "C3l4=get_result_cluster(clustersC3l2,4,3)\n",
    "C3l5=get_result_cluster(clustersC3l3,5,3)\n",
    "\n",
    "save_result_cluster(\"./mnist_512_512_512_512_512_/mnist_clusters_layer1.csv\",[C1l1,C2l1,C3l1])\n",
    "save_result_cluster(\"./mnist_512_512_512_512_512_/mnist_clusters_layer2.csv\",[C1l2,C2l2,C3l2])\n",
    "save_result_cluster(\"./mnist_512_512_512_512_512_/mnist_clusters_layer3.csv\",[C1l3,C2l3,C3l3])\n",
    "save_result_cluster(\"./mnist_512_512_512_512_512_/mnist_clusters_layer4.csv\",[C1l4,C2l4,C3l4])\n",
    "save_result_cluster(\"./mnist_512_512_512_512_512_/mnist_clusters_layer5.csv\",[C1l5,C2l5,C3l5])\n",
    "\n",
    "merge_csv(\"./mnist_512_512_512_512_512_/mnist_clusters_layers.csv\",[\"./mnist_512_512_512_512_512_/mnist_clusters_layer1.csv\",\"./mnist_512_512_512_512_512_/mnist_clusters_layer2.csv\",\"./mnist_512_512_512_512_512_/mnist_clusters_layer3.csv\",\"./mnist_512_512_512_512_512_/mnist_clusters_layer4.csv\",\"./mnist_512_512_512_512_512_/mnist_clusters_layer5.csv\"] )\n",
    "convert_to_sankey('./affichageWeb/sankeyMnist.csv',\"./mnist_512_512_512_512_512_/mnist_clusters_layers.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
