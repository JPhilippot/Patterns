{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.265565Z",
     "start_time": "2020-02-13T18:33:00.211348Z"
    },
    "scrolled": true
   },
   "outputs": [
   
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import keras\n",
    "import os\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.282036Z",
     "start_time": "2020-02-13T18:33:02.268265Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_goodXy (X,y):\n",
    "    ynew = model.predict_classes(X)\n",
    "    X_good =[]\n",
    "    y_good=[]\n",
    "    for i in range(len(X)):\n",
    "        if (ynew[i]==0 and y[i]==1) or (ynew[i]==1 and y[i]==0):\n",
    "            print (\"error prediction for X=%s, Predicted=%s, Real=%s\"% (X[i], ynew[i], y[i]))\n",
    "        else :\n",
    "            X_good.append(X[i])\n",
    "            y_good.append(y[i])\n",
    "    return X_good,y_good        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.299211Z",
     "start_time": "2020-02-13T18:33:02.285314Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_result_layers(model,X):\n",
    "    result_layers=[]\n",
    "    for i in range (len(model.layers)-1):\n",
    "        hidden_layers= keras.backend.function(\n",
    "                [model.layers[0].input],   \n",
    "                [model.layers[i].output,] \n",
    "                )    \n",
    "        result_layers.append(hidden_layers([X])[0])  \n",
    "    return result_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:02.324742Z",
     "start_time": "2020-02-13T18:33:02.303088Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_result_layers(filename,X,y,result_layers):\n",
    "    f = open(filename, \"w\")\n",
    "    for nb_X in range (len(X)):\n",
    "        #my_string=\"\"\n",
    "        my_string=str(y[nb_X])+','\n",
    "        for nb_layers in range (len(model.layers)-1):\n",
    "            my_string+=\"<b>,\"\n",
    "            for j in range (len(result_layers[nb_layers][nb_X])):\n",
    "                my_string+=str(result_layers[nb_layers][nb_X][j])+','\n",
    "            my_string+=\"</b>,\"    \n",
    "        my_string=my_string [0:-1]\n",
    "        my_string+='\\n'\n",
    "        f.write(my_string)    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:03.159174Z",
     "start_time": "2020-02-13T18:33:02.327414Z"
    }
   },
   "outputs": [],
   "source": [
    "url=\"./iris.csv\"\n",
    "names = ['SepalLengthCm', 'SepalWidthCm', \n",
    "         'PetalLengthCm', 'PetalWidthCm', \n",
    "         'Species']\n",
    "\n",
    "data = pd.read_csv(url, names=names)\n",
    "\n",
    "#Classification binaire sur Virginica et Setosa seulement\n",
    "data=data[data['Species'].isin(['Iris-virginica', 'Iris-setosa'])]\n",
    "\n",
    "i = 8\n",
    "data_to_predict = data[:i].reset_index(drop = True) \n",
    "\"\"\"\n",
    "reset_index() is a method to reset index of a Data Frame. \n",
    "reset_index() method sets a list of integer ranging from 0 to length of data as index. \n",
    "\"\"\"\n",
    "predict_species = data_to_predict.Species \n",
    "\"\"\" Species de la class Iris \"\"\"\n",
    "\n",
    "predict_species = np.array(predict_species) \n",
    "\"\"\"\n",
    "An array object satisfying the specified requirements.\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html\n",
    "\"\"\"\n",
    "\n",
    "prediction = np.array(data_to_predict.drop(['Species'],axis= 1))\n",
    "\n",
    "data = data[i:].reset_index(drop = True)\n",
    "\n",
    "X = data.drop(['Species'], axis = 1) \n",
    "\"\"\"\n",
    "The drop() function is used to drop specified labels from rows or columns.\n",
    "\"\"\"\n",
    "X = np.array(X)\n",
    "y = data['Species']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y=encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "train_X, test_X, train_y, test_y = model_selection.train_test_split(X,y,test_size = 0.1, random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.678441Z",
     "start_time": "2020-02-13T18:33:03.161667Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 1s 14ms/step - loss: 0.7776 - acc: 0.4390\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.6952 - acc: 0.4512\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.6732 - acc: 0.9024\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.6271 - acc: 1.0000\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.5136 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2748 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.0669 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.0174 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.0068 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.0040 - acc: 1.0000\n",
      "10/10 [==============================] - 0s 10ms/step\n",
      "\n",
      "acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Utilisation de keras comme classifieur\n",
    "# mettre sigmoid comme fonction car binaire. Attention 1 seul neurone en sortie\n",
    "input_dim = len(data.columns) - 1\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\n",
    "\n",
    "model.fit(train_X, train_y, epochs = 10, batch_size = 2)\n",
    "\n",
    "scores = model.evaluate(test_X, test_y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.723085Z",
     "start_time": "2020-02-13T18:33:04.681566Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Récupération seulement des bons classés\n",
    "X_good,y_good=get_goodXy (train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.796700Z",
     "start_time": "2020-02-13T18:33:04.725841Z"
    }
   },
   "outputs": [],
   "source": [
    "# Récupération des valeurs de tous les layers sauf le dernier\n",
    "result_layers=get_result_layers(model,X_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:33:04.834457Z",
     "start_time": "2020-02-13T18:33:04.799432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sauvegarde du fichier\n",
    "# structure :\n",
    "# 0/1 = valeur de la classe\n",
    "# chaque valeur de layer est entourée par un []\n",
    "save_result_layers(\"iris_8_10_8_8_10_8_8_10_8_tmp\",X_good,y_good,result_layers)\n",
    "# tri du fichier\n",
    "os.system ('sort iris_8_10_8_8_10_8_8_10_8_tmp > iris_8_10_8_8_10_8_8_10_8_.csv')\n",
    "# effacer le fichier intermédiaire\n",
    "os.system ('rm iris_8_10_8_8_10_8_8_10_8_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_layers_from_csv(filename):\n",
    "    tokens=filename.split(\"_\")\n",
    "    df = pd.read_csv(filename, sep = ',', header = None) \n",
    "\n",
    "    \n",
    "    # creation d'un répertoire pour sauver tous les fichiers\n",
    "    repertoire=filename[0:-4]\n",
    "    os.makedirs(repertoire, exist_ok=True)\n",
    "    string = repertoire+'/'+tokens[0]+'_'\n",
    "    f=[]\n",
    "    filenames=[]\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        name_file=string+'l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'.csv'\n",
    "        f.append(open(name_file, \"w\"))\n",
    "        filenames.append(name_file)\n",
    "        \n",
    "        \n",
    "    # sauvegarde du dataframe dans une chaîne de caracteres\n",
    "    ch = df.to_string(header=False,\n",
    "                  index=False,\n",
    "                  index_names=False).split('\\n')\n",
    "    vals = [','.join(ele.split()) for ele in ch]\n",
    "    \n",
    "    # sauvegarde dans des fichiers spécifiques par layer\n",
    "    token_layer=[]\n",
    "    token_exemples=[]\n",
    "    for nb_exemples in range (len(vals)):\n",
    "        deb=str(df[0][nb_exemples])+','\n",
    "        # 1 ligne correspond à une chaine\n",
    "        s=vals[nb_exemples]\n",
    "        listoftokens=re.findall(r'<b>,(.+?),</b>', s)\n",
    "        nb_layers=len(listoftokens)\n",
    "        \n",
    "        for nb_token in range (nb_layers):\n",
    "            save_token=''\n",
    "            save_token=deb+str(listoftokens[nb_token])+'\\n'\n",
    "            \n",
    "            f[nb_token].write(save_token)\n",
    "\n",
    "    # sauvegarde d'un fichier qui contient tous les layers en une fois\n",
    "    # récupération des données pour enlever les <b> et </b>\n",
    "    df_all=pd.DataFrame()\n",
    "    myindex=0\n",
    "    for nb_columns in range(df.shape[1]):\n",
    "        df[nb_columns]=df[nb_columns].astype(str)\n",
    "        if (df[nb_columns]!='<b>').all() and (df[nb_columns]!='</b>').all():\n",
    "            df_all[myindex]=df[nb_columns]\n",
    "            myindex+=1\n",
    "\n",
    "    # construction du nom du fichier de sauvegarde\n",
    "    string = repertoire+'/'+tokens[0]+'_'\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        string+='l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'_'\n",
    "    string+='.csv'       \n",
    "    # sauvegarde en .csv\n",
    "    df_all.to_csv(string, sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from numpy import ndarray\n",
    "from matplotlib import colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def plot_clusters_3D (X,y_predict, nb_clusters, pca_done=False):\n",
    "    if pca_done==False:\n",
    "        pca = PCA(n_components=3) \n",
    "        pca.fit(X) \n",
    "        pca_data = pd.DataFrame(pca.transform(X))\n",
    "    else: pca_data = pd.DataFrame(X)\n",
    "    colors = list(zip(*sorted(( \n",
    "                    tuple(mcolors.rgb_to_hsv( \n",
    "                          mcolors.to_rgba(color)[:3])), name) \n",
    "                     for name, color in dict( \n",
    "                            mcolors.BASE_COLORS, **mcolors.CSS4_COLORS \n",
    "                                                      ).items())))[1] \n",
    "    # number of steps to taken generate n(clusters) colors\n",
    "    skips = math.floor(len(colors[5 : -5])/nb_clusters) \n",
    "    cluster_colors = colors[5 : -5 : skips] \n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax = fig.add_subplot(111, projection = '3d') \n",
    "    ax.scatter(pca_data[0], pca_data[1], pca_data[2],\n",
    "           c = list(map(lambda label : cluster_colors[label], \n",
    "                                            y_predict))) \n",
    "\n",
    "    str_labels = list(map(lambda label:'% s' % label, y_predict)) \n",
    "\n",
    "    list(map(lambda data1, data2, data3, str_label: \n",
    "        ax.text(data1, data2, data3, s = str_label, size = 16.5, \n",
    "        zorder = 20, color = 'k'), pca_data[0], pca_data[1], \n",
    "        pca_data[2], str_labels)) \n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def plot_clusters_2D (X,y_predict, nb_clusters, pca_done=False):\n",
    "    if pca_done==False:\n",
    "        pca = PCA(n_components=2) \n",
    "        X_r = pca.fit(X).transform(X)\n",
    "    else: X_r = X\n",
    "    data = pd.DataFrame(X_r, columns=['x','y'])\n",
    "    data['label']=y_predict\n",
    "    list_clusters=list(set(y_predict))\n",
    "    print (list_clusters)\n",
    "    #create a new figure\n",
    "    plt.figure(figsize=(5,5))\n",
    "\n",
    "    #loop through labels and plot each cluster\n",
    "    for i, label in enumerate(list_clusters):\n",
    "\n",
    "        #add data points \n",
    "        plt.scatter(x=data.loc[data['label']==label, 'x'], \n",
    "                y=data.loc[data['label']==label,'y'], \n",
    "                color=color[i], \n",
    "                alpha=0.20)\n",
    "\n",
    "        #add label\n",
    "        plt.annotate(label, \n",
    "                 data.loc[data['label']==label,['x','y']].mean(),\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 size=20, weight='bold',\n",
    "                 color=color[i])\n",
    "        \n",
    "def get_directory_layers_from_csv(filename,destination):\n",
    "    tokens=filename.split(\"_\")\n",
    "    print(tokens)\n",
    "    tokens[0]=tokens[0].split(\"/\")[len(tokens[0].split(\"/\"))-1]\n",
    "    df = pd.read_csv(filename, sep = ',', header = None) \n",
    "\n",
    "    \n",
    "    # creation d'un répertoire pour sauver tous les fichiers\n",
    "    repertoire=filename[0:-4]\n",
    "    print(repertoire)\n",
    "    os.makedirs(repertoire, exist_ok=True)\n",
    "    string = destination+\"/\"+repertoire+'/'+tokens[0]+'_'\n",
    "    f=[]\n",
    "    filenames=[]\n",
    "    for nb_tokens in range (1,len(tokens)-1):\n",
    "        name_file=string+'l'+str(nb_tokens)+'_'+tokens[nb_tokens]+'.csv'\n",
    "        print(name_file)\n",
    "        f.append(open(name_file, \"w\"))\n",
    "        filenames.append(name_file)\n",
    "        \n",
    "        \n",
    "    # sauvegarde du dataframe dans une chaîne de caracteres\n",
    "    ch = df.to_string(header=False,\n",
    "                  index=False,\n",
    "                  index_names=False).split('\\n')\n",
    "    vals = [','.join(ele.split()) for ele in ch]\n",
    "    \n",
    "    # sauvegarde dans des fichiers spécifiques par layer\n",
    "    token_layer=[]\n",
    "    token_exemples=[]\n",
    "    for nb_exemples in range (len(vals)):\n",
    "        deb=str(df[0][nb_exemples])+','\n",
    "        # 1 ligne correspond à une chaine\n",
    "        s=vals[nb_exemples]\n",
    "        listoftokens=re.findall(r'<b>,(.+?),</b>', s)\n",
    "        nb_layers=len(listoftokens)\n",
    "        \n",
    "        for nb_token in range (nb_layers):\n",
    "            save_token=''\n",
    "            save_token=deb+str(listoftokens[nb_token])+'\\n'\n",
    "            \n",
    "            f[nb_token].write(save_token)\n",
    "\n",
    "    # sauvegarde d'un fichier qui contient tous les layers en une fois\n",
    "    # récupération des données pour enlever les <b> et </b>\n",
    "    df_all=pd.DataFrame()\n",
    "    myindex=0\n",
    "    for nb_columns in range(df.shape[1]):\n",
    "        df[nb_columns]=df[nb_columns].astype(str)\n",
    "        if (df[nb_columns]!='<b>').all() and (df[nb_columns]!='</b>').all():\n",
    "            df_all[myindex]=df[nb_columns]\n",
    "            myindex+=1\n",
    "\n",
    "\n",
    "    # sauvegarde en .csv\n",
    "    df_all.to_csv(string, sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iris', '8', '10', '8', '8', '10', '8', '8', '10', '8', '.csv']\n",
      "iris_8_10_8_8_10_8_8_10_8_\n",
      "./KmeansI/iris_8_10_8_8_10_8_8_10_8_/iris_l1_8.csv\n",
      "./KmeansI/iris_8_10_8_8_10_8_8_10_8_/iris_l2_10.csv\n",
      "./KmeansI/iris_8_10_8_8_10_8_8_10_8_/iris_l3_8.csv\n",
      "./KmeansI/iris_8_10_8_8_10_8_8_10_8_/iris_l4_8.csv\n",
      "./KmeansI/iris_8_10_8_8_10_8_8_10_8_/iris_l5_10.csv\n",
      "./KmeansI/iris_8_10_8_8_10_8_8_10_8_/iris_l6_8.csv\n",
      "./KmeansI/iris_8_10_8_8_10_8_8_10_8_/iris_l7_8.csv\n",
      "./KmeansI/iris_8_10_8_8_10_8_8_10_8_/iris_l8_10.csv\n",
      "./KmeansI/iris_8_10_8_8_10_8_8_10_8_/iris_l9_8.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a directory with a specific file for all the layers\n",
    "filename=\"iris_8_10_8_8_10_8_8_10_8_.csv\"\n",
    "destination=\"./KmeansI\"\n",
    "get_directory_layers_from_csv(filename,destination)\n",
    "os.system ('rm ./KmeansI/iris_8_10_8_8_10_8_8_10_8_/iris_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_cluster(clusters,numLayer,cluster_class):\n",
    "\n",
    "    idClusters=np.unique(clusters)\n",
    "    tabCount={}\n",
    "    for id in idClusters:\n",
    "        count = 0\n",
    "        for j in range(len(clusters)):\n",
    "            if clusters[j]==id:\n",
    "                tabCount[id]=count+1\n",
    "                count+=1\n",
    "    print(tabCount)\n",
    "    my_string=\"\"\n",
    "    for key,value in tabCount.items():\n",
    "        my_string += str(numLayer)+','+str(cluster_class)+','\n",
    "        my_string+=str(value)+','+str(key)+'\\n'\n",
    "    \n",
    "    return my_string\n",
    "\n",
    "def save_result_cluster(filename,clustersArray):\n",
    "    f = open(filename, \"w\")\n",
    "    \n",
    "    f.write(\"Layer,Class,Count,Cluster\\n\")\n",
    "    for i in range(len(clustersArray)):\n",
    "        f.write(str(clustersArray[i]))\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "def clusterizeLayer(url, classtoanalyze, nbClusters=4):\n",
    "    data = pd.read_csv(url, header=None)\n",
    "\n",
    "    data.columns = ['classe']+[\"neurone{}\".format(i) for i in range(len(data.columns)-1)]\n",
    "    classes = pd.Series(data['classe'])\n",
    "    \n",
    "    data = data.drop(['classe'],axis= 1)\n",
    "        \n",
    "#     with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#         print(data)\n",
    "\n",
    "    algo = KMeans(n_clusters = nbClusters)\n",
    "    #fit_predict fait tourner l'algo et retourne y où y[i] est l'indice du cluster auquel dataC12[i] appartient\n",
    "    clustersC1C2 = algo.fit_predict(data)\n",
    "#     print(clustersC1C2)\n",
    "\n",
    "    csvname = classtoanalyze\n",
    "    dataC3=pd.read_csv(csvname, header=None)\n",
    "    dataC3.columns = ['classe']+[\"neurone{}\".format(i) for i in range(len(dataC3.columns)-1)]\n",
    "    dataC3 = dataC3.drop(['classe'],axis= 1)\n",
    "\n",
    "\n",
    "#     print(\"print dataC3:\\n\")\n",
    "#     print(dataC3)\n",
    "\n",
    "    #predict utilise les clusters calculés par fit_predict(dataC1C2) et attribue chaque ligne de dataC3 au cluster dont le centre\n",
    "    #minimise la distance\n",
    "    C3ClusterPred = algo.predict(dataC3)\n",
    "\n",
    "#     print(\"Clusters C3:\\n\")\n",
    "#     print(C3ClusterPred)\n",
    "\n",
    "    clustersC1 = clustersC1C2[classes == 0]\n",
    "    clustersC2 = clustersC1C2[classes == 1]\n",
    "\n",
    "    \n",
    "    return (C3ClusterPred,clustersC1,clustersC2)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPLEMENTATION DE LA CLASSE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(url, names=names)\n",
    "data=data[data['Species'].isin(['Iris-versicolor'])].drop(['Species'],axis= 1)\n",
    "classe3 = model.predict_classes(data)\n",
    "#print(classe3)\n",
    "\n",
    "result_layersC3=get_result_layers(model,data)\n",
    "#print(result_layersC3)\n",
    "\n",
    "#On rajoute une première colonne avec la classe prédite pour être cohérent avec les autres csv\n",
    "for i,arr  in enumerate(result_layersC3):\n",
    "    arr =  np.insert(arr,0,classe3[i], axis=1)\n",
    "    result_layersC3[i] = arr\n",
    "                                               \n",
    "rep = \"./KmeansI/iristest_8_10_8_8_10_8_8_10_8_/\"\n",
    "os.makedirs(\"./KmeansI/iristest_8_10_8_8_10_8_8_10_8_\", exist_ok=True)\n",
    "for i, (filename, arr) in enumerate(zip([\"iris_l1_8.csv\", \"iris_l2_10.csv\", \"iris_l3_8.csv\",\"iris_l4_8.csv\", \"iris_l5_10.csv\", \"iris_l6_8.csv\",\"iris_l7_8.csv\", \"iris_l8_10.csv\", \"iris_l9_8.csv\"],result_layersC3)):\n",
    "    np.savetxt(rep+filename, arr, delimiter=\",\", fmt=\"%f\")\n",
    "os.system ('rm ./KmeansI/iris_8_10_8_8_10_8_8_10_8_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_TEST=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "\n",
    "#############################################\n",
    "#########       FUNCTIONS      ##############\n",
    "############################################# \n",
    "def get_clusters_forclasses(y,y_predict):\n",
    "    df_y_nbdifferentvalues=pd.DataFrame(y)\n",
    "    list_unique_value_y=df_y_nbdifferentvalues[0].unique()\n",
    "    clusters=[]\n",
    "    for i in range (len(y_predict)):\n",
    "        dict_clusters={}\n",
    "        for j in range (len(list_unique_value_y)):\n",
    "            if y[i]==list_unique_value_y[j]:\n",
    "                dict_clusters['y']=y[i]\n",
    "                dict_clusters['y_predict']=y_predict[i]\n",
    "                clusters.append(dict_clusters)\n",
    "    df_clusters = pd.DataFrame(clusters)\n",
    "    clustersperclass=[]\n",
    "    objectsperclusterperclass=[]\n",
    "    for j in range (len(list_unique_value_y)):\n",
    "        objectsperclusterperclass=[]\n",
    "        dict_clustersperclass={}\n",
    "        df_y=df_clusters.loc[df_clusters['y'] == list_unique_value_y[j]]\n",
    "        print (\"Clusters de la classe #\",\n",
    "               list_unique_value_y[j],\":\",df_y['y_predict'].unique())\n",
    "        dict_clustersperclass['class']=list_unique_value_y[j]\n",
    "        dict_clustersperclass['clusters']=df_y['y_predict'].unique()\n",
    "        dict_clustersperclass['nbobjects']=np.nan\n",
    "        df=pd.DataFrame(df_y['y_predict'].value_counts())\n",
    "        df = df.reset_index()\n",
    "        df.columns = ['cluster', 'counts']\n",
    "        for nb_objects in range(len(df)):\n",
    "            val_to_test=df['cluster'].loc[nb_objects] #number of the cluster\n",
    "            for nb_clusters in range (len(dict_clustersperclass['clusters'])):\n",
    "                if val_to_test==dict_clustersperclass['clusters'][nb_clusters]:\n",
    "                        objectsperclusterperclass.append(df['counts'].loc[nb_objects])    \n",
    "        dict_clustersperclass['nbobjects']=objectsperclusterperclass            \n",
    "        clustersperclass.append(dict_clustersperclass)\n",
    "        \n",
    "    df_clustersperclass = pd.DataFrame(clustersperclass)\n",
    "    return df_clusters,df_clustersperclass\n",
    "\n",
    "\n",
    "def build_dataframe_forJson(df_objectsclasslayers_ext):\n",
    "    '''\n",
    "    This function creates a dataframe that will be used to generate the links and nodes \n",
    "    of the Json for the Sankey.\n",
    "    \n",
    "    Input : df_objectsclasslayers_ext example\n",
    "                class  l0  l1  l2    l0r    l1r    l2r  predict\n",
    "            0      0   1   1   1  Cl1_1  Cl2_1  Cl3_1        0\n",
    "            1      0   1   1   1  Cl1_1  Cl2_1  Cl3_1        0\n",
    "            2      0   1   1   1  Cl1_1  Cl2_1  Cl3_1        0\n",
    "            3      0   1   1   1  Cl1_1  Cl2_1  Cl3_1        0\n",
    "            4      0   1   1   1  Cl1_1  Cl2_1  Cl3_1        0\n",
    "            \n",
    "    Ouput : \n",
    "        - a dataframe for the links (df_forJsonLinks)  \n",
    "        - a dataframe for the nodes (df_forJsonNodes)\n",
    "    Example of df_forJsonLinks:\n",
    "                    class source target  value\n",
    "            0       0     X0    Cl1_1     50\n",
    "            1       1     X1    Cl1_0     25\n",
    "            2       1     X1    Cl1_2     10\n",
    "            3       1     X1    Cl1_3     15\n",
    "            4       3     X3    Cl1_1     50\n",
    "            5       0  Cl1_1    Cl2_1     50\n",
    "    Each line stands for an entry with the class, source, target and value columns.\n",
    "    At line 0, 0 stands for the number of the initial, \n",
    "    X0 means the input of the neural network. Each class has the number of the corresponding layer.\n",
    "    For instance Cl1_1 means that it is a the cluster C1 at the layer 1 (l1).\n",
    "    Value stands for the number of objects  \n",
    "    \n",
    "    Example of df_forJsonNodes:\n",
    "                source class shared\n",
    "            0      X0     0  false\n",
    "            1      X1     1   true\n",
    "            2      X3     3  false\n",
    "            3   Cl1_1     0   true\n",
    "            4   Cl1_0     1   true\n",
    "    Each line stands for a node. Source is the name, class is the initial class of the node.\n",
    "    if shared is true that means that other clusters are using this cluster (cluster multiclass or\n",
    "    test class)\n",
    "        \n",
    "    '''\n",
    "    #  build the dataframe to create the Json file\n",
    "    # Get the list of the columns, this will be used to combine the layers two per two\n",
    "    # The dataframe has n columns, 2 for class and predict then (n-2)/2 is the number\n",
    "    # where the name for Json is available, i.e. X0\n",
    "    start_column=int((df_objectsclasslayers_ext.shape[1])/2)\n",
    "    list_columns=list(df_objectsclasslayers_ext)\n",
    "    del list_columns[0:start_column]\n",
    "    df_forJsonLinks=pd.DataFrame()#the dataframe that will store the result\n",
    "    # Link part\n",
    "    # the following loop will construct the dataframe from the input to the last layer\n",
    "    for i in range(len(list_columns)-1):\n",
    "        df_val=df_objectsclasslayers_ext[['class',list_columns[i],list_columns[i+1]]]\n",
    "        result=df_val.groupby(['class',list_columns[i],list_columns[i+1]]).size().reset_index()\n",
    "        result.columns = ['class','source', 'target','value']\n",
    "        df_forJsonLinks=pd.concat([df_forJsonLinks,result])    \n",
    "    df_forJsonLinks.index = range(len(df_forJsonLinks.index))\n",
    "    \n",
    "    # Node part\n",
    "    df_forJsonNodes=pd.DataFrame()\n",
    "    df_forJsonNodes=df_forJsonLinks[['source','class']].copy()\n",
    "    df_forJsonNodes['shared']='false'\n",
    "    print(\"df_forJsonNodes \" ,df_forJsonNodes)\n",
    "    print(\"df_forJsonLinks \" ,df_forJsonLinks)\n",
    "    \n",
    "    df_dup=pd.concat(g for _, g in df_forJsonNodes.groupby([df_forJsonNodes['source']]) if len(g) > 1)\n",
    "    df_dup=df_dup.reset_index()\n",
    "    print(df_dup)\n",
    "    for index, row in df_forJsonNodes.iterrows():\n",
    "        src = []\n",
    "        for index2, row2 in df_forJsonLinks.iterrows():\n",
    "            if(row2[\"target\"] == row[\"source\"]):\n",
    "                if not(row2[\"class\"] in src):\n",
    "                    src.append(row2[\"class\"])\n",
    "        if(len(src)>1) :\n",
    "            row['shared'] = \"true\"\n",
    "        \n",
    "                \n",
    "#         df_forJsonNodes['shared'].iloc[df_dup['index'][nb_transform]]='true'\n",
    "        \n",
    "    df_forJsonNodes=df_forJsonNodes.drop_duplicates(subset ='source').reset_index(drop=True)\n",
    "    # adding the number of classes from predict, i.e. the last layer\n",
    "    df_nb = df_objectsclasslayers_ext['predict'].value_counts().rename_axis('predict').reset_index(name='counts')\n",
    "    nb=df_nb['predict']\n",
    "    for nb_classes in range (len(nb)):\n",
    "        df_forJsonNodes = df_forJsonNodes.append({'source' : str(nb[nb_classes]) , \n",
    "                                      'class' : str(nb[nb_classes]),\n",
    "                                      'shared': 'false'},\n",
    "                                        ignore_index=True)\n",
    "    df_forJsonNodes=df_forJsonNodes.reset_index(drop=True)  \n",
    "    \n",
    "    return df_forJsonLinks,df_forJsonNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "########             SEARCH FILES         #####\n",
    "###############################################\n",
    "def search_files(directory):\n",
    "\n",
    "    # Extract the list of directories\n",
    "    print (\"Initialisation part: reading the files in the directory\",directory)\n",
    "    content_directory=os.listdir(directory)\n",
    "    list_dir=[]\n",
    "    for file_dir in range(len(content_directory)):\n",
    "        if os.path.isdir(directory+'/'+content_directory[file_dir]):\n",
    "            list_dir.append(directory+'/'+content_directory[file_dir]) \n",
    "\n",
    "\n",
    "    # Get the different files in order to apply the clustering\n",
    "    list_files_to_learn=[]\n",
    "    list_files_to_test=[]\n",
    "    for nb_files in range (len(list_dir)):\n",
    "        content=os.listdir(list_dir[nb_files])\n",
    "        if \"test\" not in list_dir[nb_files]:\n",
    "            for nb_files_in_directory in range (len(content)):\n",
    "                if \"DS\" not in content[nb_files_in_directory]:\n",
    "                    list_files_to_learn.append(list_dir[nb_files]+'/'+content[nb_files_in_directory])\n",
    "        else:\n",
    "            TEST_EXIST=True\n",
    "            print(\"test =\",TEST_EXIST)\n",
    "            for nb_files_in_directory in range (len(content)):\n",
    "                if \"DS\" not in content[nb_files_in_directory]:\n",
    "                    list_files_to_test.append(list_dir[nb_files]+'/'+content[nb_files_in_directory])\n",
    "\n",
    "    # sort by alphabetical order the files                \n",
    "    list_files_to_learn=sorted(list_files_to_learn)     \n",
    "    if TEST_EXIST:\n",
    "        list_files_to_test=sorted(list_files_to_test)\n",
    "    \n",
    "    return (list_files_to_learn,list_files_to_test,TEST_EXIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "##################   CLUSTERING ###############\n",
    "###############################################\n",
    "'''\n",
    "Apply the kmeans for all layer for the original data\n",
    "\n",
    "- k_means_perlayer stores the return of the k_means for each layer. It will be used\n",
    "for predicting the test\n",
    "- cluster_classes_perlayer is an array of dataframe. It has as many dataframe as layers.\n",
    "Each dataframe is obtained by the get_clusters_forclasses function\n",
    "Example:\n",
    "    [class clusters nbobjects\n",
    "0    0.0   [1, 3]  [28, 22]\n",
    "1    1.0   [2, 0]  [38, 12],    \n",
    "    class clusters nbobjects\n",
    "0    0.0   [3, 1]  [26, 24]\n",
    "1    1.0   [0, 2]  [36, 14]]\n",
    "Each dataframe has for each layer the class, the corresponding clusters as well as the number of objects\n",
    "for each cluster. For instance,  0.0   [1, 3]  [28, 22]  means that at the first layer, for the class 0, \n",
    "there are 2 clusters (1 and 3) and the number of objects of cluster 1 is 28 (resp. 22 for cluster 3).\n",
    "- objects_cluster_perlayer is an array of dataframe which has as many dataframes as layers.\n",
    "Each dataframe is obtained by the get_clusters_forclasses function. For each layer it stores for each object \n",
    "the class and the y_predict from the k-means, i.e. the number of the cluster.\n",
    "Example:\n",
    "    [      y  y_predict\n",
    "    0   0.0          1\n",
    "    1   0.0          1\n",
    "    2   0.0          1\n",
    "    ...],\n",
    "    [      y  y_predict\n",
    "    0   0.0          2\n",
    "    1   0.0          2\n",
    "0   0.0          2 means that at the second layer (2nd row of the array), for the object 0 (index), \n",
    "the class (y) is 0 and the predicted cluster is 2.\n",
    "'''\n",
    "\n",
    "def clustering(list_files_to_learn, list_files_to_test,nbC,TEST_EXIST) :\n",
    "    print (\"Compute the clustering for the original data\")\n",
    "    print(\"test =\",TEST_EXIST)\n",
    "    kmeans_per_layer=[]\n",
    "    cluster_classes_perlayer=[]\n",
    "    objects_cluster_perlayer=[]\n",
    "    for nb_layers in range (len(list_files_to_learn)):\n",
    "        print(\"Layer\",nb_layers+1)\n",
    "        df = pd.read_csv(list_files_to_learn[nb_layers], sep = ',', header = None)\n",
    "        array = df.values\n",
    "        y = array[:,0]\n",
    "        X = array[:,1:df.shape[1]] \n",
    "        kmeans=KMeans(n_clusters=nbC, random_state=30).fit(X)\n",
    "        y_predict = kmeans.predict(X)\n",
    "        #title=\"iris_l1_8\"\n",
    "        #plot_clusters_2D (\"irisl1.png\",legend,title,X,y,y_predict1, nb_clusters)\n",
    "        df_clusters,df_clustersperclass=get_clusters_forclasses(y,y_predict)\n",
    "        cluster_classes_perlayer.append(df_clustersperclass)\n",
    "        objects_cluster_perlayer.append(df_clusters)\n",
    "        kmeans_per_layer.append(kmeans)\n",
    "\n",
    "    if TEST_EXIST:\n",
    "        print (\"\\nCompute the clustering for the test class\\n\")\n",
    "        # Apply the kmeans predict for all layer for the test data\n",
    "        cluster_test_perlayer=[]\n",
    "        objectstest_cluster_perlayer=[]\n",
    "        if list_files_to_test:\n",
    "            for nb_layers in range (len(list_files_to_test)):\n",
    "                print(\"Layer\",nb_layers+1)\n",
    "                df = pd.read_csv(list_files_to_test[nb_layers], sep = ',', header = None)\n",
    "                print(df.head())\n",
    "                array = df.values\n",
    "                y = array[:,0]\n",
    "                X = array[:,1:df.shape[1]] \n",
    "                y_predict = kmeans_per_layer[nb_layers].predict(X)\n",
    "                #title=\"iris_l1_8\"\n",
    "                #plot_clusters_2D (\"irisl1.png\",legend,title,X,y,y_predict1, nb_clusters)\n",
    "                df_clusters_test,df_clustersperclass=get_clusters_forclasses(y,y_predict)\n",
    "                cluster_test_perlayer.append(df_clustersperclass)\n",
    "                objectstest_cluster_perlayer.append(df_clusters_test)\n",
    "                kmeans_per_layer.append(kmeans)\n",
    "    \n",
    "    return objects_cluster_perlayer,objectstest_cluster_perlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "########  CREATION OF JSON FILE ###############\n",
    "###############################################\n",
    "\n",
    "# Build the array of objects, class, layer1, layer2, ..., layern\n",
    "'''\n",
    "objects_cluster is an array of array (i.e. the number of objects in the dataset\n",
    " where for each object we have :\n",
    "   - its class, the cluster of layer1, the cluster of layer 2, ...\n",
    "Example :  \n",
    "    [[0. 1. 3. 1.]\n",
    "     [0. 1. 3. 1.]\n",
    "     [0. 1. 3. 1.]\n",
    "The first three objects of the dataset are in class 0 (0.), are in cluster 1 at layer 0, cluster 2 \n",
    "at layer 1, 1 at layer 2.\n",
    "Be carefull layers are numeroted from 0.      \n",
    "'''\n",
    "\n",
    "'''\n",
    "Creation of a unique dataframe for original data and test\n",
    "At the end,         \n",
    "df_object_class_layers is the dataframe that store for each occurrence \n",
    "its class and the clusters for each layer\n",
    "Example:\n",
    "   class   l0   l1   l2\n",
    "0    0.0  1.0  3.0  1.0\n",
    "1    0.0  1.0  3.0  1.0\n",
    "2    0.0  1.0  3.0  1.0\n",
    "3    0.0  1.0  3.0  1.0\n",
    "4    0.0  1.0  3.0  1.0\n",
    "'''\n",
    "\n",
    "############ CREATE AN EXTENDED DATAFRAME ##############\n",
    "'''\n",
    "Creation of a unique dataframe for original data and test. It is an extension\n",
    "of the previous one :\n",
    "    - convert in int\n",
    "    - add columns with the name of the clusters for Json (l0r, l1r, ...)\n",
    "    - add a new column (predict) for the prediction of the value\n",
    "    - the lines corresponding to the test data are replaced by the number of the class, i.e.\n",
    "    if we have original data as class 0 and 1, the test class is 2.\n",
    "df_objectsclasslayers_ext example:\n",
    "    class  l0  l1  l2    l0r    l1r    l2r  predict\n",
    "0      0   1   1   1  Cl1_1  Cl2_1  Cl3_1        0\n",
    "1      0   1   1   1  Cl1_1  Cl2_1  Cl3_1        0\n",
    "2      0   1   1   1  Cl1_1  Cl2_1  Cl3_1        0\n",
    "3      0   1   1   1  Cl1_1  Cl2_1  Cl3_1        0\n",
    "4      0   1   1   1  Cl1_1  Cl2_1  Cl3_1        0\n",
    "'''\n",
    "def creation_json(objects_cluster_perlayer,objectstest_cluster_perlayer,filename,TEST_EXIST):\n",
    "\n",
    "    objects_cluster=objects_cluster_perlayer[0]['y'].copy()\n",
    "    for nb_layers in range (len(objects_cluster_perlayer)):\n",
    "        objects_cluster=np.c_[objects_cluster,objects_cluster_perlayer[nb_layers]['y_predict']]\n",
    "    print(\"test =\",TEST_EXIST)\n",
    "    if TEST_EXIST:\n",
    "        # Build the array for test objects, class, layer1, layer2, ..., layern\n",
    "        objectstest_cluster=objectstest_cluster_perlayer[0]['y'].copy()\n",
    "        for nb_layers in range (len(objects_cluster_perlayer)):\n",
    "            objectstest_cluster=np.c_[objectstest_cluster,objectstest_cluster_perlayer[nb_layers]['y_predict']]\n",
    "\n",
    "    nb_layers= len(objects_cluster_perlayer)   \n",
    "    columname=[]\n",
    "    columname.append('class')\n",
    "    for i in range (nb_layers):\n",
    "        columname.append('l'+str(i+1))\n",
    "    df_original=pd.DataFrame(objects_cluster,columns=columname)\n",
    "    # nb_X_original will be used later to know how many test are added\n",
    "    NB_X_ORIGINAL=df_original.shape[0]\n",
    "    if TEST_EXIST:\n",
    "        df_test=pd.DataFrame(objectstest_cluster, columns=columname)\n",
    "        # Concatenate original and test\n",
    "        df_object_class_layers=pd.concat([df_original, df_test])\n",
    "        df_object_class_layers = df_object_class_layers.reset_index(drop=True)\n",
    "    else:\n",
    "        df_object_class_layers=df_original.copy()\n",
    "        df_object_class_layers = df_object_class_layers.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "    df_objectsclasslayers_ext=df_object_class_layers.copy()\n",
    "    # convert to int\n",
    "    df_objectsclasslayers_ext=df_objectsclasslayers_ext.astype(int)\n",
    "\n",
    "    df_tempo=df_objectsclasslayers_ext['class'].copy()\n",
    "    if TEST_EXIST:\n",
    "        df_objectsclasslayers_ext['class'][NB_X_ORIGINAL:]=CLASS_TEST\n",
    "    for nb_layers in range(df_objectsclasslayers_ext.shape[1]-1):\n",
    "        if nb_layers==0:\n",
    "            df_objectsclasslayers_ext['l0r']=df_objectsclasslayers_ext['class'].apply(lambda x: 'X'+str(int(x)))\n",
    "        else:\n",
    "            df_objectsclasslayers_ext['l'+str(nb_layers)+'r']=\\\n",
    "            df_objectsclasslayers_ext['l'+str(nb_layers)].apply(lambda x: 'Cl'+str(nb_layers)+'_'+str(int(x)))\n",
    "    df_objectsclasslayers_ext['predict']=df_tempo\n",
    "    df_objectsclasslayers_ext['class']=df_objectsclasslayers_ext['class'].apply(lambda x: 'C'+str(x))\n",
    "    df_objectsclasslayers_ext['predict']=df_objectsclasslayers_ext['predict'].apply(lambda x: 'C'+str(x))\n",
    "    print (df_objectsclasslayers_ext['class'].head())\n",
    "\n",
    "    # CREATE THE JSON\n",
    "\n",
    "    # Call the function to create the dataframe for the Json\n",
    "    df_forJsonLinks,df_forJsonNodes=build_dataframe_forJson(df_objectsclasslayers_ext)    \n",
    "\n",
    "    # Create the link part of the Json\n",
    "    links_json=\"\\\"links\\\":[\\n\"\n",
    "    for i in range (len(df_forJsonLinks)):\n",
    "        links_json+=\"{\"+\"\\\"source\\\"\"+':'+\"\\\"\"+str(df_forJsonLinks['source'].loc[i])+\"\\\",\"\n",
    "        links_json+=\"\\\"target\\\"\"+':'+\"\\\"\"+str(df_forJsonLinks['target'].loc[i])+\"\\\",\"\n",
    "        links_json+=\"\\\"value\\\"\"+':'+\"\\\"\"+str(df_forJsonLinks['value'].loc[i])+\"\\\",\"\n",
    "        links_json+=\"\\\"classname\\\"\"+':'+\"\\\"\"+str(df_forJsonLinks['class'].loc[i])+\"\\\"}\"\n",
    "        if i != len(df_forJsonLinks)-1:\n",
    "            links_json+=\",\\n\"\n",
    "        else:\n",
    "            links_json+=\"\\n]\"# pas de saut de ligne pour mettre la virgule avant node \n",
    "\n",
    "\n",
    "    # Create the node part of the Json\n",
    "    nodes_json=\"\\\"nodes\\\":[\\n\"\n",
    "    for i in range (len(df_forJsonNodes)):\n",
    "        nodes_json+=\"{\"+\"\\\"name\\\"\"+':'+\"\\\"\"+str(df_forJsonNodes['source'].loc[i])+\"\\\",\"\n",
    "        nodes_json+=\"\\\"classname\\\"\"+':'+\"\\\"\"+str(df_forJsonNodes['class'].loc[i])+\"\\\",\"\n",
    "        nodes_json+=\"\\\"shared\\\"\"+':'+\"\\\"\"+str(df_forJsonNodes['shared'].loc[i])+\"\\\"}\"\n",
    "        if i != len(df_forJsonNodes)-1:\n",
    "            nodes_json+=\",\\n\"\n",
    "        else:\n",
    "            nodes_json+=\"\\n]\\n\"  \n",
    "\n",
    "    final_json='{\\n'+links_json+',\\n'+nodes_json+'}'\n",
    "    print (\"The final json is \\n\",final_json)\n",
    "\n",
    "    # save the file\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(final_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init test = False\n",
      "Initialisation part: reading the files in the directory ./KmeansI\n",
      "test = True\n",
      "['./KmeansI/iris_8_10_8_8_10_8_8_10_8_/iris_l1_8.csv', './KmeansI/iris_8_10_8_8_10_8_8_10_8_/iris_l2_10.csv', './KmeansI/iris_8_10_8_8_10_8_8_10_8_/iris_l3_8.csv', './KmeansI/iris_8_10_8_8_10_8_8_10_8_/iris_l4_8.csv', './KmeansI/iris_8_10_8_8_10_8_8_10_8_/iris_l5_10.csv', './KmeansI/iris_8_10_8_8_10_8_8_10_8_/iris_l6_8.csv', './KmeansI/iris_8_10_8_8_10_8_8_10_8_/iris_l7_8.csv', './KmeansI/iris_8_10_8_8_10_8_8_10_8_/iris_l8_10.csv', './KmeansI/iris_8_10_8_8_10_8_8_10_8_/iris_l9_8.csv']\n",
      "['./KmeansI/iristest_8_10_8_8_10_8_8_10_8_/iris_l1_8.csv', './KmeansI/iristest_8_10_8_8_10_8_8_10_8_/iris_l2_10.csv', './KmeansI/iristest_8_10_8_8_10_8_8_10_8_/iris_l3_8.csv', './KmeansI/iristest_8_10_8_8_10_8_8_10_8_/iris_l4_8.csv', './KmeansI/iristest_8_10_8_8_10_8_8_10_8_/iris_l5_10.csv', './KmeansI/iristest_8_10_8_8_10_8_8_10_8_/iris_l6_8.csv', './KmeansI/iristest_8_10_8_8_10_8_8_10_8_/iris_l7_8.csv', './KmeansI/iristest_8_10_8_8_10_8_8_10_8_/iris_l8_10.csv', './KmeansI/iristest_8_10_8_8_10_8_8_10_8_/iris_l9_8.csv']\n",
      "Compute the clustering for the original data\n",
      "test = True\n",
      "Layer 1\n",
      "Clusters de la classe # 0.0 : [1]\n",
      "Clusters de la classe # 1.0 : [3 2 0]\n",
      "Layer 2\n",
      "Clusters de la classe # 0.0 : [0]\n",
      "Clusters de la classe # 1.0 : [3 1 2]\n",
      "Layer 3\n",
      "Clusters de la classe # 0.0 : [0]\n",
      "Clusters de la classe # 1.0 : [3 1 2]\n",
      "Layer 4\n",
      "Clusters de la classe # 0.0 : [1]\n",
      "Clusters de la classe # 1.0 : [2 0 3]\n",
      "Layer 5\n",
      "Clusters de la classe # 0.0 : [0 2]\n",
      "Clusters de la classe # 1.0 : [1 3]\n",
      "Layer 6\n",
      "Clusters de la classe # 0.0 : [3 1]\n",
      "Clusters de la classe # 1.0 : [0 2]\n",
      "Layer 7\n",
      "Clusters de la classe # 0.0 : [3 1]\n",
      "Clusters de la classe # 1.0 : [0 2]\n",
      "Layer 8\n",
      "Clusters de la classe # 0.0 : [1 2]\n",
      "Clusters de la classe # 1.0 : [0 3]\n",
      "Layer 9\n",
      "Clusters de la classe # 0.0 : [3 1]\n",
      "Clusters de la classe # 1.0 : [0 2]\n",
      "\n",
      "Compute the clustering for the test class\n",
      "\n",
      "Layer 1\n",
      "     0         1         2    3    4         5         6         7         8\n",
      "0  1.0  6.483278  2.091702  0.0  0.0  5.150328  6.253640  4.580649  1.166574\n",
      "1  1.0  5.874118  1.841864  0.0  0.0  5.083890  5.913978  4.455657  1.114926\n",
      "2  1.0  6.430583  2.167898  0.0  0.0  5.243387  6.346798  4.626400  1.039578\n",
      "3  1.0  5.096290  1.771087  0.0  0.0  4.170662  5.144337  3.649869  0.787495\n",
      "4  1.0  5.979171  2.023500  0.0  0.0  4.868262  6.000558  4.305085  0.987637\n",
      "Clusters de la classe # 1.0 : [2 3 1]\n",
      "Layer 2\n",
      "    0    1    2    3    4         5    6    7         8         9    10\n",
      "0  1.0  0.0  0.0  0.0  0.0  4.457288  0.0  0.0  7.528398  1.343082  0.0\n",
      "1  1.0  0.0  0.0  0.0  0.0  4.427893  0.0  0.0  6.985690  1.178359  0.0\n",
      "2  1.0  0.0  0.0  0.0  0.0  4.559066  0.0  0.0  7.506947  1.162826  0.0\n",
      "3  1.0  0.0  0.0  0.0  0.0  3.688885  0.0  0.0  5.977160  0.851744  0.0\n",
      "4  1.0  0.0  0.0  0.0  0.0  4.299864  0.0  0.0  7.004611  1.060533  0.0\n",
      "Clusters de la classe # 1.0 : [1 3 0]\n",
      "Layer 3\n",
      "     0         1    2         3    4    5         6         7         8\n",
      "0  1.0  4.254553  0.0  5.418048  0.0  0.0  1.157332  2.401945  0.591420\n",
      "1  1.0  4.032703  0.0  5.218297  0.0  0.0  1.031584  2.330812  0.436448\n",
      "2  1.0  4.167960  0.0  5.508776  0.0  0.0  1.228498  2.442530  0.489037\n",
      "3  1.0  3.309900  0.0  4.435367  0.0  0.0  1.002466  1.989514  0.355288\n",
      "4  1.0  3.897581  0.0  5.172461  0.0  0.0  1.146910  2.301945  0.436752\n",
      "Clusters de la classe # 1.0 : [3 0]\n",
      "Layer 4\n",
      "     0    1         2    3         4         5    6         7         8\n",
      "0  1.0  0.0  0.336265  0.0  3.571876  0.526268  0.0  3.548644  2.083397\n",
      "1  1.0  0.0  0.289746  0.0  3.405506  0.403236  0.0  3.359845  2.017596\n",
      "2  1.0  0.0  0.383601  0.0  3.535942  0.516804  0.0  3.498771  2.234709\n",
      "3  1.0  0.0  0.307481  0.0  2.858193  0.408831  0.0  2.819211  1.825520\n",
      "4  1.0  0.0  0.354909  0.0  3.322278  0.472428  0.0  3.282607  2.103810\n",
      "Clusters de la classe # 1.0 : [0 2 1]\n",
      "Layer 5\n",
      "    0         1    2    3         4    5         6         7         8    9   \\\n",
      "0  1.0  0.816821  0.0  0.0  0.056368  0.0  1.449845  3.961157  2.247553  0.0   \n",
      "1  1.0  0.794730  0.0  0.0  0.042223  0.0  1.313269  3.795471  2.081918  0.0   \n",
      "2  1.0  0.675811  0.0  0.0  0.092758  0.0  1.318722  3.975600  2.268603  0.0   \n",
      "3  1.0  0.553679  0.0  0.0  0.076661  0.0  1.053449  3.217710  1.844263  0.0   \n",
      "4  1.0  0.642560  0.0  0.0  0.085362  0.0  1.231176  3.737093  2.126745  0.0   \n",
      "\n",
      "         10  \n",
      "0  0.425249  \n",
      "1  0.390342  \n",
      "2  0.451592  \n",
      "3  0.362616  \n",
      "4  0.421111  \n",
      "Clusters de la classe # 1.0 : [1 0]\n",
      "Layer 6\n",
      "     0    1         2    3         4         5    6    7         8\n",
      "0  1.0  0.0  4.122232  0.0  0.549454  0.590602  0.0  0.0  1.317493\n",
      "1  1.0  0.0  3.916948  0.0  0.475917  0.512099  0.0  0.0  1.285135\n",
      "2  1.0  0.0  4.112300  0.0  0.584094  0.442719  0.0  0.0  1.426177\n",
      "3  1.0  0.0  3.358480  0.0  0.465950  0.378077  0.0  0.0  1.160058\n",
      "4  1.0  0.0  3.871480  0.0  0.540687  0.420274  0.0  0.0  1.341841\n",
      "Clusters de la classe # 1.0 : [0 3]\n",
      "Layer 7\n",
      "     0    1         2         3         4         5         6         7  \\\n",
      "0  1.0  0.0  3.274692  2.749473  2.260255  2.346332  2.266077  0.688601   \n",
      "1  1.0  0.0  3.148152  2.663394  2.174034  2.228770  2.166513  0.618914   \n",
      "2  1.0  0.0  3.293009  2.873577  2.260952  2.233639  2.286744  0.491638   \n",
      "3  1.0  0.0  2.719571  2.350495  1.866869  1.839475  1.863651  0.432370   \n",
      "4  1.0  0.0  3.112179  2.708297  2.137149  2.110122  2.152419  0.473313   \n",
      "\n",
      "          8  \n",
      "0  1.055696  \n",
      "1  0.938028  \n",
      "2  0.909306  \n",
      "3  0.750287  \n",
      "4  0.855162  \n",
      "Clusters de la classe # 1.0 : [0 3]\n",
      "Layer 8\n",
      "    0    1         2         3    4    5    6         7         8    9    10\n",
      "0  1.0  0.0  0.674227  2.845388  0.0  0.0  0.0  5.621537  3.607565  0.0  0.0\n",
      "1  1.0  0.0  0.613623  2.667566  0.0  0.0  0.0  5.328087  3.532073  0.0  0.0\n",
      "2  1.0  0.0  0.478838  2.646453  0.0  0.0  0.0  5.379019  3.897586  0.0  0.0\n",
      "3  1.0  0.0  0.418185  2.195876  0.0  0.0  0.0  4.469749  3.199133  0.0  0.0\n",
      "4  1.0  0.0  0.461043  2.502843  0.0  0.0  0.0  5.091106  3.677106  0.0  0.0\n",
      "Clusters de la classe # 1.0 : [0]\n",
      "Layer 9\n",
      "     0    1    2         3         4    5         6    7         8\n",
      "0  1.0  0.0  0.0  0.889938  3.421373  0.0  3.482431  0.0  1.752011\n",
      "1  1.0  0.0  0.0  0.749927  3.345770  0.0  3.380198  0.0  1.630254\n",
      "2  1.0  0.0  0.0  0.477793  3.696999  0.0  3.643813  0.0  1.548582\n",
      "3  1.0  0.0  0.0  0.434535  3.044578  0.0  3.011556  0.0  1.287604\n",
      "4  1.0  0.0  0.0  0.463529  3.490345  0.0  3.443862  0.0  1.466252\n",
      "Clusters de la classe # 1.0 : [0]\n",
      "[      y  y_predict\n",
      "0   0.0          1\n",
      "1   0.0          1\n",
      "2   0.0          1\n",
      "3   0.0          1\n",
      "4   0.0          1\n",
      "..  ...        ...\n",
      "77  1.0          0\n",
      "78  1.0          0\n",
      "79  1.0          0\n",
      "80  1.0          0\n",
      "81  1.0          0\n",
      "\n",
      "[82 rows x 2 columns],       y  y_predict\n",
      "0   0.0          0\n",
      "1   0.0          0\n",
      "2   0.0          0\n",
      "3   0.0          0\n",
      "4   0.0          0\n",
      "..  ...        ...\n",
      "77  1.0          2\n",
      "78  1.0          2\n",
      "79  1.0          2\n",
      "80  1.0          2\n",
      "81  1.0          2\n",
      "\n",
      "[82 rows x 2 columns],       y  y_predict\n",
      "0   0.0          0\n",
      "1   0.0          0\n",
      "2   0.0          0\n",
      "3   0.0          0\n",
      "4   0.0          0\n",
      "..  ...        ...\n",
      "77  1.0          2\n",
      "78  1.0          2\n",
      "79  1.0          2\n",
      "80  1.0          2\n",
      "81  1.0          2\n",
      "\n",
      "[82 rows x 2 columns],       y  y_predict\n",
      "0   0.0          1\n",
      "1   0.0          1\n",
      "2   0.0          1\n",
      "3   0.0          1\n",
      "4   0.0          1\n",
      "..  ...        ...\n",
      "77  1.0          3\n",
      "78  1.0          3\n",
      "79  1.0          3\n",
      "80  1.0          3\n",
      "81  1.0          3\n",
      "\n",
      "[82 rows x 2 columns],       y  y_predict\n",
      "0   0.0          0\n",
      "1   0.0          0\n",
      "2   0.0          0\n",
      "3   0.0          0\n",
      "4   0.0          0\n",
      "..  ...        ...\n",
      "77  1.0          3\n",
      "78  1.0          3\n",
      "79  1.0          3\n",
      "80  1.0          3\n",
      "81  1.0          3\n",
      "\n",
      "[82 rows x 2 columns],       y  y_predict\n",
      "0   0.0          3\n",
      "1   0.0          3\n",
      "2   0.0          3\n",
      "3   0.0          3\n",
      "4   0.0          3\n",
      "..  ...        ...\n",
      "77  1.0          2\n",
      "78  1.0          2\n",
      "79  1.0          2\n",
      "80  1.0          2\n",
      "81  1.0          2\n",
      "\n",
      "[82 rows x 2 columns],       y  y_predict\n",
      "0   0.0          3\n",
      "1   0.0          3\n",
      "2   0.0          3\n",
      "3   0.0          3\n",
      "4   0.0          3\n",
      "..  ...        ...\n",
      "77  1.0          2\n",
      "78  1.0          2\n",
      "79  1.0          2\n",
      "80  1.0          2\n",
      "81  1.0          2\n",
      "\n",
      "[82 rows x 2 columns],       y  y_predict\n",
      "0   0.0          1\n",
      "1   0.0          1\n",
      "2   0.0          1\n",
      "3   0.0          1\n",
      "4   0.0          1\n",
      "..  ...        ...\n",
      "77  1.0          3\n",
      "78  1.0          3\n",
      "79  1.0          3\n",
      "80  1.0          3\n",
      "81  1.0          3\n",
      "\n",
      "[82 rows x 2 columns],       y  y_predict\n",
      "0   0.0          3\n",
      "1   0.0          3\n",
      "2   0.0          3\n",
      "3   0.0          3\n",
      "4   0.0          3\n",
      "..  ...        ...\n",
      "77  1.0          2\n",
      "78  1.0          2\n",
      "79  1.0          2\n",
      "80  1.0          2\n",
      "81  1.0          2\n",
      "\n",
      "[82 rows x 2 columns]]\n",
      "test = True\n",
      "0    C0\n",
      "1    C0\n",
      "2    C0\n",
      "3    C0\n",
      "4    C0\n",
      "Name: class, dtype: object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_forJsonNodes     source class shared\n",
      "0      X0    C0  false\n",
      "1      X1    C1  false\n",
      "2      X1    C1  false\n",
      "3      X1    C1  false\n",
      "4     X11   C11  false\n",
      "..    ...   ...    ...\n",
      "62  Cl8_1    C0  false\n",
      "63  Cl8_2    C0  false\n",
      "64  Cl8_0    C1  false\n",
      "65  Cl8_3    C1  false\n",
      "66  Cl8_0   C11  false\n",
      "\n",
      "[67 rows x 3 columns]\n",
      "df_forJsonLinks     class source target  value\n",
      "0     C0     X0  Cl1_1     36\n",
      "1     C1     X1  Cl1_0     11\n",
      "2     C1     X1  Cl1_2     23\n",
      "3     C1     X1  Cl1_3     12\n",
      "4    C11    X11  Cl1_1      4\n",
      "..   ...    ...    ...    ...\n",
      "62    C0  Cl8_1     C0     20\n",
      "63    C0  Cl8_2     C0     16\n",
      "64    C1  Cl8_0     C1     25\n",
      "65    C1  Cl8_3     C1     21\n",
      "66   C11  Cl8_0     C1     50\n",
      "\n",
      "[67 rows x 4 columns]\n",
      "    index source class shared\n",
      "0       7  Cl1_1    C0  false\n",
      "1      11  Cl1_1   C11  false\n",
      "2       9  Cl1_2    C1  false\n",
      "3      12  Cl1_2   C11  false\n",
      "4      10  Cl1_3    C1  false\n",
      "5      13  Cl1_3   C11  false\n",
      "6      14  Cl2_0    C0  false\n",
      "7      20  Cl2_0   C11  false\n",
      "8      15  Cl2_1    C1  false\n",
      "9      16  Cl2_1    C1  false\n",
      "10     21  Cl2_1   C11  false\n",
      "11     17  Cl2_2    C1  false\n",
      "12     18  Cl2_2    C1  false\n",
      "13     19  Cl2_3    C1  false\n",
      "14     22  Cl2_3   C11  false\n",
      "15     23  Cl3_0    C0  false\n",
      "16     27  Cl3_0   C11  false\n",
      "17     28  Cl3_0   C11  false\n",
      "18     26  Cl3_3    C1  false\n",
      "19     29  Cl3_3   C11  false\n",
      "20     30  Cl3_3   C11  false\n",
      "21     33  Cl4_0    C1  false\n",
      "22     34  Cl4_0    C1  false\n",
      "23     37  Cl4_0   C11  false\n",
      "24     31  Cl4_1    C0  false\n",
      "25     32  Cl4_1    C0  false\n",
      "26     38  Cl4_1   C11  false\n",
      "27     35  Cl4_2    C1  false\n",
      "28     39  Cl4_2   C11  false\n",
      "29     40  Cl5_0    C0  false\n",
      "30     45  Cl5_0   C11  false\n",
      "31     42  Cl5_1    C1  false\n",
      "32     43  Cl5_1    C1  false\n",
      "33     46  Cl5_1   C11  false\n",
      "34     49  Cl6_0    C1  false\n",
      "35     52  Cl6_0   C11  false\n",
      "36     50  Cl6_2    C1  false\n",
      "37     51  Cl6_2    C1  false\n",
      "38     48  Cl6_3    C0  false\n",
      "39     53  Cl6_3   C11  false\n",
      "40     57  Cl7_0    C1  false\n",
      "41     58  Cl7_0    C1  false\n",
      "42     60  Cl7_0   C11  false\n",
      "43     54  Cl7_1    C0  false\n",
      "44     55  Cl7_1    C0  false\n",
      "45     56  Cl7_3    C0  false\n",
      "46     61  Cl7_3   C11  false\n",
      "47     64  Cl8_0    C1  false\n",
      "48     66  Cl8_0   C11  false\n",
      "49      1     X1    C1  false\n",
      "50      2     X1    C1  false\n",
      "51      3     X1    C1  false\n",
      "52      4    X11   C11  false\n",
      "53      5    X11   C11  false\n",
      "54      6    X11   C11  false\n",
      "The final json is \n",
      " {\n",
      "\"links\":[\n",
      "{\"source\":\"X0\",\"target\":\"Cl1_1\",\"value\":\"36\",\"classname\":\"C0\"},\n",
      "{\"source\":\"X1\",\"target\":\"Cl1_0\",\"value\":\"11\",\"classname\":\"C1\"},\n",
      "{\"source\":\"X1\",\"target\":\"Cl1_2\",\"value\":\"23\",\"classname\":\"C1\"},\n",
      "{\"source\":\"X1\",\"target\":\"Cl1_3\",\"value\":\"12\",\"classname\":\"C1\"},\n",
      "{\"source\":\"X11\",\"target\":\"Cl1_1\",\"value\":\"4\",\"classname\":\"C11\"},\n",
      "{\"source\":\"X11\",\"target\":\"Cl1_2\",\"value\":\"3\",\"classname\":\"C11\"},\n",
      "{\"source\":\"X11\",\"target\":\"Cl1_3\",\"value\":\"43\",\"classname\":\"C11\"},\n",
      "{\"source\":\"Cl1_1\",\"target\":\"Cl2_0\",\"value\":\"36\",\"classname\":\"C0\"},\n",
      "{\"source\":\"Cl1_0\",\"target\":\"Cl2_2\",\"value\":\"11\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl1_2\",\"target\":\"Cl2_1\",\"value\":\"23\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl1_3\",\"target\":\"Cl2_3\",\"value\":\"12\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl1_1\",\"target\":\"Cl2_0\",\"value\":\"4\",\"classname\":\"C11\"},\n",
      "{\"source\":\"Cl1_2\",\"target\":\"Cl2_1\",\"value\":\"3\",\"classname\":\"C11\"},\n",
      "{\"source\":\"Cl1_3\",\"target\":\"Cl2_3\",\"value\":\"43\",\"classname\":\"C11\"},\n",
      "{\"source\":\"Cl2_0\",\"target\":\"Cl3_0\",\"value\":\"36\",\"classname\":\"C0\"},\n",
      "{\"source\":\"Cl2_1\",\"target\":\"Cl3_1\",\"value\":\"22\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl2_1\",\"target\":\"Cl3_3\",\"value\":\"1\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl2_2\",\"target\":\"Cl3_1\",\"value\":\"2\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl2_2\",\"target\":\"Cl3_2\",\"value\":\"9\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl2_3\",\"target\":\"Cl3_3\",\"value\":\"12\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl2_0\",\"target\":\"Cl3_0\",\"value\":\"4\",\"classname\":\"C11\"},\n",
      "{\"source\":\"Cl2_1\",\"target\":\"Cl3_3\",\"value\":\"3\",\"classname\":\"C11\"},\n",
      "{\"source\":\"Cl2_3\",\"target\":\"Cl3_3\",\"value\":\"43\",\"classname\":\"C11\"},\n",
      "{\"source\":\"Cl3_0\",\"target\":\"Cl4_1\",\"value\":\"36\",\"classname\":\"C0\"},\n",
      "{\"source\":\"Cl3_1\",\"target\":\"Cl4_0\",\"value\":\"24\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl3_2\",\"target\":\"Cl4_3\",\"value\":\"9\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl3_3\",\"target\":\"Cl4_2\",\"value\":\"13\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl3_0\",\"target\":\"Cl4_1\",\"value\":\"1\",\"classname\":\"C11\"},\n",
      "{\"source\":\"Cl3_0\",\"target\":\"Cl4_2\",\"value\":\"3\",\"classname\":\"C11\"},\n",
      "{\"source\":\"Cl3_3\",\"target\":\"Cl4_0\",\"value\":\"3\",\"classname\":\"C11\"},\n",
      "{\"source\":\"Cl3_3\",\"target\":\"Cl4_2\",\"value\":\"43\",\"classname\":\"C11\"},\n",
      "{\"source\":\"Cl4_1\",\"target\":\"Cl5_0\",\"value\":\"19\",\"classname\":\"C0\"},\n",
      "{\"source\":\"Cl4_1\",\"target\":\"Cl5_2\",\"value\":\"17\",\"classname\":\"C0\"},\n",
      "{\"source\":\"Cl4_0\",\"target\":\"Cl5_1\",\"value\":\"17\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl4_0\",\"target\":\"Cl5_3\",\"value\":\"7\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl4_2\",\"target\":\"Cl5_1\",\"value\":\"13\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl4_3\",\"target\":\"Cl5_3\",\"value\":\"9\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl4_0\",\"target\":\"Cl5_1\",\"value\":\"3\",\"classname\":\"C11\"},\n",
      "{\"source\":\"Cl4_1\",\"target\":\"Cl5_0\",\"value\":\"1\",\"classname\":\"C11\"},\n",
      "{\"source\":\"Cl4_2\",\"target\":\"Cl5_1\",\"value\":\"46\",\"classname\":\"C11\"},\n",
      "{\"source\":\"Cl5_0\",\"target\":\"Cl6_3\",\"value\":\"19\",\"classname\":\"C0\"},\n",
      "{\"source\":\"Cl5_2\",\"target\":\"Cl6_1\",\"value\":\"17\",\"classname\":\"C0\"},\n",
      "{\"source\":\"Cl5_1\",\"target\":\"Cl6_0\",\"value\":\"26\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl5_1\",\"target\":\"Cl6_2\",\"value\":\"4\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl5_3\",\"target\":\"Cl6_2\",\"value\":\"16\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl5_0\",\"target\":\"Cl6_3\",\"value\":\"1\",\"classname\":\"C11\"},\n",
      "{\"source\":\"Cl5_1\",\"target\":\"Cl6_0\",\"value\":\"49\",\"classname\":\"C11\"},\n",
      "{\"source\":\"Cl6_1\",\"target\":\"Cl7_1\",\"value\":\"17\",\"classname\":\"C0\"},\n",
      "{\"source\":\"Cl6_3\",\"target\":\"Cl7_3\",\"value\":\"19\",\"classname\":\"C0\"},\n",
      "{\"source\":\"Cl6_0\",\"target\":\"Cl7_0\",\"value\":\"26\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl6_2\",\"target\":\"Cl7_0\",\"value\":\"1\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl6_2\",\"target\":\"Cl7_2\",\"value\":\"19\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl6_0\",\"target\":\"Cl7_0\",\"value\":\"49\",\"classname\":\"C11\"},\n",
      "{\"source\":\"Cl6_3\",\"target\":\"Cl7_3\",\"value\":\"1\",\"classname\":\"C11\"},\n",
      "{\"source\":\"Cl7_1\",\"target\":\"Cl8_1\",\"value\":\"1\",\"classname\":\"C0\"},\n",
      "{\"source\":\"Cl7_1\",\"target\":\"Cl8_2\",\"value\":\"16\",\"classname\":\"C0\"},\n",
      "{\"source\":\"Cl7_3\",\"target\":\"Cl8_1\",\"value\":\"19\",\"classname\":\"C0\"},\n",
      "{\"source\":\"Cl7_0\",\"target\":\"Cl8_0\",\"value\":\"25\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl7_0\",\"target\":\"Cl8_3\",\"value\":\"2\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl7_2\",\"target\":\"Cl8_3\",\"value\":\"19\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl7_0\",\"target\":\"Cl8_0\",\"value\":\"49\",\"classname\":\"C11\"},\n",
      "{\"source\":\"Cl7_3\",\"target\":\"Cl8_0\",\"value\":\"1\",\"classname\":\"C11\"},\n",
      "{\"source\":\"Cl8_1\",\"target\":\"C0\",\"value\":\"20\",\"classname\":\"C0\"},\n",
      "{\"source\":\"Cl8_2\",\"target\":\"C0\",\"value\":\"16\",\"classname\":\"C0\"},\n",
      "{\"source\":\"Cl8_0\",\"target\":\"C1\",\"value\":\"25\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl8_3\",\"target\":\"C1\",\"value\":\"21\",\"classname\":\"C1\"},\n",
      "{\"source\":\"Cl8_0\",\"target\":\"C1\",\"value\":\"50\",\"classname\":\"C11\"}\n",
      "],\n",
      "\"nodes\":[\n",
      "{\"name\":\"X0\",\"classname\":\"C0\",\"shared\":\"false\"},\n",
      "{\"name\":\"X1\",\"classname\":\"C1\",\"shared\":\"false\"},\n",
      "{\"name\":\"X11\",\"classname\":\"C11\",\"shared\":\"false\"},\n",
      "{\"name\":\"Cl1_1\",\"classname\":\"C0\",\"shared\":\"true\"},\n",
      "{\"name\":\"Cl1_0\",\"classname\":\"C1\",\"shared\":\"false\"},\n",
      "{\"name\":\"Cl1_2\",\"classname\":\"C1\",\"shared\":\"true\"},\n",
      "{\"name\":\"Cl1_3\",\"classname\":\"C1\",\"shared\":\"true\"},\n",
      "{\"name\":\"Cl2_0\",\"classname\":\"C0\",\"shared\":\"true\"},\n",
      "{\"name\":\"Cl2_1\",\"classname\":\"C1\",\"shared\":\"true\"},\n",
      "{\"name\":\"Cl2_2\",\"classname\":\"C1\",\"shared\":\"false\"},\n",
      "{\"name\":\"Cl2_3\",\"classname\":\"C1\",\"shared\":\"true\"},\n",
      "{\"name\":\"Cl3_0\",\"classname\":\"C0\",\"shared\":\"true\"},\n",
      "{\"name\":\"Cl3_1\",\"classname\":\"C1\",\"shared\":\"false\"},\n",
      "{\"name\":\"Cl3_2\",\"classname\":\"C1\",\"shared\":\"false\"},\n",
      "{\"name\":\"Cl3_3\",\"classname\":\"C1\",\"shared\":\"true\"},\n",
      "{\"name\":\"Cl4_1\",\"classname\":\"C0\",\"shared\":\"true\"},\n",
      "{\"name\":\"Cl4_0\",\"classname\":\"C1\",\"shared\":\"true\"},\n",
      "{\"name\":\"Cl4_2\",\"classname\":\"C1\",\"shared\":\"true\"},\n",
      "{\"name\":\"Cl4_3\",\"classname\":\"C1\",\"shared\":\"false\"},\n",
      "{\"name\":\"Cl5_0\",\"classname\":\"C0\",\"shared\":\"true\"},\n",
      "{\"name\":\"Cl5_2\",\"classname\":\"C0\",\"shared\":\"false\"},\n",
      "{\"name\":\"Cl5_1\",\"classname\":\"C1\",\"shared\":\"true\"},\n",
      "{\"name\":\"Cl5_3\",\"classname\":\"C1\",\"shared\":\"false\"},\n",
      "{\"name\":\"Cl6_1\",\"classname\":\"C0\",\"shared\":\"false\"},\n",
      "{\"name\":\"Cl6_3\",\"classname\":\"C0\",\"shared\":\"true\"},\n",
      "{\"name\":\"Cl6_0\",\"classname\":\"C1\",\"shared\":\"true\"},\n",
      "{\"name\":\"Cl6_2\",\"classname\":\"C1\",\"shared\":\"false\"},\n",
      "{\"name\":\"Cl7_1\",\"classname\":\"C0\",\"shared\":\"false\"},\n",
      "{\"name\":\"Cl7_3\",\"classname\":\"C0\",\"shared\":\"true\"},\n",
      "{\"name\":\"Cl7_0\",\"classname\":\"C1\",\"shared\":\"true\"},\n",
      "{\"name\":\"Cl7_2\",\"classname\":\"C1\",\"shared\":\"false\"},\n",
      "{\"name\":\"Cl8_1\",\"classname\":\"C0\",\"shared\":\"false\"},\n",
      "{\"name\":\"Cl8_2\",\"classname\":\"C0\",\"shared\":\"false\"},\n",
      "{\"name\":\"Cl8_0\",\"classname\":\"C1\",\"shared\":\"true\"},\n",
      "{\"name\":\"Cl8_3\",\"classname\":\"C1\",\"shared\":\"false\"},\n",
      "{\"name\":\"C1\",\"classname\":\"C1\",\"shared\":\"false\"},\n",
      "{\"name\":\"C0\",\"classname\":\"C0\",\"shared\":\"false\"}\n",
      "]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "TEST_EXIST=False\n",
    "print(\"Init test =\",TEST_EXIST)\n",
    "list_files_to_learn,list_files_to_test,TEST_EXIST = search_files(\"./KmeansI\")\n",
    "print(list_files_to_learn)\n",
    "print(list_files_to_test)\n",
    "objects_cluster_perlayer,objectstest_cluster_perlayer= clustering(list_files_to_learn, list_files_to_test,4,TEST_EXIST)\n",
    "print(objects_cluster_perlayer)\n",
    "creation_json(objects_cluster_perlayer,objectstest_cluster_perlayer,'./affichageWeb/iris10layers.json',TEST_EXIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ./KmeansI/iris_8_10_8_/iris_l1_8.csv does not exist: './KmeansI/iris_8_10_8_/iris_l1_8.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-06633acb04bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclustersC3l1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclustersC1l1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclustersC2l1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclusterizeLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'iris_8_10_8_/iris_l1_8.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/iristest_8_10_8_/iris_l1_8.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclustersC3l2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclustersC1l2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclustersC2l2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclusterizeLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/iris_8_10_8_/iris_l2_10.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/iristest_8_10_8_/iris_l2_10.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclustersC3l3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclustersC1l3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclustersC2l3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclusterizeLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/iris_8_10_8_/iris_l3_8.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/iristest_8_10_8_/iris_l3_8.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mC1l1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_result_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclustersC1l1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-5c359aea518a>\u001b[0m in \u001b[0;36mclusterizeLayer\u001b[0;34m(url, classtoanalyze, nbClusters)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclusterizeLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasstoanalyze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbClusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'classe'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"neurone{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cawosh/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cawosh/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cawosh/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cawosh/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cawosh/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ./KmeansI/iris_8_10_8_/iris_l1_8.csv does not exist: './KmeansI/iris_8_10_8_/iris_l1_8.csv'"
     ]
    }
   ],
   "source": [
    "clustersC3l1,clustersC1l1,clustersC2l1=clusterizeLayer(destination+'/'+'iris_8_10_8_/iris_l1_8.csv',destination+'/iristest_8_10_8_/iris_l1_8.csv')\n",
    "clustersC3l2,clustersC1l2,clustersC2l2=clusterizeLayer(destination+'/iris_8_10_8_/iris_l2_10.csv',destination+'/iristest_8_10_8_/iris_l2_10.csv')\n",
    "clustersC3l3,clustersC1l3,clustersC2l3=clusterizeLayer(destination+'/iris_8_10_8_/iris_l3_8.csv',destination+'/iristest_8_10_8_/iris_l3_8.csv')\n",
    "\n",
    "C1l1=get_result_cluster(clustersC1l1,1,1)\n",
    "C1l2=get_result_cluster(clustersC1l2,2,1)\n",
    "C1l3=get_result_cluster(clustersC1l3,3,1)\n",
    "\n",
    "C2l1=get_result_cluster(clustersC2l1,1,2)\n",
    "C2l2=get_result_cluster(clustersC2l2,2,2)\n",
    "C2l3=get_result_cluster(clustersC2l3,3,2)\n",
    "\n",
    "C3l1=get_result_cluster(clustersC3l1,1,3)\n",
    "C3l2=get_result_cluster(clustersC3l2,2,3)\n",
    "C3l3=get_result_cluster(clustersC3l3,3,3)\n",
    "\n",
    "save_result_cluster(destination+\"/iris_8_10_8_/iris_clusters_layer1.csv\",[C1l1,C2l1,C3l1])\n",
    "save_result_cluster(destination+\"/iris_8_10_8_/iris_clusters_layer2.csv\",[C1l2,C2l2,C3l2])\n",
    "save_result_cluster(destination+\"/iris_8_10_8_/iris_clusters_layer3.csv\",[C1l3,C2l3,C3l3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphCluster(layer,title):\n",
    "    names = ['cluster0c1', 'cluster0c2', 'cluster0c3' ,\n",
    "             'cluster1c1', 'cluster1c2', 'cluster1c3' , \n",
    "             'cluster2c1', 'cluster2c2', 'cluster2c3',\n",
    "             'cluster3c1', 'cluster3c2', 'cluster3c3']\n",
    "\n",
    "    cluster0 = [0,0,0]\n",
    "    cluster1 = [0,0,0]\n",
    "    cluster2 = [0,0,0]\n",
    "    cluster3 = [0,0,0]\n",
    "\n",
    "    for index,element in layer.iterrows():\n",
    "        if element[\"Cluster\"]==0:\n",
    "            cluster0[element[\"Class\"] - 1]=element[\"Count\"]\n",
    "        if element[\"Cluster\"]==1:\n",
    "            cluster1[element[\"Class\"]-1]=element[\"Count\"]\n",
    "        if element[\"Cluster\"]==2:\n",
    "            cluster2[element[\"Class\"]-1]=element[\"Count\"]\n",
    "        if element[\"Cluster\"]==3:\n",
    "            cluster3[element[\"Class\"]-1]=element[\"Count\"]\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    values = cluster0 + cluster1 + cluster2 +cluster3 \n",
    "#     print(cluster0)\n",
    "#     print(cluster1)\n",
    "#     print(cluster2)\n",
    "#     print(cluster3)\n",
    "\n",
    "    colors = {'Classe 1':'#f91212', 'Classe 2':'#FFAA00', 'Classe 3':'#0055FF'}\n",
    "    labels = list(colors.keys())\n",
    "    handles = [plt.bar(names,values,color=colors[label],width=1) for label in labels]\n",
    "    plt.legend(handles, labels)\n",
    "    plt.bar(names,values,color = list(colors.values()),width=1)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layer1 = pd.read_csv(destination+\"/iris_8_10_8_/iris_clusters_layer1.csv\")\n",
    "print(\"layer1:\\n\",layer1)\n",
    "graphCluster(layer1,'Clusters du layer 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2 = pd.read_csv(destination+\"/iris_8_10_8_/iris_clusters_layer2.csv\")\n",
    "print(\"layer1:\\n\",layer2)\n",
    "graphCluster(layer2,'Clusters du layer 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layer3 = pd.read_csv(destination+\"/iris_8_10_8_/iris_clusters_layer3.csv\")\n",
    "print(\"layer1:\\n\",layer3)\n",
    "graphCluster(layer3,'Clusters du layer 3')\n",
    "os.system ('rm ./KmeansI/iris_8_10_8_/iris_clusters*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x_test', 'x_train', 'y_train', 'y_test']\n"
     ]
    }
   ],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train) = (X_class0_1,y_class0_1)\n",
    "(X_test,y_test) = (X_class2,y_class2)\n",
    "\n",
    "X_train_sample=X_train[0:100]\n",
    "y_train_sample=y_train[0:100]\n",
    "print(len(X_train_sample))\n",
    "print(len(y_train_sample))\n",
    "X_train=X_train_sample\n",
    "y_train=y_train_sample\n",
    "X_train = np.reshape(X_train, (784, 100)).T\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "X_test_sample=X_test[0:100]\n",
    "y_test_sample=y_test[0:100]\n",
    "\n",
    "print(len(X_test_sample))\n",
    "print(len(y_test_sample))\n",
    "X_test=X_test_sample\n",
    "y_test=11\n",
    "X_test = np.reshape(X_test, (784, 100)).T\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(y_train)\n",
    "X_01=[]\n",
    "y_01=[]\n",
    "nb_X=0\n",
    "# for i in range(X_train.shape[0]):\n",
    "#     if (y_train[i]==0 or y_train[i]==1):\n",
    "        \n",
    "#         nb_X+=1\n",
    "#         X_01.append(X_train[i])\n",
    "#         y_01.append(y_train[i])\n",
    "\n",
    "       \n",
    "\n",
    "# train_X=np.asarray(X_01)\n",
    "\n",
    "# train_y=y_01\n",
    "nbClusters = 10\n",
    "encoder = LabelEncoder()\n",
    "y_train=encoder.fit_transform(y_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(512, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(512, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(512, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(512, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 40, batch_size = 32)\n",
    "\n",
    "X_good = X_train\n",
    "y_good= y_train\n",
    "\n",
    "# Récupération des valeurs de tous les layers sauf le dernier\n",
    "result_layers=get_result_layers(model,X_good)\n",
    "print(len(y_class0_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du fichier\n",
    "# structure :\n",
    "# 0/1 = valeur de la classe\n",
    "# chaque valeur de layer est entourée par une étoile *\n",
    "save_result_layers(\"mnist_512_512_512_512_512_tmp\",X_good,y_good,result_layers)\n",
    "# tri du fichier\n",
    "os.system ('sort mnist_512_512_512_512_512_tmp > mnist_512_512_512_512_512_.csv')\n",
    "# effacer le fichier intermédiaire\n",
    "os.system ('rm -f mnist_512_512_512_512_512_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename=\"mnist_512_512_512_512_512_.csv\"    \n",
    "destination=\"./KmeansM\"\n",
    "get_directory_layers_from_csv(filename,destination)\n",
    "os.system ('rm -f ./KmeansM/mnist_512_512_512_512_512_/mnist_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(X_test)\n",
    "values_y=[]\n",
    "for i in range (len(predictions)):\n",
    "    values_y.append(predictions[i][0])\n",
    "print(values_y)\n",
    "result_layers=get_result_layers(model,X_test)\n",
    "save_result_layers(\"mnisttest_512_512_512_512_512_tmp\",X_test,values_y,result_layers)\n",
    "\n",
    "os.system ('sort mnisttest_512_512_512_512_512_tmp > mnisttest_512_512_512_512_512_.csv')\n",
    "# effacer le fichier intermédiaire\n",
    "os.system ('rm -f mnisttest_512_512_512_512_512_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"mnisttest_512_512_512_512_512_.csv\"    \n",
    "destination=\"./KmeansM\"\n",
    "get_directory_layers_from_csv(filename,destination)\n",
    "os.system ('rm -f ./KmeansM/mnisttest_512_512_512_512_512_/mnisttest_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TEST_EXIST=False\n",
    "print(\"Init test =\",TEST_EXIST)\n",
    "list_files_to_learn,list_files_to_test,TEST_EXIST = search_files(\"./KmeansM\")\n",
    "print(list_files_to_learn)\n",
    "print(list_files_to_test)\n",
    "objects_cluster_perlayer,objectstest_cluster_perlayer= clustering(list_files_to_learn, list_files_to_test,4,TEST_EXIST)\n",
    "print(objects_cluster_perlayer)\n",
    "creation_json(objects_cluster_perlayer,objectstest_cluster_perlayer,'./affichageWeb/mnist.json',TEST_EXIST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
